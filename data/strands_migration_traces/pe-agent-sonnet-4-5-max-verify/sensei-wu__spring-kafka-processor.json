{
    "max_migration_success": true,
    "min_migration_success": true,
    "trajectory": [
        {
            "role": "user",
            "content": [
                {
                    "text": "The code repository located at /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor is currently written in Java 8. Please migrate the entire codebase to Java 17."
                }
            ]
        },
        {
            "role": "assistant",
            "content": [
                {
                    "reasoningContent": {
                        "reasoningText": {
                            "text": "I need to migrate a Java project from JDK 8 to JDK 17. Let me start by exploring the repository structure to understand what we're working with.\n\nThe steps I'll need to follow:\n1. First, explore the repository structure\n2. Check the pom.xml file to see current Java version and dependencies\n3. Update the Java version to 17 in pom.xml\n4. Update all dependencies to versions that support Java 17\n5. Check for any code issues that might arise from the migration\n6. Run `mvn clean verify` to ensure everything works\n\nLet's start by examining the repository.",
                            "signature": "EtEFCkgICxABGAIqQNOTYb6vx9WvZOtgUWs6mdxLG5hbe8o8SGxROnnZYyvhqUgPXgSaTfHHhRMJS68lIgs5fL9zl4x/nXiqMci5O8ASDO72w4PTpsq0U5qbNhoMV2RWIr8LYhtksGDRIjAEhJJsI5fU7DJxu7n933RWgUp9ZlunJYd/hkMeCRZ7ReuXyFi7mdnpKlerVDQZcRsqtgQX/jULQjwvg8xrQTjTgop3eeX+tsZLlukETBboiiRib+5OA/dNUbXiIaTF6HEGienLx3lEtxjh3rJmR5yaZ0SKdLTY/aiURD5+VsGjvBHAaDkYA1ZQeJnPLrJBWQ7pa+cuQATGNemYf8H0n8w1nkqBAKAXAlpagQ9DdNGcr2pJhTJs49ibXumgyUEMPb8CLK7+eWKqELC1V0ZAo7y5vFx/rIK0qhVWD5sZOJA8DgNfS+GGPUqq4WoDhjT/D2pzHyObVAkvbIjNrH4jbLxcJQGvslWESS/saDbUQjvhsS16CnCqSOwJQu6P9aAP1sMnB46eNfx2ClNrCscS99mDlvuzCSsCQAzMKs0zHM3h+SoIToGQEEZznTgDQv/nqbVRzfPsYhkaTnCPTmbLw8QmjhNJ/bVVHnrMhqNSzh47ifMrZAuhV3VylOsDUP+HMjmWOMh7zCeYjJ5SWKEwH3WZAbTL2SKUwomcMnn5h3Dq19+R4ey6z9oEStGNHprU3fXUS2O3OZIrLO7kp4T4ror5EG93X+8+u3er0LWuwKQTief33PLe1OMXJz4PmXiUXK8F2eT/rvMOnwsmq3CHhYzLjUS0HyV3YuOgle7W8o607pNvoqaAB23JFubGQCrAtLifEp78xvFWmNOlp3kaaofE7AhY0c3ETkhgTgBjCOp9ceZtJFlhNtkh1wQQhGJL/v6iXUpuh0bvxGH2J7yhYnmalIQb5ohLSqvpqHXgShQIHi/uPgE/48kJERgB"
                        }
                    }
                },
                {
                    "text": "I'll help you migrate the Java project from JDK 8 to JDK 17. Let me start by exploring the repository structure and understanding the current configuration."
                },
                {
                    "toolUse": {
                        "toolUseId": "tooluse_4emX8BrOQDOHRCY8PZ_eDg",
                        "name": "editor",
                        "input": {
                            "command": "view",
                            "path": "/tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor"
                        }
                    }
                }
            ]
        },
        {
            "role": "user",
            "content": [
                {
                    "toolResult": {
                        "status": "success",
                        "content": [
                            {
                                "text": "Directory structure displayed in console.\nDirectory tree: /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor"
                            }
                        ],
                        "toolUseId": "tooluse_4emX8BrOQDOHRCY8PZ_eDg"
                    }
                }
            ]
        },
        {
            "role": "assistant",
            "content": [
                {
                    "toolUse": {
                        "toolUseId": "tooluse_7MOqLQ7KTpSYBYuqnGM-uw",
                        "name": "restricted_shell",
                        "input": {
                            "command": "find /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor -type f -name \"*.xml\" -o -name \"*.java\" | head -20"
                        }
                    }
                }
            ]
        },
        {
            "role": "user",
            "content": [
                {
                    "toolResult": {
                        "toolUseId": "tooluse_7MOqLQ7KTpSYBYuqnGM-uw",
                        "status": "success",
                        "content": [
                            {
                                "text": "/tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/pom.xml\n/tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/test/java/com/zen/lab/services/program/consumer/LiveFeedPojoListenerTest.java\n/tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/main/java/com/zen/lab/services/program/infra/KafkaConfig.java\n/tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/main/java/com/zen/lab/services/program/infra/MetricsConfig.java\n/tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/main/java/com/zen/lab/services/program/infra/KafkaProperties.java\n/tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/main/java/com/zen/lab/services/program/infra/service/impl/MicrometerMetricServiceImpl.java\n/tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/main/java/com/zen/lab/services/program/infra/service/MetricService.java\n/tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/main/java/com/zen/lab/services/program/Application.java\n/tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/main/java/com/zen/lab/services/program/service/LiveFeedProcessorImpl.java\n/tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/main/java/com/zen/lab/services/program/service/LiveFeedStoreClient.java\n/tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/main/java/com/zen/lab/services/program/service/LiveFeedProcessor.java\n/tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/main/java/com/zen/lab/services/program/model/Outcome.java\n/tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/main/java/com/zen/lab/services/program/model/Event.java\n/tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/main/java/com/zen/lab/services/program/model/BetMarket.java\n/tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/main/java/com/zen/lab/services/program/model/SportType.java\n/tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/main/java/com/zen/lab/services/program/model/BetMarketStatus.java\n/tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/main/java/com/zen/lab/services/program/model/BetMarketType.java\n/tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/main/java/com/zen/lab/services/program/model/CompetitionType.java\n/tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/main/java/com/zen/lab/services/program/model/RegionType.java\n/tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/main/java/com/zen/lab/services/program/model/Group.java\n"
                            }
                        ]
                    }
                }
            ]
        },
        {
            "role": "assistant",
            "content": [
                {
                    "toolUse": {
                        "toolUseId": "tooluse_7lHYcMF-RZeNUWT2NMqk4Q",
                        "name": "editor",
                        "input": {
                            "command": "view",
                            "path": "/tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/pom.xml"
                        }
                    }
                }
            ]
        },
        {
            "role": "user",
            "content": [
                {
                    "toolResult": {
                        "status": "success",
                        "content": [
                            {
                                "text": "File content displayed in console.\nContent: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n    <modelVersion>4.0.0</modelVersion>\n\n    <groupId>com.zen.lab.services.program</groupId>\n    <artifactId>sports-feed-processor-service</artifactId>\n    <version>1.0</version>\n\n    <parent>\n        <groupId>org.springframework.boot</groupId>\n        <artifactId>spring-boot-starter-parent</artifactId>\n        <version>2.0.4.RELEASE</version>\n    </parent>\n\n    <dependencyManagement>\n        <dependencies>\n            <dependency>\n                <groupId>org.springframework.cloud</groupId>\n                <artifactId>spring-cloud-dependencies</artifactId>\n                <version>Finchley.SR1</version>\n                <type>pom</type>\n                <scope>import</scope>\n            </dependency>\n        </dependencies>\n    </dependencyManagement>\n    <dependencies>\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-web</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-actuator</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.kafka</groupId>\n            <artifactId>spring-kafka</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.cloud</groupId>\n            <artifactId>spring-cloud-starter-openfeign</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>net.logstash.logback</groupId>\n            <artifactId>logstash-logback-encoder</artifactId>\n            <version>5.0</version>\n        </dependency>\n        <dependency>\n            <groupId>org.aspectj</groupId>\n            <artifactId>aspectjweaver</artifactId>\n            <version>1.8.13</version>\n        </dependency>\n        <!-- Micrometer Prometheus registry  -->\n        <dependency>\n            <groupId>io.micrometer</groupId>\n            <artifactId>micrometer-registry-prometheus</artifactId>\n            <scope>runtime</scope>\n        </dependency>\n        <!--test-->\n        <dependency>\n            <groupId>org.springframework.kafka</groupId>\n            <artifactId>spring-kafka-test</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.assertj</groupId>\n            <artifactId>assertj-core</artifactId>\n            <scope>test</scope>\n        </dependency>\n        <dependency>\n            <groupId>org.hamcrest</groupId>\n            <artifactId>hamcrest-all</artifactId>\n            <version>1.3</version>\n            <scope>test</scope>\n        </dependency>\n        <dependency>\n            <groupId>junit</groupId>\n            <artifactId>junit</artifactId>\n            <version>4.12</version>\n            <scope>test</scope>\n        </dependency>\n    </dependencies>\n\n    <build>\n        <plugins>\n            <plugin>\n                <groupId>org.springframework.boot</groupId>\n                <artifactId>spring-boot-maven-plugin</artifactId>\n                <version>2.0.4.RELEASE</version>\n                <executions>\n                    <execution>\n                        <goals>\n                            <goal>repackage</goal>\n                        </goals>\n                    </execution>\n                </executions>\n            </plugin>\n        </plugins>\n    </build>\n\n</project>"
                            }
                        ],
                        "toolUseId": "tooluse_7lHYcMF-RZeNUWT2NMqk4Q"
                    }
                }
            ]
        },
        {
            "role": "assistant",
            "content": [
                {
                    "text": "Now let me check if there's a Java version specification and look at some of the source files:"
                },
                {
                    "toolUse": {
                        "toolUseId": "tooluse_OYbQ0JuIRFyCCcdpHVzcLQ",
                        "name": "restricted_shell",
                        "input": {
                            "command": "cd /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor && find . -name \"*.java\" | wc -l"
                        }
                    }
                }
            ]
        },
        {
            "role": "user",
            "content": [
                {
                    "toolResult": {
                        "toolUseId": "tooluse_OYbQ0JuIRFyCCcdpHVzcLQ",
                        "status": "success",
                        "content": [
                            {
                                "text": "23\n"
                            }
                        ]
                    }
                }
            ]
        },
        {
            "role": "assistant",
            "content": [
                {
                    "toolUse": {
                        "toolUseId": "tooluse_ot3aOG8oS1idDx5rE6daJg",
                        "name": "restricted_shell",
                        "input": {
                            "command": "cd /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor && java -version 2>&1"
                        }
                    }
                }
            ]
        },
        {
            "role": "user",
            "content": [
                {
                    "toolResult": {
                        "toolUseId": "tooluse_ot3aOG8oS1idDx5rE6daJg",
                        "status": "success",
                        "content": [
                            {
                                "text": "openjdk version \"17.0.17\" 2025-10-21\nOpenJDK Runtime Environment (build 17.0.17+10-Ubuntu-122.04)\nOpenJDK 64-Bit Server VM (build 17.0.17+10-Ubuntu-122.04, mixed mode, sharing)\n"
                            }
                        ]
                    }
                }
            ]
        },
        {
            "role": "assistant",
            "content": [
                {
                    "text": "Good! JDK 17 is available. Now let me check a few Java files to understand the code structure:"
                },
                {
                    "toolUse": {
                        "toolUseId": "tooluse_4LSyyC_XQ7qGwwjhToqYGg",
                        "name": "editor",
                        "input": {
                            "command": "view",
                            "path": "/tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/main/java/com/zen/lab/services/program/Application.java"
                        }
                    }
                }
            ]
        },
        {
            "role": "user",
            "content": [
                {
                    "toolResult": {
                        "status": "success",
                        "content": [
                            {
                                "text": "File content displayed in console.\nContent: package com.zen.lab.services.program;\n\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\nimport org.springframework.cloud.openfeign.EnableFeignClients;\n\n@SpringBootApplication\n@EnableFeignClients(basePackageClasses = com.zen.lab.services.program.service.LiveFeedStoreClient.class)\npublic class Application {\n    public static void main(String[] args) {\n        SpringApplication.run(Application.class, args);\n    }\n}\n"
                            }
                        ],
                        "toolUseId": "tooluse_4LSyyC_XQ7qGwwjhToqYGg"
                    }
                }
            ]
        },
        {
            "role": "assistant",
            "content": [
                {
                    "text": "Now let me update the pom.xml to Java 17 with updated dependencies. I'll need to:\n1. Add Java 17 compiler properties\n2. Update Spring Boot version to one that supports Java 17\n3. Update Spring Cloud version\n4. Update other dependencies"
                },
                {
                    "toolUse": {
                        "toolUseId": "tooluse_D1LgfKPaTJWr5tlT9LOsgg",
                        "name": "editor",
                        "input": {
                            "command": "str_replace",
                            "path": "/tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/pom.xml",
                            "old_str": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n    <modelVersion>4.0.0</modelVersion>\n\n    <groupId>com.zen.lab.services.program</groupId>\n    <artifactId>sports-feed-processor-service</artifactId>\n    <version>1.0</version>\n\n    <parent>\n        <groupId>org.springframework.boot</groupId>\n        <artifactId>spring-boot-starter-parent</artifactId>\n        <version>2.0.4.RELEASE</version>\n    </parent>\n\n    <dependencyManagement>\n        <dependencies>\n            <dependency>\n                <groupId>org.springframework.cloud</groupId>\n                <artifactId>spring-cloud-dependencies</artifactId>\n                <version>Finchley.SR1</version>\n                <type>pom</type>\n                <scope>import</scope>\n            </dependency>\n        </dependencies>\n    </dependencyManagement>",
                            "new_str": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n    <modelVersion>4.0.0</modelVersion>\n\n    <groupId>com.zen.lab.services.program</groupId>\n    <artifactId>sports-feed-processor-service</artifactId>\n    <version>1.0</version>\n\n    <parent>\n        <groupId>org.springframework.boot</groupId>\n        <artifactId>spring-boot-starter-parent</artifactId>\n        <version>3.2.1</version>\n    </parent>\n\n    <properties>\n        <java.version>17</java.version>\n        <maven.compiler.source>17</maven.compiler.source>\n        <maven.compiler.target>17</maven.compiler.target>\n        <spring-cloud.version>2023.0.0</spring-cloud.version>\n    </properties>\n\n    <dependencyManagement>\n        <dependencies>\n            <dependency>\n                <groupId>org.springframework.cloud</groupId>\n                <artifactId>spring-cloud-dependencies</artifactId>\n                <version>${spring-cloud.version}</version>\n                <type>pom</type>\n                <scope>import</scope>\n            </dependency>\n        </dependencies>\n    </dependencyManagement>"
                        }
                    }
                }
            ]
        },
        {
            "role": "user",
            "content": [
                {
                    "toolResult": {
                        "status": "success",
                        "content": [
                            {
                                "text": "Text replacement complete and details displayed in console.\nFile: /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/pom.xml\nReplaced 1 occurrence\nOld string: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n    <modelVersion>4.0.0</modelVersion>\n\n    <groupId>com.zen.lab.services.program</groupId>\n    <artifactId>sports-feed-processor-service</artifactId>\n    <version>1.0</version>\n\n    <parent>\n        <groupId>org.springframework.boot</groupId>\n        <artifactId>spring-boot-starter-parent</artifactId>\n        <version>2.0.4.RELEASE</version>\n    </parent>\n\n    <dependencyManagement>\n        <dependencies>\n            <dependency>\n                <groupId>org.springframework.cloud</groupId>\n                <artifactId>spring-cloud-dependencies</artifactId>\n                <version>Finchley.SR1</version>\n                <type>pom</type>\n                <scope>import</scope>\n            </dependency>\n        </dependencies>\n    </dependencyManagement>\nNew string: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n    <modelVersion>4.0.0</modelVersion>\n\n    <groupId>com.zen.lab.services.program</groupId>\n    <artifactId>sports-feed-processor-service</artifactId>\n    <version>1.0</version>\n\n    <parent>\n        <groupId>org.springframework.boot</groupId>\n        <artifactId>spring-boot-starter-parent</artifactId>\n        <version>3.2.1</version>\n    </parent>\n\n    <properties>\n        <java.version>17</java.version>\n        <maven.compiler.source>17</maven.compiler.source>\n        <maven.compiler.target>17</maven.compiler.target>\n        <spring-cloud.version>2023.0.0</spring-cloud.version>\n    </properties>\n\n    <dependencyManagement>\n        <dependencies>\n            <dependency>\n                <groupId>org.springframework.cloud</groupId>\n                <artifactId>spring-cloud-dependencies</artifactId>\n                <version>${spring-cloud.version}</version>\n                <type>pom</type>\n                <scope>import</scope>\n            </dependency>\n        </dependencies>\n    </dependencyManagement>\n"
                            }
                        ],
                        "toolUseId": "tooluse_D1LgfKPaTJWr5tlT9LOsgg"
                    }
                }
            ]
        },
        {
            "role": "assistant",
            "content": [
                {
                    "text": "Now let me update the dependencies section to update the library versions:"
                },
                {
                    "toolUse": {
                        "toolUseId": "tooluse_FbgrUepFQW6yyAp_G_LsAg",
                        "name": "editor",
                        "input": {
                            "command": "str_replace",
                            "path": "/tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/pom.xml",
                            "old_str": "        <dependency>\n            <groupId>net.logstash.logback</groupId>\n            <artifactId>logstash-logback-encoder</artifactId>\n            <version>5.0</version>\n        </dependency>\n        <dependency>\n            <groupId>org.aspectj</groupId>\n            <artifactId>aspectjweaver</artifactId>\n            <version>1.8.13</version>\n        </dependency>",
                            "new_str": "        <dependency>\n            <groupId>net.logstash.logback</groupId>\n            <artifactId>logstash-logback-encoder</artifactId>\n            <version>7.4</version>\n        </dependency>\n        <dependency>\n            <groupId>org.aspectj</groupId>\n            <artifactId>aspectjweaver</artifactId>\n            <version>1.9.21</version>\n        </dependency>"
                        }
                    }
                }
            ]
        },
        {
            "role": "user",
            "content": [
                {
                    "toolResult": {
                        "status": "success",
                        "content": [
                            {
                                "text": "Text replacement complete and details displayed in console.\nFile: /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/pom.xml\nReplaced 1 occurrence\nOld string:         <dependency>\n            <groupId>net.logstash.logback</groupId>\n            <artifactId>logstash-logback-encoder</artifactId>\n            <version>5.0</version>\n        </dependency>\n        <dependency>\n            <groupId>org.aspectj</groupId>\n            <artifactId>aspectjweaver</artifactId>\n            <version>1.8.13</version>\n        </dependency>\nNew string:         <dependency>\n            <groupId>net.logstash.logback</groupId>\n            <artifactId>logstash-logback-encoder</artifactId>\n            <version>7.4</version>\n        </dependency>\n        <dependency>\n            <groupId>org.aspectj</groupId>\n            <artifactId>aspectjweaver</artifactId>\n            <version>1.9.21</version>\n        </dependency>\n"
                            }
                        ],
                        "toolUseId": "tooluse_FbgrUepFQW6yyAp_G_LsAg"
                    }
                }
            ]
        },
        {
            "role": "assistant",
            "content": [
                {
                    "toolUse": {
                        "toolUseId": "tooluse_UL4w6_JhQuiiMEXvYJ7S3Q",
                        "name": "editor",
                        "input": {
                            "command": "str_replace",
                            "new_str": "        <dependency>\n            <groupId>org.hamcrest</groupId>\n            <artifactId>hamcrest-all</artifactId>\n            <version>1.3</version>\n            <scope>test</scope>\n        </dependency>\n        <dependency>\n            <groupId>org.junit.vintage</groupId>\n            <artifactId>junit-vintage-engine</artifactId>\n            <scope>test</scope>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-test</artifactId>\n            <scope>test</scope>\n        </dependency>",
                            "old_str": "        <dependency>\n            <groupId>org.hamcrest</groupId>\n            <artifactId>hamcrest-all</artifactId>\n            <version>1.3</version>\n            <scope>test</scope>\n        </dependency>\n        <dependency>\n            <groupId>junit</groupId>\n            <artifactId>junit</artifactId>\n            <version>4.12</version>\n            <scope>test</scope>\n        </dependency>",
                            "path": "/tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/pom.xml"
                        }
                    }
                }
            ]
        },
        {
            "role": "user",
            "content": [
                {
                    "toolResult": {
                        "status": "success",
                        "content": [
                            {
                                "text": "Text replacement complete and details displayed in console.\nFile: /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/pom.xml\nReplaced 1 occurrence\nOld string:         <dependency>\n            <groupId>org.hamcrest</groupId>\n            <artifactId>hamcrest-all</artifactId>\n            <version>1.3</version>\n            <scope>test</scope>\n        </dependency>\n        <dependency>\n            <groupId>junit</groupId>\n            <artifactId>junit</artifactId>\n            <version>4.12</version>\n            <scope>test</scope>\n        </dependency>\nNew string:         <dependency>\n            <groupId>org.hamcrest</groupId>\n            <artifactId>hamcrest-all</artifactId>\n            <version>1.3</version>\n            <scope>test</scope>\n        </dependency>\n        <dependency>\n            <groupId>org.junit.vintage</groupId>\n            <artifactId>junit-vintage-engine</artifactId>\n            <scope>test</scope>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-test</artifactId>\n            <scope>test</scope>\n        </dependency>\n"
                            }
                        ],
                        "toolUseId": "tooluse_UL4w6_JhQuiiMEXvYJ7S3Q"
                    }
                }
            ]
        },
        {
            "role": "assistant",
            "content": [
                {
                    "text": "Now update the build section:"
                },
                {
                    "toolUse": {
                        "toolUseId": "tooluse_vUiKsc_DSAKZWgeAUIufqw",
                        "name": "editor",
                        "input": {
                            "command": "str_replace",
                            "new_str": "    <build>\n        <plugins>\n            <plugin>\n                <groupId>org.springframework.boot</groupId>\n                <artifactId>spring-boot-maven-plugin</artifactId>\n            </plugin>\n        </plugins>\n    </build>\n\n</project>",
                            "old_str": "    <build>\n        <plugins>\n            <plugin>\n                <groupId>org.springframework.boot</groupId>\n                <artifactId>spring-boot-maven-plugin</artifactId>\n                <version>2.0.4.RELEASE</version>\n                <executions>\n                    <execution>\n                        <goals>\n                            <goal>repackage</goal>\n                        </goals>\n                    </execution>\n                </executions>\n            </plugin>\n        </plugins>\n    </build>\n\n</project>",
                            "path": "/tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/pom.xml"
                        }
                    }
                }
            ]
        },
        {
            "role": "user",
            "content": [
                {
                    "toolResult": {
                        "status": "success",
                        "content": [
                            {
                                "text": "Text replacement complete and details displayed in console.\nFile: /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/pom.xml\nReplaced 1 occurrence\nOld string:     <build>\n        <plugins>\n            <plugin>\n                <groupId>org.springframework.boot</groupId>\n                <artifactId>spring-boot-maven-plugin</artifactId>\n                <version>2.0.4.RELEASE</version>\n                <executions>\n                    <execution>\n                        <goals>\n                            <goal>repackage</goal>\n                        </goals>\n                    </execution>\n                </executions>\n            </plugin>\n        </plugins>\n    </build>\n\n</project>\nNew string:     <build>\n        <plugins>\n            <plugin>\n                <groupId>org.springframework.boot</groupId>\n                <artifactId>spring-boot-maven-plugin</artifactId>\n            </plugin>\n        </plugins>\n    </build>\n\n</project>\n"
                            }
                        ],
                        "toolUseId": "tooluse_vUiKsc_DSAKZWgeAUIufqw"
                    }
                }
            ]
        },
        {
            "role": "assistant",
            "content": [
                {
                    "text": "Let me verify the updated pom.xml:"
                },
                {
                    "toolUse": {
                        "toolUseId": "tooluse_n0lFcTDZSXKF59pikx7aTA",
                        "name": "editor",
                        "input": {
                            "command": "view",
                            "path": "/tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/pom.xml"
                        }
                    }
                }
            ]
        },
        {
            "role": "user",
            "content": [
                {
                    "toolResult": {
                        "status": "success",
                        "content": [
                            {
                                "text": "File content displayed in console.\nContent: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n    <modelVersion>4.0.0</modelVersion>\n\n    <groupId>com.zen.lab.services.program</groupId>\n    <artifactId>sports-feed-processor-service</artifactId>\n    <version>1.0</version>\n\n    <parent>\n        <groupId>org.springframework.boot</groupId>\n        <artifactId>spring-boot-starter-parent</artifactId>\n        <version>3.2.1</version>\n    </parent>\n\n    <properties>\n        <java.version>17</java.version>\n        <maven.compiler.source>17</maven.compiler.source>\n        <maven.compiler.target>17</maven.compiler.target>\n        <spring-cloud.version>2023.0.0</spring-cloud.version>\n    </properties>\n\n    <dependencyManagement>\n        <dependencies>\n            <dependency>\n                <groupId>org.springframework.cloud</groupId>\n                <artifactId>spring-cloud-dependencies</artifactId>\n                <version>${spring-cloud.version}</version>\n                <type>pom</type>\n                <scope>import</scope>\n            </dependency>\n        </dependencies>\n    </dependencyManagement>\n    <dependencies>\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-web</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-actuator</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.kafka</groupId>\n            <artifactId>spring-kafka</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.cloud</groupId>\n            <artifactId>spring-cloud-starter-openfeign</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>net.logstash.logback</groupId>\n            <artifactId>logstash-logback-encoder</artifactId>\n            <version>7.4</version>\n        </dependency>\n        <dependency>\n            <groupId>org.aspectj</groupId>\n            <artifactId>aspectjweaver</artifactId>\n            <version>1.9.21</version>\n        </dependency>\n        <!-- Micrometer Prometheus registry  -->\n        <dependency>\n            <groupId>io.micrometer</groupId>\n            <artifactId>micrometer-registry-prometheus</artifactId>\n            <scope>runtime</scope>\n        </dependency>\n        <!--test-->\n        <dependency>\n            <groupId>org.springframework.kafka</groupId>\n            <artifactId>spring-kafka-test</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.assertj</groupId>\n            <artifactId>assertj-core</artifactId>\n            <scope>test</scope>\n        </dependency>\n        <dependency>\n            <groupId>org.hamcrest</groupId>\n            <artifactId>hamcrest-all</artifactId>\n            <version>1.3</version>\n            <scope>test</scope>\n        </dependency>\n        <dependency>\n            <groupId>org.junit.vintage</groupId>\n            <artifactId>junit-vintage-engine</artifactId>\n            <scope>test</scope>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-test</artifactId>\n            <scope>test</scope>\n        </dependency>\n    </dependencies>\n\n    <build>\n        <plugins>\n            <plugin>\n                <groupId>org.springframework.boot</groupId>\n                <artifactId>spring-boot-maven-plugin</artifactId>\n            </plugin>\n        </plugins>\n    </build>\n\n</project>"
                            }
                        ],
                        "toolUseId": "tooluse_n0lFcTDZSXKF59pikx7aTA"
                    }
                }
            ]
        },
        {
            "role": "assistant",
            "content": [
                {
                    "text": "Good! Now let me run a Maven build to see if there are any compilation or code issues. Since Spring Boot 3.x uses Jakarta EE instead of Java EE, we may need to update imports:"
                },
                {
                    "toolUse": {
                        "toolUseId": "tooluse_SrQwy2kHQxawmoW14pLcew",
                        "name": "restricted_shell",
                        "input": {
                            "command": "cd /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor && mvn clean compile 2>&1 | head -100"
                        }
                    }
                }
            ]
        },
        {
            "role": "user",
            "content": [
                {
                    "toolResult": {
                        "toolUseId": "tooluse_SrQwy2kHQxawmoW14pLcew",
                        "status": "success",
                        "content": [
                            {
                                "text": "[INFO] Scanning for projects...\n[INFO] \n[INFO] -----< com.zen.lab.services.program:sports-feed-processor-service >-----\n[INFO] Building sports-feed-processor-service 1.0\n[INFO]   from pom.xml\n[INFO] --------------------------------[ jar ]---------------------------------\n[INFO] \n[INFO] --- clean:3.3.2:clean (default-clean) @ sports-feed-processor-service ---\n[INFO] \n[INFO] --- resources:3.3.1:resources (default-resources) @ sports-feed-processor-service ---\n[INFO] Copying 1 resource from src/main/resources to target/classes\n[INFO] Copying 0 resource from src/main/resources to target/classes\n[INFO] \n[INFO] --- compiler:3.11.0:compile (default-compile) @ sports-feed-processor-service ---\n[INFO] Changes detected - recompiling the module! :source\n[INFO] Compiling 22 source files with javac [debug release 17] to target/classes\n[INFO] -------------------------------------------------------------\n[ERROR] COMPILATION ERROR : \n[INFO] -------------------------------------------------------------\n[ERROR] /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/main/java/com/zen/lab/services/program/infra/KafkaConfig.java:[24,49] package org.springframework.kafka.listener.config does not exist\n[ERROR] /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/main/java/com/zen/lab/services/program/consumer/LiveFeedPojoListener.java:[29,44] cannot find symbol\n  symbol:   variable RECEIVED_PARTITION_ID\n  location: class org.springframework.kafka.support.KafkaHeaders\n[ERROR] /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/main/java/com/zen/lab/services/program/consumer/LiveFeedPojoListener.java:[30,44] cannot find symbol\n  symbol:   variable RECEIVED_MESSAGE_KEY\n  location: class org.springframework.kafka.support.KafkaHeaders\n[ERROR] /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/main/java/com/zen/lab/services/program/infra/KafkaConfig.java:[61,9] cannot find symbol\n  symbol:   class ContainerProperties\n  location: class com.zen.lab.services.program.infra.KafkaConfig\n[ERROR] /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/main/java/com/zen/lab/services/program/infra/KafkaConfig.java:[61,38] cannot find symbol\n  symbol:   class ContainerProperties\n  location: class com.zen.lab.services.program.infra.KafkaConfig\n[ERROR] /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/main/java/com/zen/lab/services/program/infra/KafkaConfig.java:[66,63] cannot find symbol\n  symbol:   variable AckMode\n  location: class org.springframework.kafka.listener.AbstractMessageListenerContainer\n[INFO] 6 errors \n[INFO] -------------------------------------------------------------\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD FAILURE\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time:  3.079 s\n[INFO] Finished at: 2026-01-29T05:47:05Z\n[INFO] ------------------------------------------------------------------------\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.11.0:compile (default-compile) on project sports-feed-processor-service: Compilation failure: Compilation failure: \n[ERROR] /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/main/java/com/zen/lab/services/program/infra/KafkaConfig.java:[24,49] package org.springframework.kafka.listener.config does not exist\n[ERROR] /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/main/java/com/zen/lab/services/program/consumer/LiveFeedPojoListener.java:[29,44] cannot find symbol\n[ERROR]   symbol:   variable RECEIVED_PARTITION_ID\n[ERROR]   location: class org.springframework.kafka.support.KafkaHeaders\n[ERROR] /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/main/java/com/zen/lab/services/program/consumer/LiveFeedPojoListener.java:[30,44] cannot find symbol\n[ERROR]   symbol:   variable RECEIVED_MESSAGE_KEY\n[ERROR]   location: class org.springframework.kafka.support.KafkaHeaders\n[ERROR] /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/main/java/com/zen/lab/services/program/infra/KafkaConfig.java:[61,9] cannot find symbol\n[ERROR]   symbol:   class ContainerProperties\n[ERROR]   location: class com.zen.lab.services.program.infra.KafkaConfig\n[ERROR] /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/main/java/com/zen/lab/services/program/infra/KafkaConfig.java:[61,38] cannot find symbol\n[ERROR]   symbol:   class ContainerProperties\n[ERROR]   location: class com.zen.lab.services.program.infra.KafkaConfig\n[ERROR] /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/main/java/com/zen/lab/services/program/infra/KafkaConfig.java:[66,63] cannot find symbol\n[ERROR]   symbol:   variable AckMode\n[ERROR]   location: class org.springframework.kafka.listener.AbstractMessageListenerContainer\n[ERROR] -> [Help 1]\n[ERROR] \n[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.\n[ERROR] Re-run Maven using the -X switch to enable full debug logging.\n[ERROR] \n[ERROR] For more information about the errors and possible solutions, please read the following articles:\n[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException\n"
                            }
                        ]
                    }
                }
            ]
        },
        {
            "role": "assistant",
            "content": [
                {
                    "text": "Good, now I can see the errors. The package structure has changed in Spring Kafka. Let me look at the problematic files:"
                },
                {
                    "toolUse": {
                        "toolUseId": "tooluse_sbNNd5UBREqQI_T9HdcfbA",
                        "name": "editor",
                        "input": {
                            "command": "view",
                            "path": "/tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/main/java/com/zen/lab/services/program/infra/KafkaConfig.java"
                        }
                    }
                }
            ]
        },
        {
            "role": "user",
            "content": [
                {
                    "toolResult": {
                        "status": "success",
                        "content": [
                            {
                                "text": "File content displayed in console.\nContent: package com.zen.lab.services.program.infra;\n\nimport com.fasterxml.jackson.databind.ObjectMapper;\nimport com.fasterxml.jackson.databind.SerializationFeature;\nimport com.fasterxml.jackson.datatype.jsr310.JavaTimeModule;\nimport com.zen.lab.services.program.consumer.LiveFeedBatchListener;\nimport com.zen.lab.services.program.consumer.LiveFeedMessageListener;\nimport com.zen.lab.services.program.model.Event;\nimport org.apache.kafka.clients.consumer.ConsumerConfig;\nimport org.apache.kafka.common.serialization.Deserializer;\nimport org.apache.kafka.common.serialization.StringDeserializer;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.beans.factory.annotation.Value;\nimport org.springframework.boot.autoconfigure.condition.ConditionalOnProperty;\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.context.annotation.Configuration;\nimport org.springframework.kafka.annotation.EnableKafka;\nimport org.springframework.kafka.config.ConcurrentKafkaListenerContainerFactory;\nimport org.springframework.kafka.config.KafkaListenerContainerFactory;\nimport org.springframework.kafka.core.ConsumerFactory;\nimport org.springframework.kafka.core.DefaultKafkaConsumerFactory;\nimport org.springframework.kafka.listener.AbstractMessageListenerContainer;\nimport org.springframework.kafka.listener.ConcurrentMessageListenerContainer;\nimport org.springframework.kafka.listener.config.ContainerProperties;\nimport org.springframework.kafka.support.serializer.JsonDeserializer;\n\nimport java.util.HashMap;\nimport java.util.Map;\n\n@Configuration\n@EnableKafka\npublic class KafkaConfig {\n\n    private final LiveFeedMessageListener liveFeedMessageListener;\n\n    private final LiveFeedBatchListener liveFeedBatchListener;\n\n    private @Value(\"${messageListener.implementation}\") String messageListenerImplementation;\n\n    @Autowired\n    public KafkaConfig(LiveFeedMessageListener liveFeedMessageListener, LiveFeedBatchListener liveFeedBatchListener) {\n        this.liveFeedMessageListener = liveFeedMessageListener;\n        this.liveFeedBatchListener = liveFeedBatchListener;\n    }\n\n    //In-order to use @KafkaListener, we need a bean with the name kafkaListenerContainerFactory\n    @Bean\n    @ConditionalOnProperty(value = \"pojoListener.enabled\")\n    KafkaListenerContainerFactory<ConcurrentMessageListenerContainer<String, Event>> kafkaListenerContainerFactory() {\n        ConcurrentKafkaListenerContainerFactory<String, Event> factory = new ConcurrentKafkaListenerContainerFactory<>();\n        factory.setConsumerFactory(consumerFactory());\n        factory.setConcurrency(3);\n        factory.getContainerProperties().setPollTimeout(3000);\n        factory.setBatchListener(true);\n        return factory;\n    }\n\n    @Bean\n    ConcurrentMessageListenerContainer<String, Event> sportsFeedListenerContainer() throws Exception {\n        ConsumerFactory<String, Event> cf = consumerFactory();\n        ContainerProperties cp = new ContainerProperties(KafkaProperties.TOPIC_LIVE_FEED);\n\n        switch (messageListenerImplementation) {\n            case \"messageListener\":\n                cp.setMessageListener(liveFeedMessageListener);\n                cp.setAckMode(AbstractMessageListenerContainer.AckMode.RECORD);\n                break;\n            case \"batchListener\":\n                cp.setMessageListener(liveFeedBatchListener);\n                break;\n            default:\n                throw new Exception(\"No listener configured\");\n        }\n\n        ConcurrentMessageListenerContainer<String, Event> concurrentMessageListenerContainer = new ConcurrentMessageListenerContainer<>(cf,cp);\n        concurrentMessageListenerContainer.setConcurrency(3);\n\n        return concurrentMessageListenerContainer;\n    }\n\n    private ConsumerFactory<String, Event> consumerFactory() {\n        return new DefaultKafkaConsumerFactory<>(consumerConfigs(), stringKeyDeserializer(), jsonValueDeserializer());\n    }\n\n    private Map<String, Object> consumerConfigs() {\n        Map<String, Object> props = new HashMap<>();\n        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, KafkaProperties.KAFKA_SERVER_URL + \":\" + KafkaProperties.KAFKA_SERVER_PORT);\n        props.put(ConsumerConfig.GROUP_ID_CONFIG, \"sportsdata.consumer\");\n        return props;\n    }\n\n    @Bean\n    public Deserializer<String> stringKeyDeserializer() {\n        return new StringDeserializer();\n    }\n\n    @Bean\n    public Deserializer<Event> jsonValueDeserializer() {\n        final ObjectMapper objectMapper = new ObjectMapper();\n        objectMapper.registerModule(new JavaTimeModule());\n        objectMapper.configure(SerializationFeature.WRITE_DATES_AS_TIMESTAMPS, false);\n        return new JsonDeserializer<>(Event.class, objectMapper);\n    }\n}\n"
                            }
                        ],
                        "toolUseId": "tooluse_sbNNd5UBREqQI_T9HdcfbA"
                    }
                }
            ]
        },
        {
            "role": "assistant",
            "content": [
                {
                    "toolUse": {
                        "toolUseId": "tooluse_KC84o5WrS1KiFJxrByae0Q",
                        "name": "editor",
                        "input": {
                            "command": "view",
                            "path": "/tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/main/java/com/zen/lab/services/program/consumer/LiveFeedPojoListener.java"
                        }
                    }
                }
            ]
        },
        {
            "role": "user",
            "content": [
                {
                    "toolResult": {
                        "status": "success",
                        "content": [
                            {
                                "text": "File content displayed in console.\nContent: package com.zen.lab.services.program.consumer;\n\nimport com.zen.lab.services.program.infra.KafkaProperties;\nimport com.zen.lab.services.program.model.Event;\nimport com.zen.lab.services.program.service.LiveFeedProcessor;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.boot.autoconfigure.condition.ConditionalOnBean;\nimport org.springframework.kafka.annotation.KafkaListener;\nimport org.springframework.kafka.support.KafkaHeaders;\nimport org.springframework.messaging.handler.annotation.Header;\nimport org.springframework.stereotype.Service;\n\n@ConditionalOnBean(type = \"kafkaListenerContainerFactory\")\n@Service\npublic class LiveFeedPojoListener {\n\n    private static final Logger LOGGER = LoggerFactory.getLogger(LiveFeedPojoListener.class);\n\n    private final LiveFeedProcessor liveFeedProcessor;\n\n    @Autowired(required = false)\n    public LiveFeedPojoListener(LiveFeedProcessor liveFeedProcessor) {this.liveFeedProcessor = liveFeedProcessor;}\n\n    @KafkaListener(topics = KafkaProperties.TOPIC_LIVE_FEED)\n    public void listen(Event message,\n                       @Header(KafkaHeaders.OFFSET) Integer offset,\n                       @Header(KafkaHeaders.RECEIVED_PARTITION_ID) Integer partitionId,\n                       @Header(KafkaHeaders.RECEIVED_MESSAGE_KEY) String messageKey,\n                       @Header(KafkaHeaders.RECEIVED_TOPIC) String topic) {\n\n        LOGGER.info(\"Processing Message -- Partition {}, Offset {}, Key {}\", partitionId, offset, messageKey);\n\n        try {\n            liveFeedProcessor.process(messageKey, message, offset);\n        } catch (Exception e) {\n            e.printStackTrace();\n        }\n    }\n}\n"
                            }
                        ],
                        "toolUseId": "tooluse_KC84o5WrS1KiFJxrByae0Q"
                    }
                }
            ]
        },
        {
            "role": "assistant",
            "content": [
                {
                    "text": "Now I need to fix the issues. In Spring Kafka 3.x (used by Spring Boot 3.x), the imports and constants have changed:\n\n1. `ContainerProperties` moved from `org.springframework.kafka.listener.config` to `org.springframework.kafka.listener`\n2. `RECEIVED_PARTITION_ID` is now `RECEIVED_PARTITION`  \n3. `RECEIVED_MESSAGE_KEY` is now `RECEIVED_KEY`\n4. `AckMode` is now in `ContainerProperties.AckMode`\n\nLet me fix KafkaConfig.java first:"
                },
                {
                    "toolUse": {
                        "toolUseId": "tooluse_3x8aILhXRoaTgkrqTvzTiA",
                        "name": "editor",
                        "input": {
                            "command": "str_replace",
                            "path": "/tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/main/java/com/zen/lab/services/program/infra/KafkaConfig.java",
                            "old_str": "import org.springframework.kafka.listener.AbstractMessageListenerContainer;\nimport org.springframework.kafka.listener.ConcurrentMessageListenerContainer;\nimport org.springframework.kafka.listener.config.ContainerProperties;\nimport org.springframework.kafka.support.serializer.JsonDeserializer;",
                            "new_str": "import org.springframework.kafka.listener.ConcurrentMessageListenerContainer;\nimport org.springframework.kafka.listener.ContainerProperties;\nimport org.springframework.kafka.support.serializer.JsonDeserializer;"
                        }
                    }
                }
            ]
        },
        {
            "role": "user",
            "content": [
                {
                    "toolResult": {
                        "status": "success",
                        "content": [
                            {
                                "text": "Text replacement complete and details displayed in console.\nFile: /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/main/java/com/zen/lab/services/program/infra/KafkaConfig.java\nReplaced 1 occurrence\nOld string: import org.springframework.kafka.listener.AbstractMessageListenerContainer;\nimport org.springframework.kafka.listener.ConcurrentMessageListenerContainer;\nimport org.springframework.kafka.listener.config.ContainerProperties;\nimport org.springframework.kafka.support.serializer.JsonDeserializer;\nNew string: import org.springframework.kafka.listener.ConcurrentMessageListenerContainer;\nimport org.springframework.kafka.listener.ContainerProperties;\nimport org.springframework.kafka.support.serializer.JsonDeserializer;\n"
                            }
                        ],
                        "toolUseId": "tooluse_3x8aILhXRoaTgkrqTvzTiA"
                    }
                }
            ]
        },
        {
            "role": "assistant",
            "content": [
                {
                    "toolUse": {
                        "toolUseId": "tooluse_Vf0liEbvR-KYNfqIzWTrvw",
                        "name": "editor",
                        "input": {
                            "command": "str_replace",
                            "new_str": "        switch (messageListenerImplementation) {\n            case \"messageListener\":\n                cp.setMessageListener(liveFeedMessageListener);\n                cp.setAckMode(ContainerProperties.AckMode.RECORD);\n                break;\n            case \"batchListener\":\n                cp.setMessageListener(liveFeedBatchListener);\n                break;\n            default:\n                throw new Exception(\"No listener configured\");\n        }",
                            "old_str": "        switch (messageListenerImplementation) {\n            case \"messageListener\":\n                cp.setMessageListener(liveFeedMessageListener);\n                cp.setAckMode(AbstractMessageListenerContainer.AckMode.RECORD);\n                break;\n            case \"batchListener\":\n                cp.setMessageListener(liveFeedBatchListener);\n                break;\n            default:\n                throw new Exception(\"No listener configured\");\n        }",
                            "path": "/tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/main/java/com/zen/lab/services/program/infra/KafkaConfig.java"
                        }
                    }
                }
            ]
        },
        {
            "role": "user",
            "content": [
                {
                    "toolResult": {
                        "status": "success",
                        "content": [
                            {
                                "text": "Text replacement complete and details displayed in console.\nFile: /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/main/java/com/zen/lab/services/program/infra/KafkaConfig.java\nReplaced 1 occurrence\nOld string:         switch (messageListenerImplementation) {\n            case \"messageListener\":\n                cp.setMessageListener(liveFeedMessageListener);\n                cp.setAckMode(AbstractMessageListenerContainer.AckMode.RECORD);\n                break;\n            case \"batchListener\":\n                cp.setMessageListener(liveFeedBatchListener);\n                break;\n            default:\n                throw new Exception(\"No listener configured\");\n        }\nNew string:         switch (messageListenerImplementation) {\n            case \"messageListener\":\n                cp.setMessageListener(liveFeedMessageListener);\n                cp.setAckMode(ContainerProperties.AckMode.RECORD);\n                break;\n            case \"batchListener\":\n                cp.setMessageListener(liveFeedBatchListener);\n                break;\n            default:\n                throw new Exception(\"No listener configured\");\n        }\n"
                            }
                        ],
                        "toolUseId": "tooluse_Vf0liEbvR-KYNfqIzWTrvw"
                    }
                }
            ]
        },
        {
            "role": "assistant",
            "content": [
                {
                    "text": "Now let me fix LiveFeedPojoListener.java:"
                },
                {
                    "toolUse": {
                        "toolUseId": "tooluse_6m13l-DbSHWJqm-MxVnPpA",
                        "name": "editor",
                        "input": {
                            "command": "str_replace",
                            "new_str": "    @KafkaListener(topics = KafkaProperties.TOPIC_LIVE_FEED)\n    public void listen(Event message,\n                       @Header(KafkaHeaders.OFFSET) Integer offset,\n                       @Header(KafkaHeaders.RECEIVED_PARTITION) Integer partitionId,\n                       @Header(KafkaHeaders.RECEIVED_KEY) String messageKey,\n                       @Header(KafkaHeaders.RECEIVED_TOPIC) String topic) {",
                            "old_str": "    @KafkaListener(topics = KafkaProperties.TOPIC_LIVE_FEED)\n    public void listen(Event message,\n                       @Header(KafkaHeaders.OFFSET) Integer offset,\n                       @Header(KafkaHeaders.RECEIVED_PARTITION_ID) Integer partitionId,\n                       @Header(KafkaHeaders.RECEIVED_MESSAGE_KEY) String messageKey,\n                       @Header(KafkaHeaders.RECEIVED_TOPIC) String topic) {",
                            "path": "/tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/main/java/com/zen/lab/services/program/consumer/LiveFeedPojoListener.java"
                        }
                    }
                }
            ]
        },
        {
            "role": "user",
            "content": [
                {
                    "toolResult": {
                        "status": "success",
                        "content": [
                            {
                                "text": "Text replacement complete and details displayed in console.\nFile: /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/main/java/com/zen/lab/services/program/consumer/LiveFeedPojoListener.java\nReplaced 1 occurrence\nOld string:     @KafkaListener(topics = KafkaProperties.TOPIC_LIVE_FEED)\n    public void listen(Event message,\n                       @Header(KafkaHeaders.OFFSET) Integer offset,\n                       @Header(KafkaHeaders.RECEIVED_PARTITION_ID) Integer partitionId,\n                       @Header(KafkaHeaders.RECEIVED_MESSAGE_KEY) String messageKey,\n                       @Header(KafkaHeaders.RECEIVED_TOPIC) String topic) {\nNew string:     @KafkaListener(topics = KafkaProperties.TOPIC_LIVE_FEED)\n    public void listen(Event message,\n                       @Header(KafkaHeaders.OFFSET) Integer offset,\n                       @Header(KafkaHeaders.RECEIVED_PARTITION) Integer partitionId,\n                       @Header(KafkaHeaders.RECEIVED_KEY) String messageKey,\n                       @Header(KafkaHeaders.RECEIVED_TOPIC) String topic) {\n"
                            }
                        ],
                        "toolUseId": "tooluse_6m13l-DbSHWJqm-MxVnPpA"
                    }
                }
            ]
        },
        {
            "role": "assistant",
            "content": [
                {
                    "text": "Now let me try to compile again:"
                },
                {
                    "toolUse": {
                        "toolUseId": "tooluse_XDBk8xqTS0-d1VRuU_Bxzg",
                        "name": "restricted_shell",
                        "input": {
                            "command": "cd /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor && mvn clean compile 2>&1 | tail -50"
                        }
                    }
                }
            ]
        },
        {
            "role": "user",
            "content": [
                {
                    "toolResult": {
                        "toolUseId": "tooluse_XDBk8xqTS0-d1VRuU_Bxzg",
                        "status": "success",
                        "content": [
                            {
                                "text": "[INFO] Scanning for projects...\n[INFO] \n[INFO] -----< com.zen.lab.services.program:sports-feed-processor-service >-----\n[INFO] Building sports-feed-processor-service 1.0\n[INFO]   from pom.xml\n[INFO] --------------------------------[ jar ]---------------------------------\n[INFO] \n[INFO] --- clean:3.3.2:clean (default-clean) @ sports-feed-processor-service ---\n[INFO] Deleting /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/target\n[INFO] \n[INFO] --- resources:3.3.1:resources (default-resources) @ sports-feed-processor-service ---\n[INFO] Copying 1 resource from src/main/resources to target/classes\n[INFO] Copying 0 resource from src/main/resources to target/classes\n[INFO] \n[INFO] --- compiler:3.11.0:compile (default-compile) @ sports-feed-processor-service ---\n[INFO] Changes detected - recompiling the module! :source\n[INFO] Compiling 22 source files with javac [debug release 17] to target/classes\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD SUCCESS\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time:  2.944 s\n[INFO] Finished at: 2026-01-29T05:47:33Z\n[INFO] ------------------------------------------------------------------------\n"
                            }
                        ]
                    }
                }
            ]
        },
        {
            "role": "assistant",
            "content": [
                {
                    "text": "Great! Compilation succeeded. Now let's run the full test suite with `mvn clean verify`:"
                },
                {
                    "toolUse": {
                        "toolUseId": "tooluse_nXyGjekPT_m42ks1gMYkDQ",
                        "name": "restricted_shell",
                        "input": {
                            "command": "cd /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor && mvn clean verify 2>&1"
                        }
                    }
                }
            ]
        },
        {
            "role": "user",
            "content": [
                {
                    "toolResult": {
                        "toolUseId": "tooluse_nXyGjekPT_m42ks1gMYkDQ",
                        "status": "success",
                        "content": [
                            {
                                "text": "[INFO] Scanning for projects...\n[INFO] \n[INFO] -----< com.zen.lab.services.program:sports-feed-processor-service >-----\n[INFO] Building sports-feed-processor-service 1.0\n[INFO]   from pom.xml\n[INFO] --------------------------------[ jar ]---------------------------------\n[INFO] \n[INFO] --- clean:3.3.2:clean (default-clean) @ sports-feed-processor-service ---\n[INFO] Deleting /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/target\n[INFO] \n[INFO] --- resources:3.3.1:resources (default-resources) @ sports-feed-processor-service ---\n[INFO] Copying 1 resource from src/main/resources to target/classes\n[INFO] Copying 0 resource from src/main/resources to target/classes\n[INFO] \n[INFO] --- compiler:3.11.0:compile (default-compile) @ sports-feed-processor-service ---\n[INFO] Changes detected - recompiling the module! :source\n[INFO] Compiling 22 source files with javac [debug release 17] to target/classes\n[INFO] \n[INFO] --- resources:3.3.1:testResources (default-testResources) @ sports-feed-processor-service ---\n[INFO] skip non existing resourceDirectory /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/test/resources\n[INFO] \n[INFO] --- compiler:3.11.0:testCompile (default-testCompile) @ sports-feed-processor-service ---\n[INFO] Changes detected - recompiling the module! :dependency\n[INFO] Compiling 1 source file with javac [debug release 17] to target/test-classes\n[INFO] /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/test/java/com/zen/lab/services/program/consumer/LiveFeedPojoListenerTest.java: /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/test/java/com/zen/lab/services/program/consumer/LiveFeedPojoListenerTest.java uses or overrides a deprecated API.\n[INFO] /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/test/java/com/zen/lab/services/program/consumer/LiveFeedPojoListenerTest.java: Recompile with -Xlint:deprecation for details.\n[INFO] -------------------------------------------------------------\n[ERROR] COMPILATION ERROR : \n[INFO] -------------------------------------------------------------\n[ERROR] /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/test/java/com/zen/lab/services/program/consumer/LiveFeedPojoListenerTest.java:[23,49] package org.springframework.kafka.listener.config does not exist\n[ERROR] /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/test/java/com/zen/lab/services/program/consumer/LiveFeedPojoListenerTest.java:[24,43] cannot find symbol\n  symbol:   class KafkaEmbedded\n  location: package org.springframework.kafka.test.rule\n[ERROR] /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/test/java/com/zen/lab/services/program/consumer/LiveFeedPojoListenerTest.java:[49,19] cannot find symbol\n  symbol:   class KafkaEmbedded\n  location: class com.zen.lab.services.program.consumer.LiveFeedPojoListenerTest\n[ERROR] /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/test/java/com/zen/lab/services/program/consumer/LiveFeedPojoListenerTest.java:[49,53] cannot find symbol\n  symbol:   class KafkaEmbedded\n  location: class com.zen.lab.services.program.consumer.LiveFeedPojoListenerTest\n[ERROR] /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/test/java/com/zen/lab/services/program/consumer/LiveFeedPojoListenerTest.java:[56,9] cannot find symbol\n  symbol:   class ContainerProperties\n  location: class com.zen.lab.services.program.consumer.LiveFeedPojoListenerTest\n[ERROR] /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/test/java/com/zen/lab/services/program/consumer/LiveFeedPojoListenerTest.java:[56,50] cannot find symbol\n  symbol:   class ContainerProperties\n  location: class com.zen.lab.services.program.consumer.LiveFeedPojoListenerTest\n[INFO] 6 errors \n[INFO] -------------------------------------------------------------\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD FAILURE\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time:  3.355 s\n[INFO] Finished at: 2026-01-29T05:47:40Z\n[INFO] ------------------------------------------------------------------------\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.11.0:testCompile (default-testCompile) on project sports-feed-processor-service: Compilation failure: Compilation failure: \n[ERROR] /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/test/java/com/zen/lab/services/program/consumer/LiveFeedPojoListenerTest.java:[23,49] package org.springframework.kafka.listener.config does not exist\n[ERROR] /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/test/java/com/zen/lab/services/program/consumer/LiveFeedPojoListenerTest.java:[24,43] cannot find symbol\n[ERROR]   symbol:   class KafkaEmbedded\n[ERROR]   location: package org.springframework.kafka.test.rule\n[ERROR] /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/test/java/com/zen/lab/services/program/consumer/LiveFeedPojoListenerTest.java:[49,19] cannot find symbol\n[ERROR]   symbol:   class KafkaEmbedded\n[ERROR]   location: class com.zen.lab.services.program.consumer.LiveFeedPojoListenerTest\n[ERROR] /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/test/java/com/zen/lab/services/program/consumer/LiveFeedPojoListenerTest.java:[49,53] cannot find symbol\n[ERROR]   symbol:   class KafkaEmbedded\n[ERROR]   location: class com.zen.lab.services.program.consumer.LiveFeedPojoListenerTest\n[ERROR] /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/test/java/com/zen/lab/services/program/consumer/LiveFeedPojoListenerTest.java:[56,9] cannot find symbol\n[ERROR]   symbol:   class ContainerProperties\n[ERROR]   location: class com.zen.lab.services.program.consumer.LiveFeedPojoListenerTest\n[ERROR] /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/test/java/com/zen/lab/services/program/consumer/LiveFeedPojoListenerTest.java:[56,50] cannot find symbol\n[ERROR]   symbol:   class ContainerProperties\n[ERROR]   location: class com.zen.lab.services.program.consumer.LiveFeedPojoListenerTest\n[ERROR] -> [Help 1]\n[ERROR] \n[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.\n[ERROR] Re-run Maven using the -X switch to enable full debug logging.\n[ERROR] \n[ERROR] For more information about the errors and possible solutions, please read the following articles:\n[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException\n"
                            }
                        ]
                    }
                }
            ]
        },
        {
            "role": "assistant",
            "content": [
                {
                    "text": "Now I need to fix the test file. Let me look at it:"
                },
                {
                    "toolUse": {
                        "toolUseId": "tooluse_11gx1uTSRmyfHGjWkgfzjA",
                        "name": "editor",
                        "input": {
                            "command": "view",
                            "path": "/tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/test/java/com/zen/lab/services/program/consumer/LiveFeedPojoListenerTest.java"
                        }
                    }
                }
            ]
        },
        {
            "role": "user",
            "content": [
                {
                    "toolResult": {
                        "status": "success",
                        "content": [
                            {
                                "text": "File content displayed in console.\nContent: package com.zen.lab.services.program.consumer;\n\nimport com.zen.lab.services.program.infra.service.impl.MicrometerMetricServiceImpl;\nimport com.zen.lab.services.program.model.Event;\nimport com.zen.lab.services.program.service.LiveFeedProcessorImpl;\nimport com.zen.lab.services.program.service.LiveFeedStoreClient;\nimport io.micrometer.core.instrument.simple.SimpleMeterRegistry;\nimport org.apache.kafka.clients.consumer.ConsumerRecord;\nimport org.apache.kafka.clients.consumer.ConsumerRecords;\nimport org.apache.kafka.clients.consumer.KafkaConsumer;\nimport org.apache.kafka.clients.producer.KafkaProducer;\nimport org.apache.kafka.clients.producer.ProducerRecord;\nimport org.junit.ClassRule;\nimport org.junit.Test;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport org.springframework.kafka.core.DefaultKafkaConsumerFactory;\nimport org.springframework.kafka.core.DefaultKafkaProducerFactory;\nimport org.springframework.kafka.core.KafkaTemplate;\nimport org.springframework.kafka.core.ProducerFactory;\nimport org.springframework.kafka.listener.AcknowledgingMessageListener;\nimport org.springframework.kafka.listener.KafkaMessageListenerContainer;\nimport org.springframework.kafka.listener.config.ContainerProperties;\nimport org.springframework.kafka.test.rule.KafkaEmbedded;\nimport org.springframework.kafka.test.utils.KafkaTestUtils;\n\nimport java.util.Collections;\nimport java.util.Map;\nimport java.util.concurrent.CountDownLatch;\nimport java.util.concurrent.ExecutorService;\nimport java.util.concurrent.Executors;\nimport java.util.concurrent.TimeUnit;\n\nimport static org.assertj.core.api.Assertions.assertThat;\n\npublic class LiveFeedPojoListenerTest {\n\n    private static final Logger LOGGER = LoggerFactory.getLogger(LiveFeedPojoListenerTest.class);\n\n    private LiveFeedPojoListener liveFeedPojoListener =\n            new LiveFeedPojoListener(new LiveFeedProcessorImpl(new LiveFeedStoreClient() {\n                @Override\n                public Event createEvent(Long eventId, Event event) {\n                    return null;\n                }\n            }, new MicrometerMetricServiceImpl(new SimpleMeterRegistry())));\n\n    @ClassRule\n    public static KafkaEmbedded embeddedKafka = new KafkaEmbedded(2, true, 2, \"messages\");\n\n    @Test\n    public void testSpringKafka() throws Exception {\n        Map<String, Object> consumerProps = KafkaTestUtils.consumerProps(\"sampleConsumer\", \"false\", embeddedKafka);\n        consumerProps.put(\"auto.offset.reset\", \"earliest\");\n        DefaultKafkaConsumerFactory<Integer, String> cf = new DefaultKafkaConsumerFactory<>(consumerProps);\n        ContainerProperties containerProps = new ContainerProperties(\"messages\");\n\n        final CountDownLatch latch = new CountDownLatch(4);\n\n        containerProps.setMessageListener(liveFeedPojoListener);\n\n        containerProps.setMessageListener((AcknowledgingMessageListener<Integer, String>) (message, ack) -> {\n            LOGGER.info(\"Receiving: \" + message);\n            try {\n                Thread.sleep(200);\n            } catch (InterruptedException e) {\n                Thread.currentThread().interrupt();\n            }\n            latch.countDown();\n        });\n        \n        KafkaMessageListenerContainer<Integer, String> container =\n                new KafkaMessageListenerContainer<>(cf, containerProps);\n        container.setBeanName(\"sampleConsumer\");\n\n\n        container.start();\n//        ContainerTestUtils.waitForAssignment(container, embeddedKafka.getPartitionsPerTopic());\n\n        Map<String, Object> senderProps = KafkaTestUtils.producerProps(embeddedKafka);\n        ProducerFactory<Integer, String> pf = new DefaultKafkaProducerFactory<Integer, String>(senderProps);\n        KafkaTemplate<Integer, String> template = new KafkaTemplate<>(pf);\n        template.setDefaultTopic(\"messages\");\n        template.sendDefault(0, 0, \"message1\");\n        template.sendDefault(0, 1, \"message2\");\n        template.sendDefault(1, 2, \"message3\");\n        template.sendDefault(1, 3, \"message4\");\n        template.flush();\n        assertThat(latch.await(20, TimeUnit.SECONDS)).isTrue();\n        container.stop();\n    }\n\n    @Test\n    public void testEmbeddedRawKafka() throws Exception {\n\n\n        Map<String, Object> senderProps = KafkaTestUtils.producerProps(embeddedKafka);\n        KafkaProducer<Integer, String> producer = new KafkaProducer<>(senderProps);\n        producer.send(new ProducerRecord<>(\"messages\", 0, 0, \"message0\")).get();\n        producer.send(new ProducerRecord<>(\"messages\", 0, 1, \"message1\")).get();\n        producer.send(new ProducerRecord<>(\"messages\", 1, 2, \"message2\")).get();\n        producer.send(new ProducerRecord<>(\"messages\", 1, 3, \"message3\")).get();\n\n\n        Map<String, Object> consumerProps = KafkaTestUtils.consumerProps(\"sampleRawConsumer\", \"false\", embeddedKafka);\n        consumerProps.put(\"auto.offset.reset\", \"earliest\");\n\n        final CountDownLatch latch = new CountDownLatch(4);\n        ExecutorService executorService = Executors.newSingleThreadExecutor();\n        executorService.execute(() -> {\n            KafkaConsumer<Integer, String> kafkaConsumer = new KafkaConsumer<>(consumerProps);\n            kafkaConsumer.subscribe(Collections.singletonList(\"messages\"));\n            try {\n                while (true) {\n                    ConsumerRecords<Integer, String> records = kafkaConsumer.poll(100);\n                    for (ConsumerRecord<Integer, String> record : records) {\n                        LOGGER.info(\"consuming from topic = {}, partition = {}, offset = {}, key = {}, value = {}\",\n                                record.topic(), record.partition(), record.offset(), record.key(), record.value());\n                        latch.countDown();\n                    }\n                }\n            } finally {\n                kafkaConsumer.close();\n            }\n        });\n\n        assertThat(latch.await(90, TimeUnit.SECONDS)).isTrue();\n    }\n\n}"
                            }
                        ],
                        "toolUseId": "tooluse_11gx1uTSRmyfHGjWkgfzjA"
                    }
                }
            ]
        },
        {
            "role": "assistant",
            "content": [
                {
                    "text": "The test file uses old Spring Kafka Test classes. In Spring Kafka 3.x:\n- `KafkaEmbedded` has been replaced with `EmbeddedKafkaBroker`\n- The package structure has changed\n- `@ClassRule` should be replaced with `@RegisterExtension` or `@EmbeddedKafka` annotation\n\nLet me update the test file:"
                },
                {
                    "toolUse": {
                        "toolUseId": "tooluse_92QpfMGTQK2M4KGzIWhHnA",
                        "name": "editor",
                        "input": {
                            "command": "str_replace",
                            "new_str": "package com.zen.lab.services.program.consumer;\n\nimport com.zen.lab.services.program.infra.service.impl.MicrometerMetricServiceImpl;\nimport com.zen.lab.services.program.model.Event;\nimport com.zen.lab.services.program.service.LiveFeedProcessorImpl;\nimport com.zen.lab.services.program.service.LiveFeedStoreClient;\nimport io.micrometer.core.instrument.simple.SimpleMeterRegistry;\nimport org.apache.kafka.clients.consumer.ConsumerRecord;\nimport org.apache.kafka.clients.consumer.ConsumerRecords;\nimport org.apache.kafka.clients.consumer.KafkaConsumer;\nimport org.apache.kafka.clients.producer.KafkaProducer;\nimport org.apache.kafka.clients.producer.ProducerRecord;\nimport org.junit.ClassRule;\nimport org.junit.Test;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport org.springframework.kafka.core.DefaultKafkaConsumerFactory;\nimport org.springframework.kafka.core.DefaultKafkaProducerFactory;\nimport org.springframework.kafka.core.KafkaTemplate;\nimport org.springframework.kafka.core.ProducerFactory;\nimport org.springframework.kafka.listener.AcknowledgingMessageListener;\nimport org.springframework.kafka.listener.KafkaMessageListenerContainer;\nimport org.springframework.kafka.listener.ContainerProperties;\nimport org.springframework.kafka.test.EmbeddedKafkaBroker;\nimport org.springframework.kafka.test.utils.KafkaTestUtils;",
                            "old_str": "package com.zen.lab.services.program.consumer;\n\nimport com.zen.lab.services.program.infra.service.impl.MicrometerMetricServiceImpl;\nimport com.zen.lab.services.program.model.Event;\nimport com.zen.lab.services.program.service.LiveFeedProcessorImpl;\nimport com.zen.lab.services.program.service.LiveFeedStoreClient;\nimport io.micrometer.core.instrument.simple.SimpleMeterRegistry;\nimport org.apache.kafka.clients.consumer.ConsumerRecord;\nimport org.apache.kafka.clients.consumer.ConsumerRecords;\nimport org.apache.kafka.clients.consumer.KafkaConsumer;\nimport org.apache.kafka.clients.producer.KafkaProducer;\nimport org.apache.kafka.clients.producer.ProducerRecord;\nimport org.junit.ClassRule;\nimport org.junit.Test;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport org.springframework.kafka.core.DefaultKafkaConsumerFactory;\nimport org.springframework.kafka.core.DefaultKafkaProducerFactory;\nimport org.springframework.kafka.core.KafkaTemplate;\nimport org.springframework.kafka.core.ProducerFactory;\nimport org.springframework.kafka.listener.AcknowledgingMessageListener;\nimport org.springframework.kafka.listener.KafkaMessageListenerContainer;\nimport org.springframework.kafka.listener.config.ContainerProperties;\nimport org.springframework.kafka.test.rule.KafkaEmbedded;\nimport org.springframework.kafka.test.utils.KafkaTestUtils;",
                            "path": "/tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/test/java/com/zen/lab/services/program/consumer/LiveFeedPojoListenerTest.java"
                        }
                    }
                }
            ]
        },
        {
            "role": "user",
            "content": [
                {
                    "toolResult": {
                        "status": "success",
                        "content": [
                            {
                                "text": "Text replacement complete and details displayed in console.\nFile: /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/test/java/com/zen/lab/services/program/consumer/LiveFeedPojoListenerTest.java\nReplaced 1 occurrence\nOld string: package com.zen.lab.services.program.consumer;\n\nimport com.zen.lab.services.program.infra.service.impl.MicrometerMetricServiceImpl;\nimport com.zen.lab.services.program.model.Event;\nimport com.zen.lab.services.program.service.LiveFeedProcessorImpl;\nimport com.zen.lab.services.program.service.LiveFeedStoreClient;\nimport io.micrometer.core.instrument.simple.SimpleMeterRegistry;\nimport org.apache.kafka.clients.consumer.ConsumerRecord;\nimport org.apache.kafka.clients.consumer.ConsumerRecords;\nimport org.apache.kafka.clients.consumer.KafkaConsumer;\nimport org.apache.kafka.clients.producer.KafkaProducer;\nimport org.apache.kafka.clients.producer.ProducerRecord;\nimport org.junit.ClassRule;\nimport org.junit.Test;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport org.springframework.kafka.core.DefaultKafkaConsumerFactory;\nimport org.springframework.kafka.core.DefaultKafkaProducerFactory;\nimport org.springframework.kafka.core.KafkaTemplate;\nimport org.springframework.kafka.core.ProducerFactory;\nimport org.springframework.kafka.listener.AcknowledgingMessageListener;\nimport org.springframework.kafka.listener.KafkaMessageListenerContainer;\nimport org.springframework.kafka.listener.config.ContainerProperties;\nimport org.springframework.kafka.test.rule.KafkaEmbedded;\nimport org.springframework.kafka.test.utils.KafkaTestUtils;\nNew string: package com.zen.lab.services.program.consumer;\n\nimport com.zen.lab.services.program.infra.service.impl.MicrometerMetricServiceImpl;\nimport com.zen.lab.services.program.model.Event;\nimport com.zen.lab.services.program.service.LiveFeedProcessorImpl;\nimport com.zen.lab.services.program.service.LiveFeedStoreClient;\nimport io.micrometer.core.instrument.simple.SimpleMeterRegistry;\nimport org.apache.kafka.clients.consumer.ConsumerRecord;\nimport org.apache.kafka.clients.consumer.ConsumerRecords;\nimport org.apache.kafka.clients.consumer.KafkaConsumer;\nimport org.apache.kafka.clients.producer.KafkaProducer;\nimport org.apache.kafka.clients.producer.ProducerRecord;\nimport org.junit.ClassRule;\nimport org.junit.Test;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport org.springframework.kafka.core.DefaultKafkaConsumerFactory;\nimport org.springframework.kafka.core.DefaultKafkaProducerFactory;\nimport org.springframework.kafka.core.KafkaTemplate;\nimport org.springframework.kafka.core.ProducerFactory;\nimport org.springframework.kafka.listener.AcknowledgingMessageListener;\nimport org.springframework.kafka.listener.KafkaMessageListenerContainer;\nimport org.springframework.kafka.listener.ContainerProperties;\nimport org.springframework.kafka.test.EmbeddedKafkaBroker;\nimport org.springframework.kafka.test.utils.KafkaTestUtils;\n"
                            }
                        ],
                        "toolUseId": "tooluse_92QpfMGTQK2M4KGzIWhHnA"
                    }
                }
            ]
        },
        {
            "role": "assistant",
            "content": [
                {
                    "toolUse": {
                        "toolUseId": "tooluse_wy2arwYVTk6g9kQgnE-QFQ",
                        "name": "editor",
                        "input": {
                            "command": "str_replace",
                            "new_str": "    @ClassRule\n    public static EmbeddedKafkaBroker embeddedKafka = new EmbeddedKafkaBroker(2, true, 2, \"messages\");",
                            "old_str": "    @ClassRule\n    public static KafkaEmbedded embeddedKafka = new KafkaEmbedded(2, true, 2, \"messages\");",
                            "path": "/tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/test/java/com/zen/lab/services/program/consumer/LiveFeedPojoListenerTest.java"
                        }
                    }
                }
            ]
        },
        {
            "role": "user",
            "content": [
                {
                    "toolResult": {
                        "status": "success",
                        "content": [
                            {
                                "text": "Text replacement complete and details displayed in console.\nFile: /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/test/java/com/zen/lab/services/program/consumer/LiveFeedPojoListenerTest.java\nReplaced 1 occurrence\nOld string:     @ClassRule\n    public static KafkaEmbedded embeddedKafka = new KafkaEmbedded(2, true, 2, \"messages\");\nNew string:     @ClassRule\n    public static EmbeddedKafkaBroker embeddedKafka = new EmbeddedKafkaBroker(2, true, 2, \"messages\");\n"
                            }
                        ],
                        "toolUseId": "tooluse_wy2arwYVTk6g9kQgnE-QFQ"
                    }
                }
            ]
        },
        {
            "role": "assistant",
            "content": [
                {
                    "text": "Now let's try to compile the tests again:"
                },
                {
                    "toolUse": {
                        "toolUseId": "tooluse_nqoiCgPpRvmyzRSKuX5CDg",
                        "name": "restricted_shell",
                        "input": {
                            "command": "cd /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor && mvn clean test-compile 2>&1 | tail -30"
                        }
                    }
                }
            ]
        },
        {
            "role": "user",
            "content": [
                {
                    "toolResult": {
                        "toolUseId": "tooluse_nqoiCgPpRvmyzRSKuX5CDg",
                        "status": "success",
                        "content": [
                            {
                                "text": "[INFO] --- resources:3.3.1:testResources (default-testResources) @ sports-feed-processor-service ---\n[INFO] skip non existing resourceDirectory /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/test/resources\n[INFO] \n[INFO] --- compiler:3.11.0:testCompile (default-testCompile) @ sports-feed-processor-service ---\n[INFO] Changes detected - recompiling the module! :dependency\n[INFO] Compiling 1 source file with javac [debug release 17] to target/test-classes\n[INFO] /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/test/java/com/zen/lab/services/program/consumer/LiveFeedPojoListenerTest.java: /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/test/java/com/zen/lab/services/program/consumer/LiveFeedPojoListenerTest.java uses or overrides a deprecated API.\n[INFO] /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/test/java/com/zen/lab/services/program/consumer/LiveFeedPojoListenerTest.java: Recompile with -Xlint:deprecation for details.\n[INFO] -------------------------------------------------------------\n[ERROR] COMPILATION ERROR : \n[INFO] -------------------------------------------------------------\n[ERROR] /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/test/java/com/zen/lab/services/program/consumer/LiveFeedPojoListenerTest.java:[49,55] org.springframework.kafka.test.EmbeddedKafkaBroker is abstract; cannot be instantiated\n[INFO] 1 error\n[INFO] -------------------------------------------------------------\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD FAILURE\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time:  3.390 s\n[INFO] Finished at: 2026-01-29T05:48:02Z\n[INFO] ------------------------------------------------------------------------\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.11.0:testCompile (default-testCompile) on project sports-feed-processor-service: Compilation failure\n[ERROR] /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/test/java/com/zen/lab/services/program/consumer/LiveFeedPojoListenerTest.java:[49,55] org.springframework.kafka.test.EmbeddedKafkaBroker is abstract; cannot be instantiated\n[ERROR] \n[ERROR] -> [Help 1]\n[ERROR] \n[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.\n[ERROR] Re-run Maven using the -X switch to enable full debug logging.\n[ERROR] \n[ERROR] For more information about the errors and possible solutions, please read the following articles:\n[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException\n"
                            }
                        ]
                    }
                }
            ]
        },
        {
            "role": "assistant",
            "content": [
                {
                    "text": "The `EmbeddedKafkaBroker` is abstract now. We need to use `EmbeddedKafkaZKBroker` instead or use the annotation `@EmbeddedKafka`. Let me use the JUnit 4 compatible approach with the `EmbeddedKafkaRule`:"
                },
                {
                    "toolUse": {
                        "toolUseId": "tooluse_RfRriVGSQrSG88KASSLc9g",
                        "name": "editor",
                        "input": {
                            "command": "str_replace",
                            "new_str": "import org.springframework.kafka.test.EmbeddedKafkaZKBroker;\nimport org.springframework.kafka.test.utils.KafkaTestUtils;",
                            "old_str": "import org.springframework.kafka.test.EmbeddedKafkaBroker;\nimport org.springframework.kafka.test.utils.KafkaTestUtils;",
                            "path": "/tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/test/java/com/zen/lab/services/program/consumer/LiveFeedPojoListenerTest.java"
                        }
                    }
                }
            ]
        },
        {
            "role": "user",
            "content": [
                {
                    "toolResult": {
                        "status": "success",
                        "content": [
                            {
                                "text": "Text replacement complete and details displayed in console.\nFile: /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/test/java/com/zen/lab/services/program/consumer/LiveFeedPojoListenerTest.java\nReplaced 1 occurrence\nOld string: import org.springframework.kafka.test.EmbeddedKafkaBroker;\nimport org.springframework.kafka.test.utils.KafkaTestUtils;\nNew string: import org.springframework.kafka.test.EmbeddedKafkaZKBroker;\nimport org.springframework.kafka.test.utils.KafkaTestUtils;\n"
                            }
                        ],
                        "toolUseId": "tooluse_RfRriVGSQrSG88KASSLc9g"
                    }
                }
            ]
        },
        {
            "role": "assistant",
            "content": [
                {
                    "toolUse": {
                        "toolUseId": "tooluse_p9exHUGITqiuuGrT9JRnMA",
                        "name": "editor",
                        "input": {
                            "command": "str_replace",
                            "new_str": "    @ClassRule\n    public static EmbeddedKafkaZKBroker embeddedKafka = new EmbeddedKafkaZKBroker(2, true, 2, \"messages\");",
                            "old_str": "    @ClassRule\n    public static EmbeddedKafkaBroker embeddedKafka = new EmbeddedKafkaBroker(2, true, 2, \"messages\");",
                            "path": "/tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/test/java/com/zen/lab/services/program/consumer/LiveFeedPojoListenerTest.java"
                        }
                    }
                }
            ]
        },
        {
            "role": "user",
            "content": [
                {
                    "toolResult": {
                        "status": "success",
                        "content": [
                            {
                                "text": "Text replacement complete and details displayed in console.\nFile: /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/test/java/com/zen/lab/services/program/consumer/LiveFeedPojoListenerTest.java\nReplaced 1 occurrence\nOld string:     @ClassRule\n    public static EmbeddedKafkaBroker embeddedKafka = new EmbeddedKafkaBroker(2, true, 2, \"messages\");\nNew string:     @ClassRule\n    public static EmbeddedKafkaZKBroker embeddedKafka = new EmbeddedKafkaZKBroker(2, true, 2, \"messages\");\n"
                            }
                        ],
                        "toolUseId": "tooluse_p9exHUGITqiuuGrT9JRnMA"
                    }
                }
            ]
        },
        {
            "role": "assistant",
            "content": [
                {
                    "text": "Let's try again:"
                },
                {
                    "toolUse": {
                        "toolUseId": "tooluse_yKs4FzfeRkScUoNFluqLIw",
                        "name": "restricted_shell",
                        "input": {
                            "command": "cd /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor && mvn clean test-compile 2>&1 | tail -30"
                        }
                    }
                }
            ]
        },
        {
            "role": "user",
            "content": [
                {
                    "toolResult": {
                        "toolUseId": "tooluse_yKs4FzfeRkScUoNFluqLIw",
                        "status": "success",
                        "content": [
                            {
                                "text": "[INFO] -----< com.zen.lab.services.program:sports-feed-processor-service >-----\n[INFO] Building sports-feed-processor-service 1.0\n[INFO]   from pom.xml\n[INFO] --------------------------------[ jar ]---------------------------------\n[INFO] \n[INFO] --- clean:3.3.2:clean (default-clean) @ sports-feed-processor-service ---\n[INFO] Deleting /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/target\n[INFO] \n[INFO] --- resources:3.3.1:resources (default-resources) @ sports-feed-processor-service ---\n[INFO] Copying 1 resource from src/main/resources to target/classes\n[INFO] Copying 0 resource from src/main/resources to target/classes\n[INFO] \n[INFO] --- compiler:3.11.0:compile (default-compile) @ sports-feed-processor-service ---\n[INFO] Changes detected - recompiling the module! :source\n[INFO] Compiling 22 source files with javac [debug release 17] to target/classes\n[INFO] \n[INFO] --- resources:3.3.1:testResources (default-testResources) @ sports-feed-processor-service ---\n[INFO] skip non existing resourceDirectory /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/test/resources\n[INFO] \n[INFO] --- compiler:3.11.0:testCompile (default-testCompile) @ sports-feed-processor-service ---\n[INFO] Changes detected - recompiling the module! :dependency\n[INFO] Compiling 1 source file with javac [debug release 17] to target/test-classes\n[INFO] /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/test/java/com/zen/lab/services/program/consumer/LiveFeedPojoListenerTest.java: /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/test/java/com/zen/lab/services/program/consumer/LiveFeedPojoListenerTest.java uses or overrides a deprecated API.\n[INFO] /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/test/java/com/zen/lab/services/program/consumer/LiveFeedPojoListenerTest.java: Recompile with -Xlint:deprecation for details.\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD SUCCESS\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time:  2.913 s\n[INFO] Finished at: 2026-01-29T05:48:17Z\n[INFO] ------------------------------------------------------------------------\n"
                            }
                        ]
                    }
                }
            ]
        },
        {
            "role": "assistant",
            "content": [
                {
                    "text": "Excellent! The compilation succeeded. Now let's run the full verification:"
                },
                {
                    "toolUse": {
                        "toolUseId": "tooluse_QkCvv1OXSe6SygsY93lzGw",
                        "name": "restricted_shell",
                        "input": {
                            "command": "cd /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor && mvn clean verify 2>&1"
                        }
                    }
                }
            ]
        },
        {
            "role": "user",
            "content": [
                {
                    "toolResult": {
                        "toolUseId": "tooluse_QkCvv1OXSe6SygsY93lzGw",
                        "status": "success",
                        "content": [
                            {
                                "text": "[INFO] Scanning for projects...\n[INFO] \n[INFO] -----< com.zen.lab.services.program:sports-feed-processor-service >-----\n[INFO] Building sports-feed-processor-service 1.0\n[INFO]   from pom.xml\n[INFO] --------------------------------[ jar ]---------------------------------\n[INFO] \n[INFO] --- clean:3.3.2:clean (default-clean) @ sports-feed-processor-service ---\n[INFO] Deleting /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/target\n[INFO] \n[INFO] --- resources:3.3.1:resources (default-resources) @ sports-feed-processor-service ---\n[INFO] Copying 1 resource from src/main/resources to target/classes\n[INFO] Copying 0 resource from src/main/resources to target/classes\n[INFO] \n[INFO] --- compiler:3.11.0:compile (default-compile) @ sports-feed-processor-service ---\n[INFO] Changes detected - recompiling the module! :source\n[INFO] Compiling 22 source files with javac [debug release 17] to target/classes\n[INFO] \n[INFO] --- resources:3.3.1:testResources (default-testResources) @ sports-feed-processor-service ---\n[INFO] skip non existing resourceDirectory /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/test/resources\n[INFO] \n[INFO] --- compiler:3.11.0:testCompile (default-testCompile) @ sports-feed-processor-service ---\n[INFO] Changes detected - recompiling the module! :dependency\n[INFO] Compiling 1 source file with javac [debug release 17] to target/test-classes\n[INFO] /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/test/java/com/zen/lab/services/program/consumer/LiveFeedPojoListenerTest.java: /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/test/java/com/zen/lab/services/program/consumer/LiveFeedPojoListenerTest.java uses or overrides a deprecated API.\n[INFO] /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/test/java/com/zen/lab/services/program/consumer/LiveFeedPojoListenerTest.java: Recompile with -Xlint:deprecation for details.\n[INFO] \n[INFO] --- surefire:3.1.2:test (default-test) @ sports-feed-processor-service ---\n[INFO] Using auto detected provider org.apache.maven.surefire.junitplatform.JUnitPlatformProvider\n[INFO] \n[INFO] -------------------------------------------------------\n[INFO]  T E S T S\n[INFO] -------------------------------------------------------\n[INFO] Running com.zen.lab.services.program.consumer.LiveFeedPojoListenerTest\n[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.021 s <<< FAILURE! -- in com.zen.lab.services.program.consumer.LiveFeedPojoListenerTest\n[ERROR] com.zen.lab.services.program.consumer.LiveFeedPojoListenerTest -- Time elapsed: 0.008 s <<< ERROR!\norg.junit.runners.model.InvalidTestClassError: \nInvalid test class 'com.zen.lab.services.program.consumer.LiveFeedPojoListenerTest':\n  1. The @ClassRule 'embeddedKafka' must implement TestRule.\n\tat java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197)\n\tat java.base/java.util.ArrayList$ArrayListSpliterator.tryAdvance(ArrayList.java:1602)\n\tat java.base/java.util.stream.ReferencePipeline.forEachWithCancel(ReferencePipeline.java:129)\n\tat java.base/java.util.stream.AbstractPipeline.copyIntoWithCancel(AbstractPipeline.java:527)\n\tat java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:513)\n\tat java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)\n\tat java.base/java.util.stream.FindOps$FindOp.evaluateSequential(FindOps.java:150)\n\tat java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)\n\tat java.base/java.util.stream.ReferencePipeline.findFirst(ReferencePipeline.java:647)\n\n[INFO] \n[INFO] Results:\n[INFO] \n[ERROR] Errors: \n[ERROR]   LiveFeedPojoListenerTest \u00bb InvalidTestClass Invalid test class 'com.zen.lab.services.program.consumer.LiveFeedPojoListenerTest':\n  1. The @ClassRule 'embeddedKafka' must implement TestRule.\n[INFO] \n[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0\n[INFO] \n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD FAILURE\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time:  6.066 s\n[INFO] Finished at: 2026-01-29T05:48:27Z\n[INFO] ------------------------------------------------------------------------\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:3.1.2:test (default-test) on project sports-feed-processor-service: \n[ERROR] \n[ERROR] Please refer to /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/target/surefire-reports for the individual test results.\n[ERROR] Please refer to dump files (if any exist) [date].dump, [date]-jvmRun[N].dump and [date].dumpstream.\n[ERROR] -> [Help 1]\n[ERROR] \n[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.\n[ERROR] Re-run Maven using the -X switch to enable full debug logging.\n[ERROR] \n[ERROR] For more information about the errors and possible solutions, please read the following articles:\n[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException\n"
                            }
                        ]
                    }
                }
            ]
        },
        {
            "role": "assistant",
            "content": [
                {
                    "text": "The `EmbeddedKafkaZKBroker` doesn't implement `TestRule` anymore. In Spring Kafka 3.x, we need to use `EmbeddedKafkaRule` from `org.springframework.kafka.test.rule` package. Let me update the test:"
                },
                {
                    "toolUse": {
                        "toolUseId": "tooluse_Is6WuKFCQoSg6GDnj67ECA",
                        "name": "editor",
                        "input": {
                            "command": "str_replace",
                            "new_str": "import org.springframework.kafka.test.EmbeddedKafkaBroker;\nimport org.springframework.kafka.test.rule.EmbeddedKafkaRule;\nimport org.springframework.kafka.test.utils.KafkaTestUtils;",
                            "old_str": "import org.springframework.kafka.test.EmbeddedKafkaZKBroker;\nimport org.springframework.kafka.test.utils.KafkaTestUtils;",
                            "path": "/tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/test/java/com/zen/lab/services/program/consumer/LiveFeedPojoListenerTest.java"
                        }
                    }
                }
            ]
        },
        {
            "role": "user",
            "content": [
                {
                    "toolResult": {
                        "status": "success",
                        "content": [
                            {
                                "text": "Text replacement complete and details displayed in console.\nFile: /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/test/java/com/zen/lab/services/program/consumer/LiveFeedPojoListenerTest.java\nReplaced 1 occurrence\nOld string: import org.springframework.kafka.test.EmbeddedKafkaZKBroker;\nimport org.springframework.kafka.test.utils.KafkaTestUtils;\nNew string: import org.springframework.kafka.test.EmbeddedKafkaBroker;\nimport org.springframework.kafka.test.rule.EmbeddedKafkaRule;\nimport org.springframework.kafka.test.utils.KafkaTestUtils;\n"
                            }
                        ],
                        "toolUseId": "tooluse_Is6WuKFCQoSg6GDnj67ECA"
                    }
                }
            ]
        },
        {
            "role": "assistant",
            "content": [
                {
                    "toolUse": {
                        "toolUseId": "tooluse_jg4ren3BRMOYZTlK5VeEeg",
                        "name": "editor",
                        "input": {
                            "command": "str_replace",
                            "new_str": "    @ClassRule\n    public static EmbeddedKafkaRule embeddedKafkaRule = new EmbeddedKafkaRule(2, true, 2, \"messages\");\n    \n    private static EmbeddedKafkaBroker embeddedKafka = embeddedKafkaRule.getEmbeddedKafka();",
                            "old_str": "    @ClassRule\n    public static EmbeddedKafkaZKBroker embeddedKafka = new EmbeddedKafkaZKBroker(2, true, 2, \"messages\");",
                            "path": "/tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/test/java/com/zen/lab/services/program/consumer/LiveFeedPojoListenerTest.java"
                        }
                    }
                }
            ]
        },
        {
            "role": "user",
            "content": [
                {
                    "toolResult": {
                        "status": "success",
                        "content": [
                            {
                                "text": "Text replacement complete and details displayed in console.\nFile: /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/test/java/com/zen/lab/services/program/consumer/LiveFeedPojoListenerTest.java\nReplaced 1 occurrence\nOld string:     @ClassRule\n    public static EmbeddedKafkaZKBroker embeddedKafka = new EmbeddedKafkaZKBroker(2, true, 2, \"messages\");\nNew string:     @ClassRule\n    public static EmbeddedKafkaRule embeddedKafkaRule = new EmbeddedKafkaRule(2, true, 2, \"messages\");\n    \n    private static EmbeddedKafkaBroker embeddedKafka = embeddedKafkaRule.getEmbeddedKafka();\n"
                            }
                        ],
                        "toolUseId": "tooluse_jg4ren3BRMOYZTlK5VeEeg"
                    }
                }
            ]
        },
        {
            "role": "assistant",
            "content": [
                {
                    "text": "Now let's try again:"
                },
                {
                    "toolUse": {
                        "toolUseId": "tooluse_fuZgF06ATn2-0mG4p9OVmA",
                        "name": "restricted_shell",
                        "input": {
                            "command": "cd /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor && mvn clean test-compile 2>&1 | tail -20"
                        }
                    }
                }
            ]
        },
        {
            "role": "user",
            "content": [
                {
                    "toolResult": {
                        "toolUseId": "tooluse_fuZgF06ATn2-0mG4p9OVmA",
                        "status": "success",
                        "content": [
                            {
                                "text": "[INFO] Copying 0 resource from src/main/resources to target/classes\n[INFO] \n[INFO] --- compiler:3.11.0:compile (default-compile) @ sports-feed-processor-service ---\n[INFO] Changes detected - recompiling the module! :source\n[INFO] Compiling 22 source files with javac [debug release 17] to target/classes\n[INFO] \n[INFO] --- resources:3.3.1:testResources (default-testResources) @ sports-feed-processor-service ---\n[INFO] skip non existing resourceDirectory /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/test/resources\n[INFO] \n[INFO] --- compiler:3.11.0:testCompile (default-testCompile) @ sports-feed-processor-service ---\n[INFO] Changes detected - recompiling the module! :dependency\n[INFO] Compiling 1 source file with javac [debug release 17] to target/test-classes\n[INFO] /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/test/java/com/zen/lab/services/program/consumer/LiveFeedPojoListenerTest.java: /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/test/java/com/zen/lab/services/program/consumer/LiveFeedPojoListenerTest.java uses or overrides a deprecated API.\n[INFO] /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/test/java/com/zen/lab/services/program/consumer/LiveFeedPojoListenerTest.java: Recompile with -Xlint:deprecation for details.\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD SUCCESS\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time:  3.185 s\n[INFO] Finished at: 2026-01-29T05:48:43Z\n[INFO] ------------------------------------------------------------------------\n"
                            }
                        ]
                    }
                }
            ]
        },
        {
            "role": "assistant",
            "content": [
                {
                    "text": "Good! Now let's run the full verification:"
                },
                {
                    "toolUse": {
                        "toolUseId": "tooluse_-1aLsWblRpuoDkgB6oqZ5w",
                        "name": "restricted_shell",
                        "input": {
                            "command": "cd /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor && mvn clean verify 2>&1"
                        }
                    }
                }
            ]
        },
        {
            "role": "user",
            "content": [
                {
                    "toolResult": {
                        "toolUseId": "tooluse_-1aLsWblRpuoDkgB6oqZ5w",
                        "status": "success",
                        "content": [
                            {
                                "text": "[INFO] Scanning for projects...\n[INFO] \n[INFO] -----< com.zen.lab.services.program:sports-feed-processor-service >-----\n[INFO] Building sports-feed-processor-service 1.0\n[INFO]   from pom.xml\n[INFO] --------------------------------[ jar ]---------------------------------\n[INFO] \n[INFO] --- clean:3.3.2:clean (default-clean) @ sports-feed-processor-service ---\n[INFO] Deleting /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/target\n[INFO] \n[INFO] --- resources:3.3.1:resources (default-resources) @ sports-feed-processor-service ---\n[INFO] Copying 1 resource from src/main/resources to target/classes\n[INFO] Copying 0 resource from src/main/resources to target/classes\n[INFO] \n[INFO] --- compiler:3.11.0:compile (default-compile) @ sports-feed-processor-service ---\n[INFO] Changes detected - recompiling the module! :source\n[INFO] Compiling 22 source files with javac [debug release 17] to target/classes\n[INFO] \n[INFO] --- resources:3.3.1:testResources (default-testResources) @ sports-feed-processor-service ---\n[INFO] skip non existing resourceDirectory /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/test/resources\n[INFO] \n[INFO] --- compiler:3.11.0:testCompile (default-testCompile) @ sports-feed-processor-service ---\n[INFO] Changes detected - recompiling the module! :dependency\n[INFO] Compiling 1 source file with javac [debug release 17] to target/test-classes\n[INFO] /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/test/java/com/zen/lab/services/program/consumer/LiveFeedPojoListenerTest.java: /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/test/java/com/zen/lab/services/program/consumer/LiveFeedPojoListenerTest.java uses or overrides a deprecated API.\n[INFO] /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/src/test/java/com/zen/lab/services/program/consumer/LiveFeedPojoListenerTest.java: Recompile with -Xlint:deprecation for details.\n[INFO] \n[INFO] --- surefire:3.1.2:test (default-test) @ sports-feed-processor-service ---\n[INFO] Using auto detected provider org.apache.maven.surefire.junitplatform.JUnitPlatformProvider\n[INFO] \n[INFO] -------------------------------------------------------\n[INFO]  T E S T S\n[INFO] -------------------------------------------------------\n[INFO] Running com.zen.lab.services.program.consumer.LiveFeedPojoListenerTest\n05:48:50.807 [main] INFO kafka.utils.Log4jControllerRegistration$ -- Registered kafka:type=kafka.Log4jController MBean\n05:48:50.829 [main] INFO org.apache.zookeeper.server.ZooKeeperServer -- \n05:48:50.829 [main] INFO org.apache.zookeeper.server.ZooKeeperServer --   ______                  _                                          \n05:48:50.829 [main] INFO org.apache.zookeeper.server.ZooKeeperServer --  |___  /                 | |                                         \n05:48:50.830 [main] INFO org.apache.zookeeper.server.ZooKeeperServer --     / /    ___     ___   | | __   ___    ___   _ __     ___   _ __   \n05:48:50.830 [main] INFO org.apache.zookeeper.server.ZooKeeperServer --    / /    / _ \\   / _ \\  | |/ /  / _ \\  / _ \\ | '_ \\   / _ \\ | '__|\n05:48:50.830 [main] INFO org.apache.zookeeper.server.ZooKeeperServer --   / /__  | (_) | | (_) | |   <  |  __/ |  __/ | |_) | |  __/ | |    \n05:48:50.830 [main] INFO org.apache.zookeeper.server.ZooKeeperServer --  /_____|  \\___/   \\___/  |_|\\_\\  \\___|  \\___| | .__/   \\___| |_|\n05:48:50.830 [main] INFO org.apache.zookeeper.server.ZooKeeperServer --                                               | |                     \n05:48:50.830 [main] INFO org.apache.zookeeper.server.ZooKeeperServer --                                               |_|                     \n05:48:50.830 [main] INFO org.apache.zookeeper.server.ZooKeeperServer -- \n05:48:50.836 [main] INFO org.apache.zookeeper.server.ZooKeeperServer -- Server environment:zookeeper.version=3.8.3-6ad6d364c7c0bcf0de452d54ebefa3058098ab56, built on 2023-10-05 10:34 UTC\n05:48:50.836 [main] INFO org.apache.zookeeper.server.ZooKeeperServer -- Server environment:host.name=ip-10-4-212-203.us-west-2.compute.internal\n05:48:50.836 [main] INFO org.apache.zookeeper.server.ZooKeeperServer -- Server environment:java.version=17.0.17\n05:48:50.837 [main] INFO org.apache.zookeeper.server.ZooKeeperServer -- Server environment:java.vendor=Ubuntu\n05:48:50.837 [main] INFO org.apache.zookeeper.server.ZooKeeperServer -- Server environment:java.home=/usr/lib/jvm/java-17-openjdk-amd64\n05:48:50.837 [main] INFO org.apache.zookeeper.server.ZooKeeperServer -- Server environment:java.class.path=/tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/target/test-classes:/tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/target/classes:/fsx/ubuntu/.m2/repository/org/springframework/boot/spring-boot-starter-web/3.2.1/spring-boot-starter-web-3.2.1.jar:/fsx/ubuntu/.m2/repository/org/springframework/boot/spring-boot-starter/3.2.1/spring-boot-starter-3.2.1.jar:/fsx/ubuntu/.m2/repository/org/springframework/boot/spring-boot/3.2.1/spring-boot-3.2.1.jar:/fsx/ubuntu/.m2/repository/org/springframework/boot/spring-boot-autoconfigure/3.2.1/spring-boot-autoconfigure-3.2.1.jar:/fsx/ubuntu/.m2/repository/org/springframework/boot/spring-boot-starter-logging/3.2.1/spring-boot-starter-logging-3.2.1.jar:/fsx/ubuntu/.m2/repository/org/apache/logging/log4j/log4j-to-slf4j/2.21.1/log4j-to-slf4j-2.21.1.jar:/fsx/ubuntu/.m2/repository/org/apache/logging/log4j/log4j-api/2.21.1/log4j-api-2.21.1.jar:/fsx/ubuntu/.m2/repository/org/slf4j/jul-to-slf4j/2.0.9/jul-to-slf4j-2.0.9.jar:/fsx/ubuntu/.m2/repository/jakarta/annotation/jakarta.annotation-api/2.1.1/jakarta.annotation-api-2.1.1.jar:/fsx/ubuntu/.m2/repository/org/yaml/snakeyaml/2.2/snakeyaml-2.2.jar:/fsx/ubuntu/.m2/repository/org/springframework/boot/spring-boot-starter-json/3.2.1/spring-boot-starter-json-3.2.1.jar:/fsx/ubuntu/.m2/repository/com/fasterxml/jackson/datatype/jackson-datatype-jdk8/2.15.3/jackson-datatype-jdk8-2.15.3.jar:/fsx/ubuntu/.m2/repository/com/fasterxml/jackson/datatype/jackson-datatype-jsr310/2.15.3/jackson-datatype-jsr310-2.15.3.jar:/fsx/ubuntu/.m2/repository/com/fasterxml/jackson/module/jackson-module-parameter-names/2.15.3/jackson-module-parameter-names-2.15.3.jar:/fsx/ubuntu/.m2/repository/org/springframework/boot/spring-boot-starter-tomcat/3.2.1/spring-boot-starter-tomcat-3.2.1.jar:/fsx/ubuntu/.m2/repository/org/apache/tomcat/embed/tomcat-embed-core/10.1.17/tomcat-embed-core-10.1.17.jar:/fsx/ubuntu/.m2/repository/org/apache/tomcat/embed/tomcat-embed-el/10.1.17/tomcat-embed-el-10.1.17.jar:/fsx/ubuntu/.m2/repository/org/apache/tomcat/embed/tomcat-embed-websocket/10.1.17/tomcat-embed-websocket-10.1.17.jar:/fsx/ubuntu/.m2/repository/org/springframework/spring-web/6.1.2/spring-web-6.1.2.jar:/fsx/ubuntu/.m2/repository/org/springframework/spring-beans/6.1.2/spring-beans-6.1.2.jar:/fsx/ubuntu/.m2/repository/org/springframework/spring-webmvc/6.1.2/spring-webmvc-6.1.2.jar:/fsx/ubuntu/.m2/repository/org/springframework/spring-aop/6.1.2/spring-aop-6.1.2.jar:/fsx/ubuntu/.m2/repository/org/springframework/spring-expression/6.1.2/spring-expression-6.1.2.jar:/fsx/ubuntu/.m2/repository/org/springframework/boot/spring-boot-starter-actuator/3.2.1/spring-boot-starter-actuator-3.2.1.jar:/fsx/ubuntu/.m2/repository/org/springframework/boot/spring-boot-actuator-autoconfigure/3.2.1/spring-boot-actuator-autoconfigure-3.2.1.jar:/fsx/ubuntu/.m2/repository/org/springframework/boot/spring-boot-actuator/3.2.1/spring-boot-actuator-3.2.1.jar:/fsx/ubuntu/.m2/repository/io/micrometer/micrometer-observation/1.12.1/micrometer-observation-1.12.1.jar:/fsx/ubuntu/.m2/repository/io/micrometer/micrometer-commons/1.12.1/micrometer-commons-1.12.1.jar:/fsx/ubuntu/.m2/repository/io/micrometer/micrometer-jakarta9/1.12.1/micrometer-jakarta9-1.12.1.jar:/fsx/ubuntu/.m2/repository/org/springframework/kafka/spring-kafka/3.1.1/spring-kafka-3.1.1.jar:/fsx/ubuntu/.m2/repository/org/springframework/spring-context/6.1.2/spring-context-6.1.2.jar:/fsx/ubuntu/.m2/repository/org/springframework/spring-messaging/6.1.2/spring-messaging-6.1.2.jar:/fsx/ubuntu/.m2/repository/org/springframework/spring-tx/6.1.2/spring-tx-6.1.2.jar:/fsx/ubuntu/.m2/repository/org/springframework/retry/spring-retry/2.0.5/spring-retry-2.0.5.jar:/fsx/ubuntu/.m2/repository/org/apache/kafka/kafka-clients/3.6.1/kafka-clients-3.6.1.jar:/fsx/ubuntu/.m2/repository/com/github/luben/zstd-jni/1.5.5-1/zstd-jni-1.5.5-1.jar:/fsx/ubuntu/.m2/repository/org/lz4/lz4-java/1.8.0/lz4-java-1.8.0.jar:/fsx/ubuntu/.m2/repository/org/xerial/snappy/snappy-java/1.1.10.5/snappy-java-1.1.10.5.jar:/fsx/ubuntu/.m2/repository/org/slf4j/slf4j-api/2.0.9/slf4j-api-2.0.9.jar:/fsx/ubuntu/.m2/repository/com/google/code/findbugs/jsr305/3.0.2/jsr305-3.0.2.jar:/fsx/ubuntu/.m2/repository/org/springframework/cloud/spring-cloud-starter-openfeign/4.1.0/spring-cloud-starter-openfeign-4.1.0.jar:/fsx/ubuntu/.m2/repository/org/springframework/cloud/spring-cloud-starter/4.1.0/spring-cloud-starter-4.1.0.jar:/fsx/ubuntu/.m2/repository/org/springframework/cloud/spring-cloud-context/4.1.0/spring-cloud-context-4.1.0.jar:/fsx/ubuntu/.m2/repository/org/springframework/security/spring-security-rsa/1.1.1/spring-security-rsa-1.1.1.jar:/fsx/ubuntu/.m2/repository/org/bouncycastle/bcprov-jdk18on/1.74/bcprov-jdk18on-1.74.jar:/fsx/ubuntu/.m2/repository/org/springframework/cloud/spring-cloud-openfeign-core/4.1.0/spring-cloud-openfeign-core-4.1.0.jar:/fsx/ubuntu/.m2/repository/org/springframework/boot/spring-boot-starter-aop/3.2.1/spring-boot-starter-aop-3.2.1.jar:/fsx/ubuntu/.m2/repository/io/github/openfeign/form/feign-form-spring/3.8.0/feign-form-spring-3.8.0.jar:/fsx/ubuntu/.m2/repository/io/github/openfeign/form/feign-form/3.8.0/feign-form-3.8.0.jar:/fsx/ubuntu/.m2/repository/commons-fileupload/commons-fileupload/1.5/commons-fileupload-1.5.jar:/fsx/ubuntu/.m2/repository/org/springframework/cloud/spring-cloud-commons/4.1.0/spring-cloud-commons-4.1.0.jar:/fsx/ubuntu/.m2/repository/org/springframework/security/spring-security-crypto/6.2.1/spring-security-crypto-6.2.1.jar:/fsx/ubuntu/.m2/repository/io/github/openfeign/feign-core/13.1/feign-core-13.1.jar:/fsx/ubuntu/.m2/repository/io/github/openfeign/feign-slf4j/13.1/feign-slf4j-13.1.jar:/fsx/ubuntu/.m2/repository/net/logstash/logback/logstash-logback-encoder/7.4/logstash-logback-encoder-7.4.jar:/fsx/ubuntu/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.15.3/jackson-databind-2.15.3.jar:/fsx/ubuntu/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.15.3/jackson-annotations-2.15.3.jar:/fsx/ubuntu/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.15.3/jackson-core-2.15.3.jar:/fsx/ubuntu/.m2/repository/org/aspectj/aspectjweaver/1.9.21/aspectjweaver-1.9.21.jar:/fsx/ubuntu/.m2/repository/io/micrometer/micrometer-registry-prometheus/1.12.1/micrometer-registry-prometheus-1.12.1.jar:/fsx/ubuntu/.m2/repository/io/micrometer/micrometer-core/1.12.1/micrometer-core-1.12.1.jar:/fsx/ubuntu/.m2/repository/org/hdrhistogram/HdrHistogram/2.1.12/HdrHistogram-2.1.12.jar:/fsx/ubuntu/.m2/repository/org/latencyutils/LatencyUtils/2.0.3/LatencyUtils-2.0.3.jar:/fsx/ubuntu/.m2/repository/io/prometheus/simpleclient_common/0.16.0/simpleclient_common-0.16.0.jar:/fsx/ubuntu/.m2/repository/io/prometheus/simpleclient/0.16.0/simpleclient-0.16.0.jar:/fsx/ubuntu/.m2/repository/io/prometheus/simpleclient_tracer_otel/0.16.0/simpleclient_tracer_otel-0.16.0.jar:/fsx/ubuntu/.m2/repository/io/prometheus/simpleclient_tracer_common/0.16.0/simpleclient_tracer_common-0.16.0.jar:/fsx/ubuntu/.m2/repository/io/prometheus/simpleclient_tracer_otel_agent/0.16.0/simpleclient_tracer_otel_agent-0.16.0.jar:/fsx/ubuntu/.m2/repository/org/springframework/kafka/spring-kafka-test/3.1.1/spring-kafka-test-3.1.1.jar:/fsx/ubuntu/.m2/repository/org/springframework/spring-test/6.1.2/spring-test-6.1.2.jar:/fsx/ubuntu/.m2/repository/org/apache/zookeeper/zookeeper/3.8.3/zookeeper-3.8.3.jar:/fsx/ubuntu/.m2/repository/org/apache/zookeeper/zookeeper-jute/3.8.3/zookeeper-jute-3.8.3.jar:/fsx/ubuntu/.m2/repository/org/apache/yetus/audience-annotations/0.12.0/audience-annotations-0.12.0.jar:/fsx/ubuntu/.m2/repository/io/netty/netty-handler/4.1.104.Final/netty-handler-4.1.104.Final.jar:/fsx/ubuntu/.m2/repository/io/netty/netty-common/4.1.104.Final/netty-common-4.1.104.Final.jar:/fsx/ubuntu/.m2/repository/io/netty/netty-resolver/4.1.104.Final/netty-resolver-4.1.104.Final.jar:/fsx/ubuntu/.m2/repository/io/netty/netty-buffer/4.1.104.Final/netty-buffer-4.1.104.Final.jar:/fsx/ubuntu/.m2/repository/io/netty/netty-transport/4.1.104.Final/netty-transport-4.1.104.Final.jar:/fsx/ubuntu/.m2/repository/io/netty/netty-transport-native-unix-common/4.1.104.Final/netty-transport-native-unix-common-4.1.104.Final.jar:/fsx/ubuntu/.m2/repository/io/netty/netty-codec/4.1.104.Final/netty-codec-4.1.104.Final.jar:/fsx/ubuntu/.m2/repository/io/netty/netty-transport-native-epoll/4.1.104.Final/netty-transport-native-epoll-4.1.104.Final.jar:/fsx/ubuntu/.m2/repository/io/netty/netty-transport-classes-epoll/4.1.104.Final/netty-transport-classes-epoll-4.1.104.Final.jar:/fsx/ubuntu/.m2/repository/ch/qos/logback/logback-core/1.4.14/logback-core-1.4.14.jar:/fsx/ubuntu/.m2/repository/ch/qos/logback/logback-classic/1.4.14/logback-classic-1.4.14.jar:/fsx/ubuntu/.m2/repository/commons-io/commons-io/2.11.0/commons-io-2.11.0.jar:/fsx/ubuntu/.m2/repository/org/apache/kafka/kafka-clients/3.6.1/kafka-clients-3.6.1-test.jar:/fsx/ubuntu/.m2/repository/org/apache/kafka/kafka-metadata/3.6.1/kafka-metadata-3.6.1.jar:/fsx/ubuntu/.m2/repository/org/apache/kafka/kafka-raft/3.6.1/kafka-raft-3.6.1.jar:/fsx/ubuntu/.m2/repository/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar:/fsx/ubuntu/.m2/repository/org/apache/kafka/kafka-server-common/3.6.1/kafka-server-common-3.6.1.jar:/fsx/ubuntu/.m2/repository/net/sf/jopt-simple/jopt-simple/5.0.4/jopt-simple-5.0.4.jar:/fsx/ubuntu/.m2/repository/org/pcollections/pcollections/4.0.1/pcollections-4.0.1.jar:/fsx/ubuntu/.m2/repository/org/apache/kafka/kafka-server-common/3.6.1/kafka-server-common-3.6.1-test.jar:/fsx/ubuntu/.m2/repository/org/apache/kafka/kafka-streams-test-utils/3.6.1/kafka-streams-test-utils-3.6.1.jar:/fsx/ubuntu/.m2/repository/org/apache/kafka/kafka-streams/3.6.1/kafka-streams-3.6.1.jar:/fsx/ubuntu/.m2/repository/org/rocksdb/rocksdbjni/7.9.2/rocksdbjni-7.9.2.jar:/fsx/ubuntu/.m2/repository/org/apache/kafka/kafka_2.13/3.6.1/kafka_2.13-3.6.1.jar:/fsx/ubuntu/.m2/repository/org/scala-lang/scala-library/2.13.11/scala-library-2.13.11.jar:/fsx/ubuntu/.m2/repository/org/apache/kafka/kafka-group-coordinator/3.6.1/kafka-group-coordinator-3.6.1.jar:/fsx/ubuntu/.m2/repository/org/apache/kafka/kafka-storage-api/3.6.1/kafka-storage-api-3.6.1.jar:/fsx/ubuntu/.m2/repository/org/apache/kafka/kafka-tools-api/3.6.1/kafka-tools-api-3.6.1.jar:/fsx/ubuntu/.m2/repository/org/apache/kafka/kafka-storage/3.6.1/kafka-storage-3.6.1.jar:/fsx/ubuntu/.m2/repository/com/github/ben-manes/caffeine/caffeine/3.1.8/caffeine-3.1.8.jar:/fsx/ubuntu/.m2/repository/org/checkerframework/checker-qual/3.37.0/checker-qual-3.37.0.jar:/fsx/ubuntu/.m2/repository/com/google/errorprone/error_prone_annotations/2.21.1/error_prone_annotations-2.21.1.jar:/fsx/ubuntu/.m2/repository/net/sourceforge/argparse4j/argparse4j/0.7.0/argparse4j-0.7.0.jar:/fsx/ubuntu/.m2/repository/commons-validator/commons-validator/1.7/commons-validator-1.7.jar:/fsx/ubuntu/.m2/repository/commons-beanutils/commons-beanutils/1.9.4/commons-beanutils-1.9.4.jar:/fsx/ubuntu/.m2/repository/commons-digester/commons-digester/2.1/commons-digester-2.1.jar:/fsx/ubuntu/.m2/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/fsx/ubuntu/.m2/repository/com/fasterxml/jackson/module/jackson-module-scala_2.13/2.15.3/jackson-module-scala_2.13-2.15.3.jar:/fsx/ubuntu/.m2/repository/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar:/fsx/ubuntu/.m2/repository/com/fasterxml/jackson/dataformat/jackson-dataformat-csv/2.15.3/jackson-dataformat-csv-2.15.3.jar:/fsx/ubuntu/.m2/repository/org/bitbucket/b_c/jose4j/0.9.3/jose4j-0.9.3.jar:/fsx/ubuntu/.m2/repository/org/scala-lang/modules/scala-collection-compat_2.13/2.10.0/scala-collection-compat_2.13-2.10.0.jar:/fsx/ubuntu/.m2/repository/org/scala-lang/modules/scala-java8-compat_2.13/1.0.2/scala-java8-compat_2.13-1.0.2.jar:/fsx/ubuntu/.m2/repository/org/scala-lang/scala-reflect/2.13.11/scala-reflect-2.13.11.jar:/fsx/ubuntu/.m2/repository/com/typesafe/scala-logging/scala-logging_2.13/3.9.4/scala-logging_2.13-3.9.4.jar:/fsx/ubuntu/.m2/repository/io/dropwizard/metrics/metrics-core/4.2.23/metrics-core-4.2.23.jar:/fsx/ubuntu/.m2/repository/commons-cli/commons-cli/1.4/commons-cli-1.4.jar:/fsx/ubuntu/.m2/repository/org/apache/kafka/kafka_2.13/3.6.1/kafka_2.13-3.6.1-test.jar:/fsx/ubuntu/.m2/repository/org/junit/jupiter/junit-jupiter-api/5.10.1/junit-jupiter-api-5.10.1.jar:/fsx/ubuntu/.m2/repository/org/opentest4j/opentest4j/1.3.0/opentest4j-1.3.0.jar:/fsx/ubuntu/.m2/repository/org/junit/platform/junit-platform-commons/1.10.1/junit-platform-commons-1.10.1.jar:/fsx/ubuntu/.m2/repository/org/junit/platform/junit-platform-launcher/1.10.1/junit-platform-launcher-1.10.1.jar:/fsx/ubuntu/.m2/repository/org/assertj/assertj-core/3.24.2/assertj-core-3.24.2.jar:/fsx/ubuntu/.m2/repository/net/bytebuddy/byte-buddy/1.14.10/byte-buddy-1.14.10.jar:/fsx/ubuntu/.m2/repository/org/hamcrest/hamcrest-all/1.3/hamcrest-all-1.3.jar:/fsx/ubuntu/.m2/repository/org/junit/vintage/junit-vintage-engine/5.10.1/junit-vintage-engine-5.10.1.jar:/fsx/ubuntu/.m2/repository/org/junit/platform/junit-platform-engine/1.10.1/junit-platform-engine-1.10.1.jar:/fsx/ubuntu/.m2/repository/junit/junit/4.13.2/junit-4.13.2.jar:/fsx/ubuntu/.m2/repository/org/hamcrest/hamcrest-core/2.2/hamcrest-core-2.2.jar:/fsx/ubuntu/.m2/repository/org/apiguardian/apiguardian-api/1.1.2/apiguardian-api-1.1.2.jar:/fsx/ubuntu/.m2/repository/org/springframework/boot/spring-boot-starter-test/3.2.1/spring-boot-starter-test-3.2.1.jar:/fsx/ubuntu/.m2/repository/org/springframework/boot/spring-boot-test/3.2.1/spring-boot-test-3.2.1.jar:/fsx/ubuntu/.m2/repository/org/springframework/boot/spring-boot-test-autoconfigure/3.2.1/spring-boot-test-autoconfigure-3.2.1.jar:/fsx/ubuntu/.m2/repository/com/jayway/jsonpath/json-path/2.8.0/json-path-2.8.0.jar:/fsx/ubuntu/.m2/repository/jakarta/xml/bind/jakarta.xml.bind-api/4.0.1/jakarta.xml.bind-api-4.0.1.jar:/fsx/ubuntu/.m2/repository/jakarta/activation/jakarta.activation-api/2.1.2/jakarta.activation-api-2.1.2.jar:/fsx/ubuntu/.m2/repository/net/minidev/json-smart/2.5.0/json-smart-2.5.0.jar:/fsx/ubuntu/.m2/repository/net/minidev/accessors-smart/2.5.0/accessors-smart-2.5.0.jar:/fsx/ubuntu/.m2/repository/org/ow2/asm/asm/9.3/asm-9.3.jar:/fsx/ubuntu/.m2/repository/org/awaitility/awaitility/4.2.0/awaitility-4.2.0.jar:/fsx/ubuntu/.m2/repository/org/hamcrest/hamcrest/2.2/hamcrest-2.2.jar:/fsx/ubuntu/.m2/repository/org/junit/jupiter/junit-jupiter/5.10.1/junit-jupiter-5.10.1.jar:/fsx/ubuntu/.m2/repository/org/junit/jupiter/junit-jupiter-params/5.10.1/junit-jupiter-params-5.10.1.jar:/fsx/ubuntu/.m2/repository/org/junit/jupiter/junit-jupiter-engine/5.10.1/junit-jupiter-engine-5.10.1.jar:/fsx/ubuntu/.m2/repository/org/mockito/mockito-core/5.7.0/mockito-core-5.7.0.jar:/fsx/ubuntu/.m2/repository/net/bytebuddy/byte-buddy-agent/1.14.10/byte-buddy-agent-1.14.10.jar:/fsx/ubuntu/.m2/repository/org/objenesis/objenesis/3.3/objenesis-3.3.jar:/fsx/ubuntu/.m2/repository/org/mockito/mockito-junit-jupiter/5.7.0/mockito-junit-jupiter-5.7.0.jar:/fsx/ubuntu/.m2/repository/org/skyscreamer/jsonassert/1.5.1/jsonassert-1.5.1.jar:/fsx/ubuntu/.m2/repository/com/vaadin/external/google/android-json/0.0.20131108.vaadin1/android-json-0.0.20131108.vaadin1.jar:/fsx/ubuntu/.m2/repository/org/springframework/spring-core/6.1.2/spring-core-6.1.2.jar:/fsx/ubuntu/.m2/repository/org/springframework/spring-jcl/6.1.2/spring-jcl-6.1.2.jar:/fsx/ubuntu/.m2/repository/org/xmlunit/xmlunit-core/2.9.1/xmlunit-core-2.9.1.jar:\n05:48:50.838 [main] INFO org.apache.zookeeper.server.ZooKeeperServer -- Server environment:java.library.path=/usr/local/cuda-12.9/lib:/usr/local/cuda-12.9/lib64:/usr/local/cuda-12.9:/usr/local/cuda-12.9/targets/x86_64-linux/lib/:/usr/local/cuda-12.9/extras/CUPTI/lib64:/opt/amazon/openmpi/lib:/usr/local/lib:/usr/lib:/usr/local/cuda-12.9/lib:/usr/local/cuda-12.9/lib64:/usr/local/cuda-12.9:/usr/local/cuda-12.9/targets/x86_64-linux/lib/:/usr/local/cuda-12.9/extras/CUPTI/lib64:/opt/amazon/openmpi/lib:/usr/local/lib:/usr/lib:/usr/local/cuda-12.9/lib:/usr/local/cuda-12.9/lib64:/usr/local/cuda-12.9:/usr/local/cuda-12.9/targets/x86_64-linux/lib/:/usr/local/cuda-12.9/extras/CUPTI/lib64:/opt/amazon/openmpi/lib:/usr/local/lib:/usr/lib:/usr/local/cuda-12.9/lib:/usr/local/cuda-12.9/lib64:/usr/local/cuda-12.9:/usr/local/cuda-12.9/targets/x86_64-linux/lib/:/usr/local/cuda-12.9/extras/CUPTI/lib64:/opt/amazon/openmpi/lib:/usr/local/lib:/usr/lib:/usr/java/packages/lib:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib\n05:48:50.839 [main] INFO org.apache.zookeeper.server.ZooKeeperServer -- Server environment:java.io.tmpdir=/tmp\n05:48:50.839 [main] INFO org.apache.zookeeper.server.ZooKeeperServer -- Server environment:java.compiler=<NA>\n05:48:50.839 [main] INFO org.apache.zookeeper.server.ZooKeeperServer -- Server environment:os.name=Linux\n05:48:50.839 [main] INFO org.apache.zookeeper.server.ZooKeeperServer -- Server environment:os.arch=amd64\n05:48:50.839 [main] INFO org.apache.zookeeper.server.ZooKeeperServer -- Server environment:os.version=6.8.0-1044-aws\n05:48:50.839 [main] INFO org.apache.zookeeper.server.ZooKeeperServer -- Server environment:user.name=ubuntu\n05:48:50.839 [main] INFO org.apache.zookeeper.server.ZooKeeperServer -- Server environment:user.home=/fsx/ubuntu\n05:48:50.839 [main] INFO org.apache.zookeeper.server.ZooKeeperServer -- Server environment:user.dir=/tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor\n05:48:50.839 [main] INFO org.apache.zookeeper.server.ZooKeeperServer -- Server environment:os.memory.free=2016MB\n05:48:50.839 [main] INFO org.apache.zookeeper.server.ZooKeeperServer -- Server environment:os.memory.max=30688MB\n05:48:50.839 [main] INFO org.apache.zookeeper.server.ZooKeeperServer -- Server environment:os.memory.total=2080MB\n05:48:50.839 [main] INFO org.apache.zookeeper.server.ZooKeeperServer -- zookeeper.enableEagerACLCheck = false\n05:48:50.840 [main] INFO org.apache.zookeeper.server.ZooKeeperServer -- zookeeper.digest.enabled = true\n05:48:50.841 [main] INFO org.apache.zookeeper.server.ZooKeeperServer -- zookeeper.closeSessionTxn.enabled = true\n05:48:50.841 [main] INFO org.apache.zookeeper.server.ZooKeeperServer -- zookeeper.flushDelay = 0 ms\n05:48:50.841 [main] INFO org.apache.zookeeper.server.ZooKeeperServer -- zookeeper.maxWriteQueuePollTime = 0 ms\n05:48:50.841 [main] INFO org.apache.zookeeper.server.ZooKeeperServer -- zookeeper.maxBatchSize=1000\n05:48:50.841 [main] INFO org.apache.zookeeper.server.ZooKeeperServer -- zookeeper.intBufferStartingSizeBytes = 1024\n05:48:50.843 [main] INFO org.apache.zookeeper.server.persistence.FileTxnSnapLog -- zookeeper.snapshot.trust.empty : false\n05:48:50.854 [main] INFO org.apache.zookeeper.server.watch.WatchManagerFactory -- Using org.apache.zookeeper.server.watch.WatchManager as watch manager\n05:48:50.854 [main] INFO org.apache.zookeeper.server.watch.WatchManagerFactory -- Using org.apache.zookeeper.server.watch.WatchManager as watch manager\n05:48:50.854 [main] INFO org.apache.zookeeper.server.ZKDatabase -- zookeeper.snapshotSizeFactor = 0.33\n05:48:50.854 [main] INFO org.apache.zookeeper.server.ZKDatabase -- zookeeper.commitLogCount=500\n05:48:50.856 [main] INFO org.apache.zookeeper.server.BlueThrottle -- Weighed connection throttling is disabled\n05:48:50.858 [main] INFO org.apache.zookeeper.server.ZooKeeperServer -- minSessionTimeout set to 1600 ms\n05:48:50.858 [main] INFO org.apache.zookeeper.server.ZooKeeperServer -- maxSessionTimeout set to 16000 ms\n05:48:50.859 [main] INFO org.apache.zookeeper.server.ResponseCache -- getData response cache size is initialized with value 400.\n05:48:50.859 [main] INFO org.apache.zookeeper.server.ResponseCache -- getChildren response cache size is initialized with value 400.\n05:48:50.860 [main] INFO org.apache.zookeeper.server.util.RequestPathMetricsCollector -- zookeeper.pathStats.slotCapacity = 60\n05:48:50.860 [main] INFO org.apache.zookeeper.server.util.RequestPathMetricsCollector -- zookeeper.pathStats.slotDuration = 15\n05:48:50.860 [main] INFO org.apache.zookeeper.server.util.RequestPathMetricsCollector -- zookeeper.pathStats.maxDepth = 6\n05:48:50.860 [main] INFO org.apache.zookeeper.server.util.RequestPathMetricsCollector -- zookeeper.pathStats.initialDelay = 5\n05:48:50.860 [main] INFO org.apache.zookeeper.server.util.RequestPathMetricsCollector -- zookeeper.pathStats.delay = 5\n05:48:50.860 [main] INFO org.apache.zookeeper.server.util.RequestPathMetricsCollector -- zookeeper.pathStats.enabled = false\n05:48:50.861 [main] INFO org.apache.zookeeper.server.ZooKeeperServer -- The max bytes for all large requests are set to 104857600\n05:48:50.861 [main] INFO org.apache.zookeeper.server.ZooKeeperServer -- The large request threshold is set to -1\n05:48:50.862 [main] INFO org.apache.zookeeper.server.AuthenticationHelper -- zookeeper.enforce.auth.enabled = false\n05:48:50.862 [main] INFO org.apache.zookeeper.server.AuthenticationHelper -- zookeeper.enforce.auth.schemes = []\n05:48:50.862 [main] INFO org.apache.zookeeper.server.ZooKeeperServer -- Created server with tickTime 800 ms minSessionTimeout 1600 ms maxSessionTimeout 16000 ms clientPortListenBacklog -1 datadir /tmp/kafka-3499986870870487576/version-2 snapdir /tmp/kafka-7458732297543784461/version-2\n05:48:50.867 [main] WARN org.apache.zookeeper.server.ServerCnxnFactory -- maxCnxns is not configured, using default value 0.\n05:48:50.868 [main] INFO org.apache.zookeeper.server.NIOServerCnxnFactory -- Configuring NIO connection handler with 10s sessionless connection timeout, 6 selector thread(s), 192 worker threads, and 64 kB direct buffers.\n05:48:50.874 [main] INFO org.apache.zookeeper.server.NIOServerCnxnFactory -- binding to port /127.0.0.1:0\n05:48:50.889 [main] INFO org.apache.zookeeper.server.persistence.SnapStream -- zookeeper.snapshot.compression.method = CHECKED\n05:48:50.889 [main] INFO org.apache.zookeeper.server.persistence.FileTxnSnapLog -- Snapshotting: 0x0 to /tmp/kafka-7458732297543784461/version-2/snapshot.0\n05:48:50.892 [main] INFO org.apache.zookeeper.server.ZKDatabase -- Snapshot loaded in 14 ms, highest zxid is 0x0, digest is 1371985504\n05:48:50.893 [main] INFO org.apache.zookeeper.server.persistence.FileTxnSnapLog -- Snapshotting: 0x0 to /tmp/kafka-7458732297543784461/version-2/snapshot.0\n05:48:50.893 [main] INFO org.apache.zookeeper.server.ZooKeeperServer -- Snapshot taken in 0 ms\n05:48:50.900 [main] INFO org.apache.zookeeper.server.RequestThrottler -- zookeeper.request_throttler.shutdownTimeout = 10000 ms\n05:48:50.906 [ProcessThread(sid:0 cport:41533):] INFO org.apache.zookeeper.server.PrepRequestProcessor -- PrepRequestProcessor (sid:0) started, reconfigEnabled=false\n05:48:51.089 [main] INFO kafka.server.KafkaConfig -- KafkaConfig values: \n\tadvertised.listeners = null\n\talter.config.policy.class.name = null\n\talter.log.dirs.replication.quota.window.num = 11\n\talter.log.dirs.replication.quota.window.size.seconds = 1\n\tauthorizer.class.name = \n\tauto.create.topics.enable = true\n\tauto.include.jmx.reporter = true\n\tauto.leader.rebalance.enable = true\n\tbackground.threads = 2\n\tbroker.heartbeat.interval.ms = 2000\n\tbroker.id = 0\n\tbroker.id.generation.enable = true\n\tbroker.rack = null\n\tbroker.session.timeout.ms = 9000\n\tclient.quota.callback.class = null\n\tcompression.type = producer\n\tconnection.failed.authentication.delay.ms = 100\n\tconnections.max.idle.ms = 600000\n\tconnections.max.reauth.ms = 0\n\tcontrol.plane.listener.name = null\n\tcontrolled.shutdown.enable = true\n\tcontrolled.shutdown.max.retries = 3\n\tcontrolled.shutdown.retry.backoff.ms = 100\n\tcontroller.listener.names = null\n\tcontroller.quorum.append.linger.ms = 25\n\tcontroller.quorum.election.backoff.max.ms = 1000\n\tcontroller.quorum.election.timeout.ms = 1000\n\tcontroller.quorum.fetch.timeout.ms = 2000\n\tcontroller.quorum.request.timeout.ms = 2000\n\tcontroller.quorum.retry.backoff.ms = 20\n\tcontroller.quorum.voters = []\n\tcontroller.quota.window.num = 11\n\tcontroller.quota.window.size.seconds = 1\n\tcontroller.socket.timeout.ms = 1000\n\tcreate.topic.policy.class.name = null\n\tdefault.replication.factor = 2\n\tdelegation.token.expiry.check.interval.ms = 3600000\n\tdelegation.token.expiry.time.ms = 86400000\n\tdelegation.token.master.key = null\n\tdelegation.token.max.lifetime.ms = 604800000\n\tdelegation.token.secret.key = null\n\tdelete.records.purgatory.purge.interval.requests = 1\n\tdelete.topic.enable = true\n\tearly.start.listeners = null\n\tfetch.max.bytes = 57671680\n\tfetch.purgatory.purge.interval.requests = 1000\n\tgroup.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.RangeAssignor]\n\tgroup.consumer.heartbeat.interval.ms = 5000\n\tgroup.consumer.max.heartbeat.interval.ms = 15000\n\tgroup.consumer.max.session.timeout.ms = 60000\n\tgroup.consumer.max.size = 2147483647\n\tgroup.consumer.min.heartbeat.interval.ms = 5000\n\tgroup.consumer.min.session.timeout.ms = 45000\n\tgroup.consumer.session.timeout.ms = 45000\n\tgroup.coordinator.new.enable = false\n\tgroup.coordinator.threads = 1\n\tgroup.initial.rebalance.delay.ms = 0\n\tgroup.max.session.timeout.ms = 1800000\n\tgroup.max.size = 2147483647\n\tgroup.min.session.timeout.ms = 6000\n\tinitial.broker.registration.timeout.ms = 60000\n\tinter.broker.listener.name = null\n\tinter.broker.protocol.version = 3.6-IV2\n\tkafka.metrics.polling.interval.secs = 10\n\tkafka.metrics.reporters = []\n\tleader.imbalance.check.interval.seconds = 300\n\tleader.imbalance.per.broker.percentage = 10\n\tlistener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL\n\tlisteners = PLAINTEXT://localhost:0\n\tlog.cleaner.backoff.ms = 15000\n\tlog.cleaner.dedupe.buffer.size = 2097152\n\tlog.cleaner.delete.retention.ms = 86400000\n\tlog.cleaner.enable = true\n\tlog.cleaner.io.buffer.load.factor = 0.9\n\tlog.cleaner.io.buffer.size = 524288\n\tlog.cleaner.io.max.bytes.per.second = 1.7976931348623157E308\n\tlog.cleaner.max.compaction.lag.ms = 9223372036854775807\n\tlog.cleaner.min.cleanable.ratio = 0.5\n\tlog.cleaner.min.compaction.lag.ms = 0\n\tlog.cleaner.threads = 1\n\tlog.cleanup.policy = [delete]\n\tlog.dir = /tmp/spring.kafka.bca7ba27-5e93-400e-9aae-86960892bff71860864339420333442\n\tlog.dirs = null\n\tlog.flush.interval.messages = 9223372036854775807\n\tlog.flush.interval.ms = null\n\tlog.flush.offset.checkpoint.interval.ms = 60000\n\tlog.flush.scheduler.interval.ms = 9223372036854775807\n\tlog.flush.start.offset.checkpoint.interval.ms = 60000\n\tlog.index.interval.bytes = 4096\n\tlog.index.size.max.bytes = 10485760\n\tlog.local.retention.bytes = -2\n\tlog.local.retention.ms = -2\n\tlog.message.downconversion.enable = true\n\tlog.message.format.version = 3.0-IV1\n\tlog.message.timestamp.after.max.ms = 9223372036854775807\n\tlog.message.timestamp.before.max.ms = 9223372036854775807\n\tlog.message.timestamp.difference.max.ms = 9223372036854775807\n\tlog.message.timestamp.type = CreateTime\n\tlog.preallocate = false\n\tlog.retention.bytes = -1\n\tlog.retention.check.interval.ms = 300000\n\tlog.retention.hours = 168\n\tlog.retention.minutes = null\n\tlog.retention.ms = null\n\tlog.roll.hours = 168\n\tlog.roll.jitter.hours = 0\n\tlog.roll.jitter.ms = null\n\tlog.roll.ms = null\n\tlog.segment.bytes = 1073741824\n\tlog.segment.delete.delay.ms = 1000\n\tmax.connection.creation.rate = 2147483647\n\tmax.connections = 2147483647\n\tmax.connections.per.ip = 2147483647\n\tmax.connections.per.ip.overrides = \n\tmax.incremental.fetch.session.cache.slots = 1000\n\tmessage.max.bytes = 1048588\n\tmetadata.log.dir = null\n\tmetadata.log.max.record.bytes.between.snapshots = 20971520\n\tmetadata.log.max.snapshot.interval.ms = 3600000\n\tmetadata.log.segment.bytes = 1073741824\n\tmetadata.log.segment.min.bytes = 8388608\n\tmetadata.log.segment.ms = 604800000\n\tmetadata.max.idle.interval.ms = 500\n\tmetadata.max.retention.bytes = 104857600\n\tmetadata.max.retention.ms = 604800000\n\tmetric.reporters = []\n\tmetrics.num.samples = 2\n\tmetrics.recording.level = INFO\n\tmetrics.sample.window.ms = 30000\n\tmin.insync.replicas = 1\n\tnode.id = 0\n\tnum.io.threads = 8\n\tnum.network.threads = 2\n\tnum.partitions = 2\n\tnum.recovery.threads.per.data.dir = 1\n\tnum.replica.alter.log.dirs.threads = null\n\tnum.replica.fetchers = 1\n\toffset.metadata.max.bytes = 4096\n\toffsets.commit.required.acks = -1\n\toffsets.commit.timeout.ms = 5000\n\toffsets.load.buffer.size = 5242880\n\toffsets.retention.check.interval.ms = 600000\n\toffsets.retention.minutes = 10080\n\toffsets.topic.compression.codec = 0\n\toffsets.topic.num.partitions = 5\n\toffsets.topic.replication.factor = 1\n\toffsets.topic.segment.bytes = 104857600\n\tpassword.encoder.cipher.algorithm = AES/CBC/PKCS5Padding\n\tpassword.encoder.iterations = 4096\n\tpassword.encoder.key.length = 128\n\tpassword.encoder.keyfactory.algorithm = null\n\tpassword.encoder.old.secret = null\n\tpassword.encoder.secret = null\n\tprincipal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder\n\tprocess.roles = []\n\tproducer.id.expiration.check.interval.ms = 600000\n\tproducer.id.expiration.ms = 86400000\n\tproducer.purgatory.purge.interval.requests = 1000\n\tqueued.max.request.bytes = -1\n\tqueued.max.requests = 500\n\tquota.window.num = 11\n\tquota.window.size.seconds = 1\n\tremote.log.index.file.cache.total.size.bytes = 1073741824\n\tremote.log.manager.task.interval.ms = 30000\n\tremote.log.manager.task.retry.backoff.max.ms = 30000\n\tremote.log.manager.task.retry.backoff.ms = 500\n\tremote.log.manager.task.retry.jitter = 0.2\n\tremote.log.manager.thread.pool.size = 10\n\tremote.log.metadata.custom.metadata.max.bytes = 128\n\tremote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager\n\tremote.log.metadata.manager.class.path = null\n\tremote.log.metadata.manager.impl.prefix = rlmm.config.\n\tremote.log.metadata.manager.listener.name = null\n\tremote.log.reader.max.pending.tasks = 100\n\tremote.log.reader.threads = 10\n\tremote.log.storage.manager.class.name = null\n\tremote.log.storage.manager.class.path = null\n\tremote.log.storage.manager.impl.prefix = rsm.config.\n\tremote.log.storage.system.enable = false\n\treplica.fetch.backoff.ms = 1000\n\treplica.fetch.max.bytes = 1048576\n\treplica.fetch.min.bytes = 1\n\treplica.fetch.response.max.bytes = 10485760\n\treplica.fetch.wait.max.ms = 500\n\treplica.high.watermark.checkpoint.interval.ms = 9223372036854775807\n\treplica.lag.time.max.ms = 30000\n\treplica.selector.class = null\n\treplica.socket.receive.buffer.bytes = 65536\n\treplica.socket.timeout.ms = 1000\n\treplication.quota.window.num = 11\n\treplication.quota.window.size.seconds = 1\n\trequest.timeout.ms = 30000\n\treserved.broker.max.id = 1000\n\tsasl.client.callback.handler.class = null\n\tsasl.enabled.mechanisms = [GSSAPI]\n\tsasl.jaas.config = null\n\tsasl.kerberos.kinit.cmd = /usr/bin/kinit\n\tsasl.kerberos.min.time.before.relogin = 60000\n\tsasl.kerberos.principal.to.local.rules = [DEFAULT]\n\tsasl.kerberos.service.name = null\n\tsasl.kerberos.ticket.renew.jitter = 0.05\n\tsasl.kerberos.ticket.renew.window.factor = 0.8\n\tsasl.login.callback.handler.class = null\n\tsasl.login.class = null\n\tsasl.login.connect.timeout.ms = null\n\tsasl.login.read.timeout.ms = null\n\tsasl.login.refresh.buffer.seconds = 300\n\tsasl.login.refresh.min.period.seconds = 60\n\tsasl.login.refresh.window.factor = 0.8\n\tsasl.login.refresh.window.jitter = 0.05\n\tsasl.login.retry.backoff.max.ms = 10000\n\tsasl.login.retry.backoff.ms = 100\n\tsasl.mechanism.controller.protocol = GSSAPI\n\tsasl.mechanism.inter.broker.protocol = GSSAPI\n\tsasl.oauthbearer.clock.skew.seconds = 30\n\tsasl.oauthbearer.expected.audience = null\n\tsasl.oauthbearer.expected.issuer = null\n\tsasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000\n\tsasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000\n\tsasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100\n\tsasl.oauthbearer.jwks.endpoint.url = null\n\tsasl.oauthbearer.scope.claim.name = scope\n\tsasl.oauthbearer.sub.claim.name = sub\n\tsasl.oauthbearer.token.endpoint.url = null\n\tsasl.server.callback.handler.class = null\n\tsasl.server.max.receive.size = 524288\n\tsecurity.inter.broker.protocol = PLAINTEXT\n\tsecurity.providers = null\n\tserver.max.startup.time.ms = 9223372036854775807\n\tsocket.connection.setup.timeout.max.ms = 30000\n\tsocket.connection.setup.timeout.ms = 10000\n\tsocket.listen.backlog.size = 50\n\tsocket.receive.buffer.bytes = 102400\n\tsocket.request.max.bytes = 104857600\n\tsocket.send.buffer.bytes = 102400\n\tssl.cipher.suites = []\n\tssl.client.auth = none\n\tssl.enabled.protocols = [TLSv1.2, TLSv1.3]\n\tssl.endpoint.identification.algorithm = https\n\tssl.engine.factory.class = null\n\tssl.key.password = null\n\tssl.keymanager.algorithm = SunX509\n\tssl.keystore.certificate.chain = null\n\tssl.keystore.key = null\n\tssl.keystore.location = null\n\tssl.keystore.password = null\n\tssl.keystore.type = JKS\n\tssl.principal.mapping.rules = DEFAULT\n\tssl.protocol = TLSv1.3\n\tssl.provider = null\n\tssl.secure.random.implementation = null\n\tssl.trustmanager.algorithm = PKIX\n\tssl.truststore.certificates = null\n\tssl.truststore.location = null\n\tssl.truststore.password = null\n\tssl.truststore.type = JKS\n\ttransaction.abort.timed.out.transaction.cleanup.interval.ms = 10000\n\ttransaction.max.timeout.ms = 900000\n\ttransaction.partition.verification.enable = true\n\ttransaction.remove.expired.transaction.cleanup.interval.ms = 3600000\n\ttransaction.state.log.load.buffer.size = 5242880\n\ttransaction.state.log.min.isr = 2\n\ttransaction.state.log.num.partitions = 50\n\ttransaction.state.log.replication.factor = 3\n\ttransaction.state.log.segment.bytes = 104857600\n\ttransactional.id.expiration.ms = 604800000\n\tunclean.leader.election.enable = false\n\tunstable.api.versions.enable = false\n\tzookeeper.clientCnxnSocket = null\n\tzookeeper.connect = 127.0.0.1:41533\n\tzookeeper.connection.timeout.ms = 10000\n\tzookeeper.max.in.flight.requests = 10\n\tzookeeper.metadata.migration.enable = false\n\tzookeeper.session.timeout.ms = 18000\n\tzookeeper.set.acl = false\n\tzookeeper.ssl.cipher.suites = null\n\tzookeeper.ssl.client.enable = false\n\tzookeeper.ssl.crl.enable = false\n\tzookeeper.ssl.enabled.protocols = null\n\tzookeeper.ssl.endpoint.identification.algorithm = HTTPS\n\tzookeeper.ssl.keystore.location = null\n\tzookeeper.ssl.keystore.password = null\n\tzookeeper.ssl.keystore.type = null\n\tzookeeper.ssl.ocsp.enable = false\n\tzookeeper.ssl.protocol = TLSv1.2\n\tzookeeper.ssl.truststore.location = null\n\tzookeeper.ssl.truststore.password = null\n\tzookeeper.ssl.truststore.type = null\n\n05:48:51.109 [main] INFO org.apache.zookeeper.common.X509Util -- Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation\n05:48:51.168 [main] INFO kafka.server.KafkaServer -- starting\n05:48:51.169 [main] INFO kafka.server.KafkaServer -- Connecting to zookeeper on 127.0.0.1:41533\n05:48:51.181 [main] INFO kafka.zookeeper.ZooKeeperClient -- [ZooKeeperClient Kafka server] Initializing a new session to 127.0.0.1:41533.\n05:48:51.183 [main] INFO org.apache.zookeeper.ZooKeeper -- Client environment:zookeeper.version=3.8.3-6ad6d364c7c0bcf0de452d54ebefa3058098ab56, built on 2023-10-05 10:34 UTC\n05:48:51.184 [main] INFO org.apache.zookeeper.ZooKeeper -- Client environment:host.name=ip-10-4-212-203.us-west-2.compute.internal\n05:48:51.184 [main] INFO org.apache.zookeeper.ZooKeeper -- Client environment:java.version=17.0.17\n05:48:51.184 [main] INFO org.apache.zookeeper.ZooKeeper -- Client environment:java.vendor=Ubuntu\n05:48:51.184 [main] INFO org.apache.zookeeper.ZooKeeper -- Client environment:java.home=/usr/lib/jvm/java-17-openjdk-amd64\n05:48:51.184 [main] INFO org.apache.zookeeper.ZooKeeper -- Client environment:java.class.path=/tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/target/test-classes:/tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/target/classes:/fsx/ubuntu/.m2/repository/org/springframework/boot/spring-boot-starter-web/3.2.1/spring-boot-starter-web-3.2.1.jar:/fsx/ubuntu/.m2/repository/org/springframework/boot/spring-boot-starter/3.2.1/spring-boot-starter-3.2.1.jar:/fsx/ubuntu/.m2/repository/org/springframework/boot/spring-boot/3.2.1/spring-boot-3.2.1.jar:/fsx/ubuntu/.m2/repository/org/springframework/boot/spring-boot-autoconfigure/3.2.1/spring-boot-autoconfigure-3.2.1.jar:/fsx/ubuntu/.m2/repository/org/springframework/boot/spring-boot-starter-logging/3.2.1/spring-boot-starter-logging-3.2.1.jar:/fsx/ubuntu/.m2/repository/org/apache/logging/log4j/log4j-to-slf4j/2.21.1/log4j-to-slf4j-2.21.1.jar:/fsx/ubuntu/.m2/repository/org/apache/logging/log4j/log4j-api/2.21.1/log4j-api-2.21.1.jar:/fsx/ubuntu/.m2/repository/org/slf4j/jul-to-slf4j/2.0.9/jul-to-slf4j-2.0.9.jar:/fsx/ubuntu/.m2/repository/jakarta/annotation/jakarta.annotation-api/2.1.1/jakarta.annotation-api-2.1.1.jar:/fsx/ubuntu/.m2/repository/org/yaml/snakeyaml/2.2/snakeyaml-2.2.jar:/fsx/ubuntu/.m2/repository/org/springframework/boot/spring-boot-starter-json/3.2.1/spring-boot-starter-json-3.2.1.jar:/fsx/ubuntu/.m2/repository/com/fasterxml/jackson/datatype/jackson-datatype-jdk8/2.15.3/jackson-datatype-jdk8-2.15.3.jar:/fsx/ubuntu/.m2/repository/com/fasterxml/jackson/datatype/jackson-datatype-jsr310/2.15.3/jackson-datatype-jsr310-2.15.3.jar:/fsx/ubuntu/.m2/repository/com/fasterxml/jackson/module/jackson-module-parameter-names/2.15.3/jackson-module-parameter-names-2.15.3.jar:/fsx/ubuntu/.m2/repository/org/springframework/boot/spring-boot-starter-tomcat/3.2.1/spring-boot-starter-tomcat-3.2.1.jar:/fsx/ubuntu/.m2/repository/org/apache/tomcat/embed/tomcat-embed-core/10.1.17/tomcat-embed-core-10.1.17.jar:/fsx/ubuntu/.m2/repository/org/apache/tomcat/embed/tomcat-embed-el/10.1.17/tomcat-embed-el-10.1.17.jar:/fsx/ubuntu/.m2/repository/org/apache/tomcat/embed/tomcat-embed-websocket/10.1.17/tomcat-embed-websocket-10.1.17.jar:/fsx/ubuntu/.m2/repository/org/springframework/spring-web/6.1.2/spring-web-6.1.2.jar:/fsx/ubuntu/.m2/repository/org/springframework/spring-beans/6.1.2/spring-beans-6.1.2.jar:/fsx/ubuntu/.m2/repository/org/springframework/spring-webmvc/6.1.2/spring-webmvc-6.1.2.jar:/fsx/ubuntu/.m2/repository/org/springframework/spring-aop/6.1.2/spring-aop-6.1.2.jar:/fsx/ubuntu/.m2/repository/org/springframework/spring-expression/6.1.2/spring-expression-6.1.2.jar:/fsx/ubuntu/.m2/repository/org/springframework/boot/spring-boot-starter-actuator/3.2.1/spring-boot-starter-actuator-3.2.1.jar:/fsx/ubuntu/.m2/repository/org/springframework/boot/spring-boot-actuator-autoconfigure/3.2.1/spring-boot-actuator-autoconfigure-3.2.1.jar:/fsx/ubuntu/.m2/repository/org/springframework/boot/spring-boot-actuator/3.2.1/spring-boot-actuator-3.2.1.jar:/fsx/ubuntu/.m2/repository/io/micrometer/micrometer-observation/1.12.1/micrometer-observation-1.12.1.jar:/fsx/ubuntu/.m2/repository/io/micrometer/micrometer-commons/1.12.1/micrometer-commons-1.12.1.jar:/fsx/ubuntu/.m2/repository/io/micrometer/micrometer-jakarta9/1.12.1/micrometer-jakarta9-1.12.1.jar:/fsx/ubuntu/.m2/repository/org/springframework/kafka/spring-kafka/3.1.1/spring-kafka-3.1.1.jar:/fsx/ubuntu/.m2/repository/org/springframework/spring-context/6.1.2/spring-context-6.1.2.jar:/fsx/ubuntu/.m2/repository/org/springframework/spring-messaging/6.1.2/spring-messaging-6.1.2.jar:/fsx/ubuntu/.m2/repository/org/springframework/spring-tx/6.1.2/spring-tx-6.1.2.jar:/fsx/ubuntu/.m2/repository/org/springframework/retry/spring-retry/2.0.5/spring-retry-2.0.5.jar:/fsx/ubuntu/.m2/repository/org/apache/kafka/kafka-clients/3.6.1/kafka-clients-3.6.1.jar:/fsx/ubuntu/.m2/repository/com/github/luben/zstd-jni/1.5.5-1/zstd-jni-1.5.5-1.jar:/fsx/ubuntu/.m2/repository/org/lz4/lz4-java/1.8.0/lz4-java-1.8.0.jar:/fsx/ubuntu/.m2/repository/org/xerial/snappy/snappy-java/1.1.10.5/snappy-java-1.1.10.5.jar:/fsx/ubuntu/.m2/repository/org/slf4j/slf4j-api/2.0.9/slf4j-api-2.0.9.jar:/fsx/ubuntu/.m2/repository/com/google/code/findbugs/jsr305/3.0.2/jsr305-3.0.2.jar:/fsx/ubuntu/.m2/repository/org/springframework/cloud/spring-cloud-starter-openfeign/4.1.0/spring-cloud-starter-openfeign-4.1.0.jar:/fsx/ubuntu/.m2/repository/org/springframework/cloud/spring-cloud-starter/4.1.0/spring-cloud-starter-4.1.0.jar:/fsx/ubuntu/.m2/repository/org/springframework/cloud/spring-cloud-context/4.1.0/spring-cloud-context-4.1.0.jar:/fsx/ubuntu/.m2/repository/org/springframework/security/spring-security-rsa/1.1.1/spring-security-rsa-1.1.1.jar:/fsx/ubuntu/.m2/repository/org/bouncycastle/bcprov-jdk18on/1.74/bcprov-jdk18on-1.74.jar:/fsx/ubuntu/.m2/repository/org/springframework/cloud/spring-cloud-openfeign-core/4.1.0/spring-cloud-openfeign-core-4.1.0.jar:/fsx/ubuntu/.m2/repository/org/springframework/boot/spring-boot-starter-aop/3.2.1/spring-boot-starter-aop-3.2.1.jar:/fsx/ubuntu/.m2/repository/io/github/openfeign/form/feign-form-spring/3.8.0/feign-form-spring-3.8.0.jar:/fsx/ubuntu/.m2/repository/io/github/openfeign/form/feign-form/3.8.0/feign-form-3.8.0.jar:/fsx/ubuntu/.m2/repository/commons-fileupload/commons-fileupload/1.5/commons-fileupload-1.5.jar:/fsx/ubuntu/.m2/repository/org/springframework/cloud/spring-cloud-commons/4.1.0/spring-cloud-commons-4.1.0.jar:/fsx/ubuntu/.m2/repository/org/springframework/security/spring-security-crypto/6.2.1/spring-security-crypto-6.2.1.jar:/fsx/ubuntu/.m2/repository/io/github/openfeign/feign-core/13.1/feign-core-13.1.jar:/fsx/ubuntu/.m2/repository/io/github/openfeign/feign-slf4j/13.1/feign-slf4j-13.1.jar:/fsx/ubuntu/.m2/repository/net/logstash/logback/logstash-logback-encoder/7.4/logstash-logback-encoder-7.4.jar:/fsx/ubuntu/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.15.3/jackson-databind-2.15.3.jar:/fsx/ubuntu/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.15.3/jackson-annotations-2.15.3.jar:/fsx/ubuntu/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.15.3/jackson-core-2.15.3.jar:/fsx/ubuntu/.m2/repository/org/aspectj/aspectjweaver/1.9.21/aspectjweaver-1.9.21.jar:/fsx/ubuntu/.m2/repository/io/micrometer/micrometer-registry-prometheus/1.12.1/micrometer-registry-prometheus-1.12.1.jar:/fsx/ubuntu/.m2/repository/io/micrometer/micrometer-core/1.12.1/micrometer-core-1.12.1.jar:/fsx/ubuntu/.m2/repository/org/hdrhistogram/HdrHistogram/2.1.12/HdrHistogram-2.1.12.jar:/fsx/ubuntu/.m2/repository/org/latencyutils/LatencyUtils/2.0.3/LatencyUtils-2.0.3.jar:/fsx/ubuntu/.m2/repository/io/prometheus/simpleclient_common/0.16.0/simpleclient_common-0.16.0.jar:/fsx/ubuntu/.m2/repository/io/prometheus/simpleclient/0.16.0/simpleclient-0.16.0.jar:/fsx/ubuntu/.m2/repository/io/prometheus/simpleclient_tracer_otel/0.16.0/simpleclient_tracer_otel-0.16.0.jar:/fsx/ubuntu/.m2/repository/io/prometheus/simpleclient_tracer_common/0.16.0/simpleclient_tracer_common-0.16.0.jar:/fsx/ubuntu/.m2/repository/io/prometheus/simpleclient_tracer_otel_agent/0.16.0/simpleclient_tracer_otel_agent-0.16.0.jar:/fsx/ubuntu/.m2/repository/org/springframework/kafka/spring-kafka-test/3.1.1/spring-kafka-test-3.1.1.jar:/fsx/ubuntu/.m2/repository/org/springframework/spring-test/6.1.2/spring-test-6.1.2.jar:/fsx/ubuntu/.m2/repository/org/apache/zookeeper/zookeeper/3.8.3/zookeeper-3.8.3.jar:/fsx/ubuntu/.m2/repository/org/apache/zookeeper/zookeeper-jute/3.8.3/zookeeper-jute-3.8.3.jar:/fsx/ubuntu/.m2/repository/org/apache/yetus/audience-annotations/0.12.0/audience-annotations-0.12.0.jar:/fsx/ubuntu/.m2/repository/io/netty/netty-handler/4.1.104.Final/netty-handler-4.1.104.Final.jar:/fsx/ubuntu/.m2/repository/io/netty/netty-common/4.1.104.Final/netty-common-4.1.104.Final.jar:/fsx/ubuntu/.m2/repository/io/netty/netty-resolver/4.1.104.Final/netty-resolver-4.1.104.Final.jar:/fsx/ubuntu/.m2/repository/io/netty/netty-buffer/4.1.104.Final/netty-buffer-4.1.104.Final.jar:/fsx/ubuntu/.m2/repository/io/netty/netty-transport/4.1.104.Final/netty-transport-4.1.104.Final.jar:/fsx/ubuntu/.m2/repository/io/netty/netty-transport-native-unix-common/4.1.104.Final/netty-transport-native-unix-common-4.1.104.Final.jar:/fsx/ubuntu/.m2/repository/io/netty/netty-codec/4.1.104.Final/netty-codec-4.1.104.Final.jar:/fsx/ubuntu/.m2/repository/io/netty/netty-transport-native-epoll/4.1.104.Final/netty-transport-native-epoll-4.1.104.Final.jar:/fsx/ubuntu/.m2/repository/io/netty/netty-transport-classes-epoll/4.1.104.Final/netty-transport-classes-epoll-4.1.104.Final.jar:/fsx/ubuntu/.m2/repository/ch/qos/logback/logback-core/1.4.14/logback-core-1.4.14.jar:/fsx/ubuntu/.m2/repository/ch/qos/logback/logback-classic/1.4.14/logback-classic-1.4.14.jar:/fsx/ubuntu/.m2/repository/commons-io/commons-io/2.11.0/commons-io-2.11.0.jar:/fsx/ubuntu/.m2/repository/org/apache/kafka/kafka-clients/3.6.1/kafka-clients-3.6.1-test.jar:/fsx/ubuntu/.m2/repository/org/apache/kafka/kafka-metadata/3.6.1/kafka-metadata-3.6.1.jar:/fsx/ubuntu/.m2/repository/org/apache/kafka/kafka-raft/3.6.1/kafka-raft-3.6.1.jar:/fsx/ubuntu/.m2/repository/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar:/fsx/ubuntu/.m2/repository/org/apache/kafka/kafka-server-common/3.6.1/kafka-server-common-3.6.1.jar:/fsx/ubuntu/.m2/repository/net/sf/jopt-simple/jopt-simple/5.0.4/jopt-simple-5.0.4.jar:/fsx/ubuntu/.m2/repository/org/pcollections/pcollections/4.0.1/pcollections-4.0.1.jar:/fsx/ubuntu/.m2/repository/org/apache/kafka/kafka-server-common/3.6.1/kafka-server-common-3.6.1-test.jar:/fsx/ubuntu/.m2/repository/org/apache/kafka/kafka-streams-test-utils/3.6.1/kafka-streams-test-utils-3.6.1.jar:/fsx/ubuntu/.m2/repository/org/apache/kafka/kafka-streams/3.6.1/kafka-streams-3.6.1.jar:/fsx/ubuntu/.m2/repository/org/rocksdb/rocksdbjni/7.9.2/rocksdbjni-7.9.2.jar:/fsx/ubuntu/.m2/repository/org/apache/kafka/kafka_2.13/3.6.1/kafka_2.13-3.6.1.jar:/fsx/ubuntu/.m2/repository/org/scala-lang/scala-library/2.13.11/scala-library-2.13.11.jar:/fsx/ubuntu/.m2/repository/org/apache/kafka/kafka-group-coordinator/3.6.1/kafka-group-coordinator-3.6.1.jar:/fsx/ubuntu/.m2/repository/org/apache/kafka/kafka-storage-api/3.6.1/kafka-storage-api-3.6.1.jar:/fsx/ubuntu/.m2/repository/org/apache/kafka/kafka-tools-api/3.6.1/kafka-tools-api-3.6.1.jar:/fsx/ubuntu/.m2/repository/org/apache/kafka/kafka-storage/3.6.1/kafka-storage-3.6.1.jar:/fsx/ubuntu/.m2/repository/com/github/ben-manes/caffeine/caffeine/3.1.8/caffeine-3.1.8.jar:/fsx/ubuntu/.m2/repository/org/checkerframework/checker-qual/3.37.0/checker-qual-3.37.0.jar:/fsx/ubuntu/.m2/repository/com/google/errorprone/error_prone_annotations/2.21.1/error_prone_annotations-2.21.1.jar:/fsx/ubuntu/.m2/repository/net/sourceforge/argparse4j/argparse4j/0.7.0/argparse4j-0.7.0.jar:/fsx/ubuntu/.m2/repository/commons-validator/commons-validator/1.7/commons-validator-1.7.jar:/fsx/ubuntu/.m2/repository/commons-beanutils/commons-beanutils/1.9.4/commons-beanutils-1.9.4.jar:/fsx/ubuntu/.m2/repository/commons-digester/commons-digester/2.1/commons-digester-2.1.jar:/fsx/ubuntu/.m2/repository/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar:/fsx/ubuntu/.m2/repository/com/fasterxml/jackson/module/jackson-module-scala_2.13/2.15.3/jackson-module-scala_2.13-2.15.3.jar:/fsx/ubuntu/.m2/repository/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar:/fsx/ubuntu/.m2/repository/com/fasterxml/jackson/dataformat/jackson-dataformat-csv/2.15.3/jackson-dataformat-csv-2.15.3.jar:/fsx/ubuntu/.m2/repository/org/bitbucket/b_c/jose4j/0.9.3/jose4j-0.9.3.jar:/fsx/ubuntu/.m2/repository/org/scala-lang/modules/scala-collection-compat_2.13/2.10.0/scala-collection-compat_2.13-2.10.0.jar:/fsx/ubuntu/.m2/repository/org/scala-lang/modules/scala-java8-compat_2.13/1.0.2/scala-java8-compat_2.13-1.0.2.jar:/fsx/ubuntu/.m2/repository/org/scala-lang/scala-reflect/2.13.11/scala-reflect-2.13.11.jar:/fsx/ubuntu/.m2/repository/com/typesafe/scala-logging/scala-logging_2.13/3.9.4/scala-logging_2.13-3.9.4.jar:/fsx/ubuntu/.m2/repository/io/dropwizard/metrics/metrics-core/4.2.23/metrics-core-4.2.23.jar:/fsx/ubuntu/.m2/repository/commons-cli/commons-cli/1.4/commons-cli-1.4.jar:/fsx/ubuntu/.m2/repository/org/apache/kafka/kafka_2.13/3.6.1/kafka_2.13-3.6.1-test.jar:/fsx/ubuntu/.m2/repository/org/junit/jupiter/junit-jupiter-api/5.10.1/junit-jupiter-api-5.10.1.jar:/fsx/ubuntu/.m2/repository/org/opentest4j/opentest4j/1.3.0/opentest4j-1.3.0.jar:/fsx/ubuntu/.m2/repository/org/junit/platform/junit-platform-commons/1.10.1/junit-platform-commons-1.10.1.jar:/fsx/ubuntu/.m2/repository/org/junit/platform/junit-platform-launcher/1.10.1/junit-platform-launcher-1.10.1.jar:/fsx/ubuntu/.m2/repository/org/assertj/assertj-core/3.24.2/assertj-core-3.24.2.jar:/fsx/ubuntu/.m2/repository/net/bytebuddy/byte-buddy/1.14.10/byte-buddy-1.14.10.jar:/fsx/ubuntu/.m2/repository/org/hamcrest/hamcrest-all/1.3/hamcrest-all-1.3.jar:/fsx/ubuntu/.m2/repository/org/junit/vintage/junit-vintage-engine/5.10.1/junit-vintage-engine-5.10.1.jar:/fsx/ubuntu/.m2/repository/org/junit/platform/junit-platform-engine/1.10.1/junit-platform-engine-1.10.1.jar:/fsx/ubuntu/.m2/repository/junit/junit/4.13.2/junit-4.13.2.jar:/fsx/ubuntu/.m2/repository/org/hamcrest/hamcrest-core/2.2/hamcrest-core-2.2.jar:/fsx/ubuntu/.m2/repository/org/apiguardian/apiguardian-api/1.1.2/apiguardian-api-1.1.2.jar:/fsx/ubuntu/.m2/repository/org/springframework/boot/spring-boot-starter-test/3.2.1/spring-boot-starter-test-3.2.1.jar:/fsx/ubuntu/.m2/repository/org/springframework/boot/spring-boot-test/3.2.1/spring-boot-test-3.2.1.jar:/fsx/ubuntu/.m2/repository/org/springframework/boot/spring-boot-test-autoconfigure/3.2.1/spring-boot-test-autoconfigure-3.2.1.jar:/fsx/ubuntu/.m2/repository/com/jayway/jsonpath/json-path/2.8.0/json-path-2.8.0.jar:/fsx/ubuntu/.m2/repository/jakarta/xml/bind/jakarta.xml.bind-api/4.0.1/jakarta.xml.bind-api-4.0.1.jar:/fsx/ubuntu/.m2/repository/jakarta/activation/jakarta.activation-api/2.1.2/jakarta.activation-api-2.1.2.jar:/fsx/ubuntu/.m2/repository/net/minidev/json-smart/2.5.0/json-smart-2.5.0.jar:/fsx/ubuntu/.m2/repository/net/minidev/accessors-smart/2.5.0/accessors-smart-2.5.0.jar:/fsx/ubuntu/.m2/repository/org/ow2/asm/asm/9.3/asm-9.3.jar:/fsx/ubuntu/.m2/repository/org/awaitility/awaitility/4.2.0/awaitility-4.2.0.jar:/fsx/ubuntu/.m2/repository/org/hamcrest/hamcrest/2.2/hamcrest-2.2.jar:/fsx/ubuntu/.m2/repository/org/junit/jupiter/junit-jupiter/5.10.1/junit-jupiter-5.10.1.jar:/fsx/ubuntu/.m2/repository/org/junit/jupiter/junit-jupiter-params/5.10.1/junit-jupiter-params-5.10.1.jar:/fsx/ubuntu/.m2/repository/org/junit/jupiter/junit-jupiter-engine/5.10.1/junit-jupiter-engine-5.10.1.jar:/fsx/ubuntu/.m2/repository/org/mockito/mockito-core/5.7.0/mockito-core-5.7.0.jar:/fsx/ubuntu/.m2/repository/net/bytebuddy/byte-buddy-agent/1.14.10/byte-buddy-agent-1.14.10.jar:/fsx/ubuntu/.m2/repository/org/objenesis/objenesis/3.3/objenesis-3.3.jar:/fsx/ubuntu/.m2/repository/org/mockito/mockito-junit-jupiter/5.7.0/mockito-junit-jupiter-5.7.0.jar:/fsx/ubuntu/.m2/repository/org/skyscreamer/jsonassert/1.5.1/jsonassert-1.5.1.jar:/fsx/ubuntu/.m2/repository/com/vaadin/external/google/android-json/0.0.20131108.vaadin1/android-json-0.0.20131108.vaadin1.jar:/fsx/ubuntu/.m2/repository/org/springframework/spring-core/6.1.2/spring-core-6.1.2.jar:/fsx/ubuntu/.m2/repository/org/springframework/spring-jcl/6.1.2/spring-jcl-6.1.2.jar:/fsx/ubuntu/.m2/repository/org/xmlunit/xmlunit-core/2.9.1/xmlunit-core-2.9.1.jar:\n05:48:51.186 [main] INFO org.apache.zookeeper.ZooKeeper -- Client environment:java.library.path=/usr/local/cuda-12.9/lib:/usr/local/cuda-12.9/lib64:/usr/local/cuda-12.9:/usr/local/cuda-12.9/targets/x86_64-linux/lib/:/usr/local/cuda-12.9/extras/CUPTI/lib64:/opt/amazon/openmpi/lib:/usr/local/lib:/usr/lib:/usr/local/cuda-12.9/lib:/usr/local/cuda-12.9/lib64:/usr/local/cuda-12.9:/usr/local/cuda-12.9/targets/x86_64-linux/lib/:/usr/local/cuda-12.9/extras/CUPTI/lib64:/opt/amazon/openmpi/lib:/usr/local/lib:/usr/lib:/usr/local/cuda-12.9/lib:/usr/local/cuda-12.9/lib64:/usr/local/cuda-12.9:/usr/local/cuda-12.9/targets/x86_64-linux/lib/:/usr/local/cuda-12.9/extras/CUPTI/lib64:/opt/amazon/openmpi/lib:/usr/local/lib:/usr/lib:/usr/local/cuda-12.9/lib:/usr/local/cuda-12.9/lib64:/usr/local/cuda-12.9:/usr/local/cuda-12.9/targets/x86_64-linux/lib/:/usr/local/cuda-12.9/extras/CUPTI/lib64:/opt/amazon/openmpi/lib:/usr/local/lib:/usr/lib:/usr/java/packages/lib:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib\n05:48:51.186 [main] INFO org.apache.zookeeper.ZooKeeper -- Client environment:java.io.tmpdir=/tmp\n05:48:51.186 [main] INFO org.apache.zookeeper.ZooKeeper -- Client environment:java.compiler=<NA>\n05:48:51.186 [main] INFO org.apache.zookeeper.ZooKeeper -- Client environment:os.name=Linux\n05:48:51.186 [main] INFO org.apache.zookeeper.ZooKeeper -- Client environment:os.arch=amd64\n05:48:51.186 [main] INFO org.apache.zookeeper.ZooKeeper -- Client environment:os.version=6.8.0-1044-aws\n05:48:51.186 [main] INFO org.apache.zookeeper.ZooKeeper -- Client environment:user.name=ubuntu\n05:48:51.186 [main] INFO org.apache.zookeeper.ZooKeeper -- Client environment:user.home=/fsx/ubuntu\n05:48:51.186 [main] INFO org.apache.zookeeper.ZooKeeper -- Client environment:user.dir=/tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor\n05:48:51.186 [main] INFO org.apache.zookeeper.ZooKeeper -- Client environment:os.memory.free=1968MB\n05:48:51.186 [main] INFO org.apache.zookeeper.ZooKeeper -- Client environment:os.memory.max=30688MB\n05:48:51.187 [main] INFO org.apache.zookeeper.ZooKeeper -- Client environment:os.memory.total=2080MB\n05:48:51.189 [main] INFO org.apache.zookeeper.ZooKeeper -- Initiating client connection, connectString=127.0.0.1:41533 sessionTimeout=18000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@290b1b2e\n05:48:51.190 [main] INFO org.apache.zookeeper.ClientCnxnSocket -- jute.maxbuffer value is 4194304 Bytes\n05:48:51.195 [main] INFO org.apache.zookeeper.ClientCnxn -- zookeeper.request.timeout value is 0. feature enabled=false\n05:48:51.204 [main-SendThread(127.0.0.1:41533)] INFO org.apache.zookeeper.ClientCnxn -- Opening socket connection to server /127.0.0.1:41533.\n05:48:51.204 [main] INFO kafka.zookeeper.ZooKeeperClient -- [ZooKeeperClient Kafka server] Waiting until connected.\n05:48:51.206 [main-SendThread(127.0.0.1:41533)] INFO org.apache.zookeeper.ClientCnxn -- Socket connection established, initiating session, client: /127.0.0.1:47128, server: /127.0.0.1:41533\n05:48:51.211 [SyncThread:0] INFO org.apache.zookeeper.server.persistence.FileTxnLog -- Creating new log file: log.1\n05:48:51.213 [SyncThread:0] INFO org.apache.zookeeper.audit.ZKAuditProvider -- ZooKeeper audit is disabled.\n05:48:51.216 [main-SendThread(127.0.0.1:41533)] INFO org.apache.zookeeper.ClientCnxn -- Session establishment complete on server /127.0.0.1:41533, session id = 0x1001b05f8640000, negotiated timeout = 16000\n05:48:51.218 [main] INFO kafka.zookeeper.ZooKeeperClient -- [ZooKeeperClient Kafka server] Connected.\n05:48:51.438 [main] INFO kafka.server.KafkaServer -- Cluster ID = Rfe_PJqIS1WPJ2Ba9HwwNQ\n05:48:51.441 [main] WARN kafka.server.BrokerMetadataCheckpoint -- No meta.properties file under dir /tmp/spring.kafka.bca7ba27-5e93-400e-9aae-86960892bff71860864339420333442/meta.properties\n05:48:51.485 [main] INFO kafka.server.KafkaConfig -- KafkaConfig values: \n\tadvertised.listeners = null\n\talter.config.policy.class.name = null\n\talter.log.dirs.replication.quota.window.num = 11\n\talter.log.dirs.replication.quota.window.size.seconds = 1\n\tauthorizer.class.name = \n\tauto.create.topics.enable = true\n\tauto.include.jmx.reporter = true\n\tauto.leader.rebalance.enable = true\n\tbackground.threads = 2\n\tbroker.heartbeat.interval.ms = 2000\n\tbroker.id = 0\n\tbroker.id.generation.enable = true\n\tbroker.rack = null\n\tbroker.session.timeout.ms = 9000\n\tclient.quota.callback.class = null\n\tcompression.type = producer\n\tconnection.failed.authentication.delay.ms = 100\n\tconnections.max.idle.ms = 600000\n\tconnections.max.reauth.ms = 0\n\tcontrol.plane.listener.name = null\n\tcontrolled.shutdown.enable = true\n\tcontrolled.shutdown.max.retries = 3\n\tcontrolled.shutdown.retry.backoff.ms = 100\n\tcontroller.listener.names = null\n\tcontroller.quorum.append.linger.ms = 25\n\tcontroller.quorum.election.backoff.max.ms = 1000\n\tcontroller.quorum.election.timeout.ms = 1000\n\tcontroller.quorum.fetch.timeout.ms = 2000\n\tcontroller.quorum.request.timeout.ms = 2000\n\tcontroller.quorum.retry.backoff.ms = 20\n\tcontroller.quorum.voters = []\n\tcontroller.quota.window.num = 11\n\tcontroller.quota.window.size.seconds = 1\n\tcontroller.socket.timeout.ms = 1000\n\tcreate.topic.policy.class.name = null\n\tdefault.replication.factor = 2\n\tdelegation.token.expiry.check.interval.ms = 3600000\n\tdelegation.token.expiry.time.ms = 86400000\n\tdelegation.token.master.key = null\n\tdelegation.token.max.lifetime.ms = 604800000\n\tdelegation.token.secret.key = null\n\tdelete.records.purgatory.purge.interval.requests = 1\n\tdelete.topic.enable = true\n\tearly.start.listeners = null\n\tfetch.max.bytes = 57671680\n\tfetch.purgatory.purge.interval.requests = 1000\n\tgroup.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.RangeAssignor]\n\tgroup.consumer.heartbeat.interval.ms = 5000\n\tgroup.consumer.max.heartbeat.interval.ms = 15000\n\tgroup.consumer.max.session.timeout.ms = 60000\n\tgroup.consumer.max.size = 2147483647\n\tgroup.consumer.min.heartbeat.interval.ms = 5000\n\tgroup.consumer.min.session.timeout.ms = 45000\n\tgroup.consumer.session.timeout.ms = 45000\n\tgroup.coordinator.new.enable = false\n\tgroup.coordinator.threads = 1\n\tgroup.initial.rebalance.delay.ms = 0\n\tgroup.max.session.timeout.ms = 1800000\n\tgroup.max.size = 2147483647\n\tgroup.min.session.timeout.ms = 6000\n\tinitial.broker.registration.timeout.ms = 60000\n\tinter.broker.listener.name = null\n\tinter.broker.protocol.version = 3.6-IV2\n\tkafka.metrics.polling.interval.secs = 10\n\tkafka.metrics.reporters = []\n\tleader.imbalance.check.interval.seconds = 300\n\tleader.imbalance.per.broker.percentage = 10\n\tlistener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL\n\tlisteners = PLAINTEXT://localhost:0\n\tlog.cleaner.backoff.ms = 15000\n\tlog.cleaner.dedupe.buffer.size = 2097152\n\tlog.cleaner.delete.retention.ms = 86400000\n\tlog.cleaner.enable = true\n\tlog.cleaner.io.buffer.load.factor = 0.9\n\tlog.cleaner.io.buffer.size = 524288\n\tlog.cleaner.io.max.bytes.per.second = 1.7976931348623157E308\n\tlog.cleaner.max.compaction.lag.ms = 9223372036854775807\n\tlog.cleaner.min.cleanable.ratio = 0.5\n\tlog.cleaner.min.compaction.lag.ms = 0\n\tlog.cleaner.threads = 1\n\tlog.cleanup.policy = [delete]\n\tlog.dir = /tmp/spring.kafka.bca7ba27-5e93-400e-9aae-86960892bff71860864339420333442\n\tlog.dirs = null\n\tlog.flush.interval.messages = 9223372036854775807\n\tlog.flush.interval.ms = null\n\tlog.flush.offset.checkpoint.interval.ms = 60000\n\tlog.flush.scheduler.interval.ms = 9223372036854775807\n\tlog.flush.start.offset.checkpoint.interval.ms = 60000\n\tlog.index.interval.bytes = 4096\n\tlog.index.size.max.bytes = 10485760\n\tlog.local.retention.bytes = -2\n\tlog.local.retention.ms = -2\n\tlog.message.downconversion.enable = true\n\tlog.message.format.version = 3.0-IV1\n\tlog.message.timestamp.after.max.ms = 9223372036854775807\n\tlog.message.timestamp.before.max.ms = 9223372036854775807\n\tlog.message.timestamp.difference.max.ms = 9223372036854775807\n\tlog.message.timestamp.type = CreateTime\n\tlog.preallocate = false\n\tlog.retention.bytes = -1\n\tlog.retention.check.interval.ms = 300000\n\tlog.retention.hours = 168\n\tlog.retention.minutes = null\n\tlog.retention.ms = null\n\tlog.roll.hours = 168\n\tlog.roll.jitter.hours = 0\n\tlog.roll.jitter.ms = null\n\tlog.roll.ms = null\n\tlog.segment.bytes = 1073741824\n\tlog.segment.delete.delay.ms = 1000\n\tmax.connection.creation.rate = 2147483647\n\tmax.connections = 2147483647\n\tmax.connections.per.ip = 2147483647\n\tmax.connections.per.ip.overrides = \n\tmax.incremental.fetch.session.cache.slots = 1000\n\tmessage.max.bytes = 1048588\n\tmetadata.log.dir = null\n\tmetadata.log.max.record.bytes.between.snapshots = 20971520\n\tmetadata.log.max.snapshot.interval.ms = 3600000\n\tmetadata.log.segment.bytes = 1073741824\n\tmetadata.log.segment.min.bytes = 8388608\n\tmetadata.log.segment.ms = 604800000\n\tmetadata.max.idle.interval.ms = 500\n\tmetadata.max.retention.bytes = 104857600\n\tmetadata.max.retention.ms = 604800000\n\tmetric.reporters = []\n\tmetrics.num.samples = 2\n\tmetrics.recording.level = INFO\n\tmetrics.sample.window.ms = 30000\n\tmin.insync.replicas = 1\n\tnode.id = 0\n\tnum.io.threads = 8\n\tnum.network.threads = 2\n\tnum.partitions = 2\n\tnum.recovery.threads.per.data.dir = 1\n\tnum.replica.alter.log.dirs.threads = null\n\tnum.replica.fetchers = 1\n\toffset.metadata.max.bytes = 4096\n\toffsets.commit.required.acks = -1\n\toffsets.commit.timeout.ms = 5000\n\toffsets.load.buffer.size = 5242880\n\toffsets.retention.check.interval.ms = 600000\n\toffsets.retention.minutes = 10080\n\toffsets.topic.compression.codec = 0\n\toffsets.topic.num.partitions = 5\n\toffsets.topic.replication.factor = 1\n\toffsets.topic.segment.bytes = 104857600\n\tpassword.encoder.cipher.algorithm = AES/CBC/PKCS5Padding\n\tpassword.encoder.iterations = 4096\n\tpassword.encoder.key.length = 128\n\tpassword.encoder.keyfactory.algorithm = null\n\tpassword.encoder.old.secret = null\n\tpassword.encoder.secret = null\n\tprincipal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder\n\tprocess.roles = []\n\tproducer.id.expiration.check.interval.ms = 600000\n\tproducer.id.expiration.ms = 86400000\n\tproducer.purgatory.purge.interval.requests = 1000\n\tqueued.max.request.bytes = -1\n\tqueued.max.requests = 500\n\tquota.window.num = 11\n\tquota.window.size.seconds = 1\n\tremote.log.index.file.cache.total.size.bytes = 1073741824\n\tremote.log.manager.task.interval.ms = 30000\n\tremote.log.manager.task.retry.backoff.max.ms = 30000\n\tremote.log.manager.task.retry.backoff.ms = 500\n\tremote.log.manager.task.retry.jitter = 0.2\n\tremote.log.manager.thread.pool.size = 10\n\tremote.log.metadata.custom.metadata.max.bytes = 128\n\tremote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager\n\tremote.log.metadata.manager.class.path = null\n\tremote.log.metadata.manager.impl.prefix = rlmm.config.\n\tremote.log.metadata.manager.listener.name = null\n\tremote.log.reader.max.pending.tasks = 100\n\tremote.log.reader.threads = 10\n\tremote.log.storage.manager.class.name = null\n\tremote.log.storage.manager.class.path = null\n\tremote.log.storage.manager.impl.prefix = rsm.config.\n\tremote.log.storage.system.enable = false\n\treplica.fetch.backoff.ms = 1000\n\treplica.fetch.max.bytes = 1048576\n\treplica.fetch.min.bytes = 1\n\treplica.fetch.response.max.bytes = 10485760\n\treplica.fetch.wait.max.ms = 500\n\treplica.high.watermark.checkpoint.interval.ms = 9223372036854775807\n\treplica.lag.time.max.ms = 30000\n\treplica.selector.class = null\n\treplica.socket.receive.buffer.bytes = 65536\n\treplica.socket.timeout.ms = 1000\n\treplication.quota.window.num = 11\n\treplication.quota.window.size.seconds = 1\n\trequest.timeout.ms = 30000\n\treserved.broker.max.id = 1000\n\tsasl.client.callback.handler.class = null\n\tsasl.enabled.mechanisms = [GSSAPI]\n\tsasl.jaas.config = null\n\tsasl.kerberos.kinit.cmd = /usr/bin/kinit\n\tsasl.kerberos.min.time.before.relogin = 60000\n\tsasl.kerberos.principal.to.local.rules = [DEFAULT]\n\tsasl.kerberos.service.name = null\n\tsasl.kerberos.ticket.renew.jitter = 0.05\n\tsasl.kerberos.ticket.renew.window.factor = 0.8\n\tsasl.login.callback.handler.class = null\n\tsasl.login.class = null\n\tsasl.login.connect.timeout.ms = null\n\tsasl.login.read.timeout.ms = null\n\tsasl.login.refresh.buffer.seconds = 300\n\tsasl.login.refresh.min.period.seconds = 60\n\tsasl.login.refresh.window.factor = 0.8\n\tsasl.login.refresh.window.jitter = 0.05\n\tsasl.login.retry.backoff.max.ms = 10000\n\tsasl.login.retry.backoff.ms = 100\n\tsasl.mechanism.controller.protocol = GSSAPI\n\tsasl.mechanism.inter.broker.protocol = GSSAPI\n\tsasl.oauthbearer.clock.skew.seconds = 30\n\tsasl.oauthbearer.expected.audience = null\n\tsasl.oauthbearer.expected.issuer = null\n\tsasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000\n\tsasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000\n\tsasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100\n\tsasl.oauthbearer.jwks.endpoint.url = null\n\tsasl.oauthbearer.scope.claim.name = scope\n\tsasl.oauthbearer.sub.claim.name = sub\n\tsasl.oauthbearer.token.endpoint.url = null\n\tsasl.server.callback.handler.class = null\n\tsasl.server.max.receive.size = 524288\n\tsecurity.inter.broker.protocol = PLAINTEXT\n\tsecurity.providers = null\n\tserver.max.startup.time.ms = 9223372036854775807\n\tsocket.connection.setup.timeout.max.ms = 30000\n\tsocket.connection.setup.timeout.ms = 10000\n\tsocket.listen.backlog.size = 50\n\tsocket.receive.buffer.bytes = 102400\n\tsocket.request.max.bytes = 104857600\n\tsocket.send.buffer.bytes = 102400\n\tssl.cipher.suites = []\n\tssl.client.auth = none\n\tssl.enabled.protocols = [TLSv1.2, TLSv1.3]\n\tssl.endpoint.identification.algorithm = https\n\tssl.engine.factory.class = null\n\tssl.key.password = null\n\tssl.keymanager.algorithm = SunX509\n\tssl.keystore.certificate.chain = null\n\tssl.keystore.key = null\n\tssl.keystore.location = null\n\tssl.keystore.password = null\n\tssl.keystore.type = JKS\n\tssl.principal.mapping.rules = DEFAULT\n\tssl.protocol = TLSv1.3\n\tssl.provider = null\n\tssl.secure.random.implementation = null\n\tssl.trustmanager.algorithm = PKIX\n\tssl.truststore.certificates = null\n\tssl.truststore.location = null\n\tssl.truststore.password = null\n\tssl.truststore.type = JKS\n\ttransaction.abort.timed.out.transaction.cleanup.interval.ms = 10000\n\ttransaction.max.timeout.ms = 900000\n\ttransaction.partition.verification.enable = true\n\ttransaction.remove.expired.transaction.cleanup.interval.ms = 3600000\n\ttransaction.state.log.load.buffer.size = 5242880\n\ttransaction.state.log.min.isr = 2\n\ttransaction.state.log.num.partitions = 50\n\ttransaction.state.log.replication.factor = 3\n\ttransaction.state.log.segment.bytes = 104857600\n\ttransactional.id.expiration.ms = 604800000\n\tunclean.leader.election.enable = false\n\tunstable.api.versions.enable = false\n\tzookeeper.clientCnxnSocket = null\n\tzookeeper.connect = 127.0.0.1:41533\n\tzookeeper.connection.timeout.ms = 10000\n\tzookeeper.max.in.flight.requests = 10\n\tzookeeper.metadata.migration.enable = false\n\tzookeeper.session.timeout.ms = 18000\n\tzookeeper.set.acl = false\n\tzookeeper.ssl.cipher.suites = null\n\tzookeeper.ssl.client.enable = false\n\tzookeeper.ssl.crl.enable = false\n\tzookeeper.ssl.enabled.protocols = null\n\tzookeeper.ssl.endpoint.identification.algorithm = HTTPS\n\tzookeeper.ssl.keystore.location = null\n\tzookeeper.ssl.keystore.password = null\n\tzookeeper.ssl.keystore.type = null\n\tzookeeper.ssl.ocsp.enable = false\n\tzookeeper.ssl.protocol = TLSv1.2\n\tzookeeper.ssl.truststore.location = null\n\tzookeeper.ssl.truststore.password = null\n\tzookeeper.ssl.truststore.type = null\n\n05:48:51.514 [ThrottledChannelReaper-Fetch] INFO kafka.server.ClientQuotaManager$ThrottledChannelReaper -- [ThrottledChannelReaper-Fetch]: Starting\n05:48:51.515 [ThrottledChannelReaper-Produce] INFO kafka.server.ClientQuotaManager$ThrottledChannelReaper -- [ThrottledChannelReaper-Produce]: Starting\n05:48:51.516 [ThrottledChannelReaper-Request] INFO kafka.server.ClientQuotaManager$ThrottledChannelReaper -- [ThrottledChannelReaper-Request]: Starting\n05:48:51.518 [ThrottledChannelReaper-ControllerMutation] INFO kafka.server.ClientQuotaManager$ThrottledChannelReaper -- [ThrottledChannelReaper-ControllerMutation]: Starting\n05:48:51.540 [main] INFO kafka.log.LogManager -- Loading logs from log dirs ArraySeq(/tmp/spring.kafka.bca7ba27-5e93-400e-9aae-86960892bff71860864339420333442)\n05:48:51.544 [main] INFO kafka.log.LogManager -- No logs found to be loaded in /tmp/spring.kafka.bca7ba27-5e93-400e-9aae-86960892bff71860864339420333442\n05:48:51.552 [main] INFO kafka.log.LogManager -- Loaded 0 logs in 11ms\n05:48:51.553 [main] INFO kafka.log.LogManager -- Starting log cleanup with a period of 300000 ms.\n05:48:51.554 [main] INFO kafka.log.LogManager -- Starting log flusher with a default period of 9223372036854775807 ms.\n05:48:51.563 [main] INFO kafka.log.LogCleaner -- Starting the log cleaner\n05:48:51.571 [kafka-log-cleaner-thread-0] INFO kafka.log.LogCleaner$CleanerThread -- [kafka-log-cleaner-thread-0]: Starting\n05:48:51.584 [feature-zk-node-event-process-thread] INFO kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread -- [feature-zk-node-event-process-thread]: Starting\n05:48:51.591 [feature-zk-node-event-process-thread] INFO kafka.server.FinalizedFeatureChangeListener -- Feature ZK node at path: /feature does not exist\n05:48:51.610 [zk-broker-0-to-controller-forwarding-channel-manager] INFO kafka.server.BrokerToControllerRequestThread -- [zk-broker-0-to-controller-forwarding-channel-manager]: Starting\n05:48:51.894 [main] INFO kafka.network.ConnectionQuotas -- Updated connection-accept-rate max connection creation rate to 2147483647\n05:48:51.897 [main] INFO kafka.network.DataPlaneAcceptor -- Awaiting socket connections on localhost:35013.\n05:48:51.897 [main] INFO kafka.network.DataPlaneAcceptor -- Opened wildcard endpoint localhost:35013\n05:48:51.909 [main] INFO kafka.network.SocketServer -- [SocketServer listenerType=ZK_BROKER, nodeId=0] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT)\n05:48:51.913 [zk-broker-0-to-controller-alter-partition-channel-manager] INFO kafka.server.BrokerToControllerRequestThread -- [zk-broker-0-to-controller-alter-partition-channel-manager]: Starting\n05:48:51.935 [ExpirationReaper-0-Produce] INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper -- [ExpirationReaper-0-Produce]: Starting\n05:48:51.936 [ExpirationReaper-0-Fetch] INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper -- [ExpirationReaper-0-Fetch]: Starting\n05:48:51.936 [ExpirationReaper-0-DeleteRecords] INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper -- [ExpirationReaper-0-DeleteRecords]: Starting\n05:48:51.937 [ExpirationReaper-0-ElectLeader] INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper -- [ExpirationReaper-0-ElectLeader]: Starting\n05:48:51.938 [ExpirationReaper-0-RemoteFetch] INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper -- [ExpirationReaper-0-RemoteFetch]: Starting\n05:48:51.948 [LogDirFailureHandler] INFO kafka.server.ReplicaManager$LogDirFailureHandler -- [LogDirFailureHandler]: Starting\n05:48:51.948 [AddPartitionsToTxnSenderThread-0] INFO kafka.server.AddPartitionsToTxnManager -- [AddPartitionsToTxnSenderThread-0]: Starting\n05:48:51.967 [main] INFO kafka.zk.KafkaZkClient -- Creating /brokers/ids/0 (is it secure? false)\n05:48:51.981 [main] INFO kafka.zk.KafkaZkClient -- Stat of the created znode at /brokers/ids/0 is: 25,25,1769665731976,1769665731976,1,0,0,72087306494017536,204,0,25\n\n05:48:51.981 [main] INFO kafka.zk.KafkaZkClient -- Registered broker 0 at path /brokers/ids/0 with addresses: PLAINTEXT://localhost:35013, czxid (broker epoch): 25\n05:48:52.035 [controller-event-thread] INFO kafka.controller.ControllerEventManager$ControllerEventThread -- [ControllerEventThread controllerId=0] Starting\n05:48:52.040 [ExpirationReaper-0-topic] INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper -- [ExpirationReaper-0-topic]: Starting\n05:48:52.046 [ExpirationReaper-0-Heartbeat] INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper -- [ExpirationReaper-0-Heartbeat]: Starting\n05:48:52.047 [ExpirationReaper-0-Rebalance] INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper -- [ExpirationReaper-0-Rebalance]: Starting\n05:48:52.050 [controller-event-thread] INFO kafka.zk.KafkaZkClient -- Successfully created /controller_epoch with initial epoch 0\n05:48:52.054 [controller-event-thread] INFO kafka.controller.KafkaController -- [Controller id=0] 0 successfully elected as the controller. Epoch incremented to 1 and epoch zk version is now 1\n05:48:52.058 [controller-event-thread] INFO kafka.controller.KafkaController -- [Controller id=0] Creating FeatureZNode at path: /feature with contents: FeatureZNode(2,Enabled,Map())\n05:48:52.060 [main] INFO kafka.coordinator.group.GroupCoordinator -- [GroupCoordinator 0]: Starting up.\n05:48:52.060 [main-EventThread] INFO kafka.server.FinalizedFeatureChangeListener -- Feature ZK node created at path: /feature\n05:48:52.064 [main] INFO kafka.coordinator.group.GroupCoordinator -- [GroupCoordinator 0]: Startup complete.\n05:48:52.076 [main] INFO kafka.coordinator.transaction.TransactionCoordinator -- [TransactionCoordinator id=0] Starting up.\n05:48:52.079 [TxnMarkerSenderThread-0] INFO kafka.coordinator.transaction.TransactionMarkerChannelManager -- [TxnMarkerSenderThread-0]: Starting\n05:48:52.079 [main] INFO kafka.coordinator.transaction.TransactionCoordinator -- [TransactionCoordinator id=0] Startup complete.\n05:48:52.083 [controller-event-thread] INFO kafka.controller.KafkaController -- [Controller id=0] Registering handlers\n05:48:52.083 [feature-zk-node-event-process-thread] INFO kafka.server.metadata.ZkMetadataCache -- [MetadataCache brokerId=0] Updated cache from existing None to latest Features(version=3.6-IV2, finalizedFeatures={}, finalizedFeaturesEpoch=0).\n05:48:52.087 [controller-event-thread] INFO kafka.controller.KafkaController -- [Controller id=0] Deleting log dir event notifications\n05:48:52.090 [controller-event-thread] INFO kafka.controller.KafkaController -- [Controller id=0] Deleting isr change notifications\n05:48:52.092 [controller-event-thread] INFO kafka.controller.KafkaController -- [Controller id=0] Initializing controller context\n05:48:52.124 [controller-event-thread] INFO kafka.controller.KafkaController -- [Controller id=0] Initialized broker epochs cache: HashMap(0 -> 25)\n05:48:52.129 [ExpirationReaper-0-AlterAcls] INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper -- [ExpirationReaper-0-AlterAcls]: Starting\n05:48:52.138 [Controller-0-to-broker-0-send-thread] INFO kafka.controller.RequestSendThread -- [RequestSendThread controllerId=0] Starting\n05:48:52.140 [controller-event-thread] INFO kafka.controller.KafkaController -- [Controller id=0] Currently active brokers in the cluster: Set(0)\n05:48:52.140 [controller-event-thread] INFO kafka.controller.KafkaController -- [Controller id=0] Currently shutting brokers in the cluster: HashSet()\n05:48:52.140 [controller-event-thread] INFO kafka.controller.KafkaController -- [Controller id=0] Current list of topics in the cluster: HashSet()\n05:48:52.140 [controller-event-thread] INFO kafka.controller.KafkaController -- [Controller id=0] Fetching topic deletions in progress\n05:48:52.143 [controller-event-thread] INFO kafka.controller.KafkaController -- [Controller id=0] List of topics to be deleted: \n05:48:52.143 [controller-event-thread] INFO kafka.controller.KafkaController -- [Controller id=0] List of topics ineligible for deletion: \n05:48:52.143 [controller-event-thread] INFO kafka.controller.KafkaController -- [Controller id=0] Initializing topic deletion manager\n05:48:52.144 [controller-event-thread] INFO kafka.controller.TopicDeletionManager -- [Topic Deletion Manager 0] Initializing manager with initial deletions: Set(), initial ineligible deletions: HashSet()\n05:48:52.145 [controller-event-thread] INFO kafka.controller.KafkaController -- [Controller id=0] Sending update metadata request\n05:48:52.150 [controller-event-thread] INFO state.change.logger -- [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 0 partitions\n05:48:52.155 [controller-event-thread] INFO kafka.controller.ZkReplicaStateMachine -- [ReplicaStateMachine controllerId=0] Initializing replica state\n05:48:52.156 [controller-event-thread] INFO kafka.controller.ZkReplicaStateMachine -- [ReplicaStateMachine controllerId=0] Triggering online replica state changes\n05:48:52.158 [/config/changes-event-process-thread] INFO kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread -- [/config/changes-event-process-thread]: Starting\n05:48:52.158 [controller-event-thread] INFO kafka.controller.ZkReplicaStateMachine -- [ReplicaStateMachine controllerId=0] Triggering offline replica state changes\n05:48:52.159 [controller-event-thread] INFO kafka.controller.ZkPartitionStateMachine -- [PartitionStateMachine controllerId=0] Initializing partition state\n05:48:52.159 [controller-event-thread] INFO kafka.controller.ZkPartitionStateMachine -- [PartitionStateMachine controllerId=0] Triggering online partition state changes\n05:48:52.160 [Controller-0-to-broker-0-send-thread] INFO kafka.controller.RequestSendThread -- [RequestSendThread controllerId=0] Controller 0 connected to localhost:35013 (id: 0 rack: null) for sending state change requests\n05:48:52.161 [controller-event-thread] INFO kafka.controller.KafkaController -- [Controller id=0] Ready to serve as the new controller with epoch 1\n05:48:52.166 [controller-event-thread] INFO kafka.controller.KafkaController -- [Controller id=0] Partitions undergoing preferred replica election: \n05:48:52.166 [controller-event-thread] INFO kafka.controller.KafkaController -- [Controller id=0] Partitions that completed preferred replica election: \n05:48:52.166 [controller-event-thread] INFO kafka.controller.KafkaController -- [Controller id=0] Skipping preferred replica election for partitions due to topic deletion: \n05:48:52.167 [controller-event-thread] INFO kafka.controller.KafkaController -- [Controller id=0] Resuming preferred replica election for partitions: \n05:48:52.167 [controller-event-thread] INFO kafka.controller.KafkaController -- [Controller id=0] Starting replica leader election (PREFERRED) for partitions  triggered by ZkTriggered\n05:48:52.167 [main] INFO kafka.network.SocketServer -- [SocketServer listenerType=ZK_BROKER, nodeId=0] Enabling request processing.\n05:48:52.171 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- Kafka version: 3.6.1\n05:48:52.172 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- Kafka commitId: 5e3c2b738d253ff5\n05:48:52.172 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- Kafka startTimeMs: 1769665732170\n05:48:52.173 [main] INFO kafka.server.KafkaServer -- [KafkaServer id=0] started\n05:48:52.174 [controller-event-thread] INFO kafka.controller.KafkaController -- [Controller id=0] Starting the controller scheduler\n05:48:52.175 [main] INFO kafka.server.KafkaConfig -- KafkaConfig values: \n\tadvertised.listeners = null\n\talter.config.policy.class.name = null\n\talter.log.dirs.replication.quota.window.num = 11\n\talter.log.dirs.replication.quota.window.size.seconds = 1\n\tauthorizer.class.name = \n\tauto.create.topics.enable = true\n\tauto.include.jmx.reporter = true\n\tauto.leader.rebalance.enable = true\n\tbackground.threads = 2\n\tbroker.heartbeat.interval.ms = 2000\n\tbroker.id = 1\n\tbroker.id.generation.enable = true\n\tbroker.rack = null\n\tbroker.session.timeout.ms = 9000\n\tclient.quota.callback.class = null\n\tcompression.type = producer\n\tconnection.failed.authentication.delay.ms = 100\n\tconnections.max.idle.ms = 600000\n\tconnections.max.reauth.ms = 0\n\tcontrol.plane.listener.name = null\n\tcontrolled.shutdown.enable = true\n\tcontrolled.shutdown.max.retries = 3\n\tcontrolled.shutdown.retry.backoff.ms = 100\n\tcontroller.listener.names = null\n\tcontroller.quorum.append.linger.ms = 25\n\tcontroller.quorum.election.backoff.max.ms = 1000\n\tcontroller.quorum.election.timeout.ms = 1000\n\tcontroller.quorum.fetch.timeout.ms = 2000\n\tcontroller.quorum.request.timeout.ms = 2000\n\tcontroller.quorum.retry.backoff.ms = 20\n\tcontroller.quorum.voters = []\n\tcontroller.quota.window.num = 11\n\tcontroller.quota.window.size.seconds = 1\n\tcontroller.socket.timeout.ms = 1000\n\tcreate.topic.policy.class.name = null\n\tdefault.replication.factor = 2\n\tdelegation.token.expiry.check.interval.ms = 3600000\n\tdelegation.token.expiry.time.ms = 86400000\n\tdelegation.token.master.key = null\n\tdelegation.token.max.lifetime.ms = 604800000\n\tdelegation.token.secret.key = null\n\tdelete.records.purgatory.purge.interval.requests = 1\n\tdelete.topic.enable = true\n\tearly.start.listeners = null\n\tfetch.max.bytes = 57671680\n\tfetch.purgatory.purge.interval.requests = 1000\n\tgroup.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.RangeAssignor]\n\tgroup.consumer.heartbeat.interval.ms = 5000\n\tgroup.consumer.max.heartbeat.interval.ms = 15000\n\tgroup.consumer.max.session.timeout.ms = 60000\n\tgroup.consumer.max.size = 2147483647\n\tgroup.consumer.min.heartbeat.interval.ms = 5000\n\tgroup.consumer.min.session.timeout.ms = 45000\n\tgroup.consumer.session.timeout.ms = 45000\n\tgroup.coordinator.new.enable = false\n\tgroup.coordinator.threads = 1\n\tgroup.initial.rebalance.delay.ms = 0\n\tgroup.max.session.timeout.ms = 1800000\n\tgroup.max.size = 2147483647\n\tgroup.min.session.timeout.ms = 6000\n\tinitial.broker.registration.timeout.ms = 60000\n\tinter.broker.listener.name = null\n\tinter.broker.protocol.version = 3.6-IV2\n\tkafka.metrics.polling.interval.secs = 10\n\tkafka.metrics.reporters = []\n\tleader.imbalance.check.interval.seconds = 300\n\tleader.imbalance.per.broker.percentage = 10\n\tlistener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL\n\tlisteners = PLAINTEXT://localhost:0\n\tlog.cleaner.backoff.ms = 15000\n\tlog.cleaner.dedupe.buffer.size = 2097152\n\tlog.cleaner.delete.retention.ms = 86400000\n\tlog.cleaner.enable = true\n\tlog.cleaner.io.buffer.load.factor = 0.9\n\tlog.cleaner.io.buffer.size = 524288\n\tlog.cleaner.io.max.bytes.per.second = 1.7976931348623157E308\n\tlog.cleaner.max.compaction.lag.ms = 9223372036854775807\n\tlog.cleaner.min.cleanable.ratio = 0.5\n\tlog.cleaner.min.compaction.lag.ms = 0\n\tlog.cleaner.threads = 1\n\tlog.cleanup.policy = [delete]\n\tlog.dir = /tmp/spring.kafka.69cecbcc-b8aa-44e4-b673-a5243dcc9dee11809573343017669845\n\tlog.dirs = null\n\tlog.flush.interval.messages = 9223372036854775807\n\tlog.flush.interval.ms = null\n\tlog.flush.offset.checkpoint.interval.ms = 60000\n\tlog.flush.scheduler.interval.ms = 9223372036854775807\n\tlog.flush.start.offset.checkpoint.interval.ms = 60000\n\tlog.index.interval.bytes = 4096\n\tlog.index.size.max.bytes = 10485760\n\tlog.local.retention.bytes = -2\n\tlog.local.retention.ms = -2\n\tlog.message.downconversion.enable = true\n\tlog.message.format.version = 3.0-IV1\n\tlog.message.timestamp.after.max.ms = 9223372036854775807\n\tlog.message.timestamp.before.max.ms = 9223372036854775807\n\tlog.message.timestamp.difference.max.ms = 9223372036854775807\n\tlog.message.timestamp.type = CreateTime\n\tlog.preallocate = false\n\tlog.retention.bytes = -1\n\tlog.retention.check.interval.ms = 300000\n\tlog.retention.hours = 168\n\tlog.retention.minutes = null\n\tlog.retention.ms = null\n\tlog.roll.hours = 168\n\tlog.roll.jitter.hours = 0\n\tlog.roll.jitter.ms = null\n\tlog.roll.ms = null\n\tlog.segment.bytes = 1073741824\n\tlog.segment.delete.delay.ms = 1000\n\tmax.connection.creation.rate = 2147483647\n\tmax.connections = 2147483647\n\tmax.connections.per.ip = 2147483647\n\tmax.connections.per.ip.overrides = \n\tmax.incremental.fetch.session.cache.slots = 1000\n\tmessage.max.bytes = 1048588\n\tmetadata.log.dir = null\n\tmetadata.log.max.record.bytes.between.snapshots = 20971520\n\tmetadata.log.max.snapshot.interval.ms = 3600000\n\tmetadata.log.segment.bytes = 1073741824\n\tmetadata.log.segment.min.bytes = 8388608\n\tmetadata.log.segment.ms = 604800000\n\tmetadata.max.idle.interval.ms = 500\n\tmetadata.max.retention.bytes = 104857600\n\tmetadata.max.retention.ms = 604800000\n\tmetric.reporters = []\n\tmetrics.num.samples = 2\n\tmetrics.recording.level = INFO\n\tmetrics.sample.window.ms = 30000\n\tmin.insync.replicas = 1\n\tnode.id = 1\n\tnum.io.threads = 8\n\tnum.network.threads = 2\n\tnum.partitions = 2\n\tnum.recovery.threads.per.data.dir = 1\n\tnum.replica.alter.log.dirs.threads = null\n\tnum.replica.fetchers = 1\n\toffset.metadata.max.bytes = 4096\n\toffsets.commit.required.acks = -1\n\toffsets.commit.timeout.ms = 5000\n\toffsets.load.buffer.size = 5242880\n\toffsets.retention.check.interval.ms = 600000\n\toffsets.retention.minutes = 10080\n\toffsets.topic.compression.codec = 0\n\toffsets.topic.num.partitions = 5\n\toffsets.topic.replication.factor = 1\n\toffsets.topic.segment.bytes = 104857600\n\tpassword.encoder.cipher.algorithm = AES/CBC/PKCS5Padding\n\tpassword.encoder.iterations = 4096\n\tpassword.encoder.key.length = 128\n\tpassword.encoder.keyfactory.algorithm = null\n\tpassword.encoder.old.secret = null\n\tpassword.encoder.secret = null\n\tprincipal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder\n\tprocess.roles = []\n\tproducer.id.expiration.check.interval.ms = 600000\n\tproducer.id.expiration.ms = 86400000\n\tproducer.purgatory.purge.interval.requests = 1000\n\tqueued.max.request.bytes = -1\n\tqueued.max.requests = 500\n\tquota.window.num = 11\n\tquota.window.size.seconds = 1\n\tremote.log.index.file.cache.total.size.bytes = 1073741824\n\tremote.log.manager.task.interval.ms = 30000\n\tremote.log.manager.task.retry.backoff.max.ms = 30000\n\tremote.log.manager.task.retry.backoff.ms = 500\n\tremote.log.manager.task.retry.jitter = 0.2\n\tremote.log.manager.thread.pool.size = 10\n\tremote.log.metadata.custom.metadata.max.bytes = 128\n\tremote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager\n\tremote.log.metadata.manager.class.path = null\n\tremote.log.metadata.manager.impl.prefix = rlmm.config.\n\tremote.log.metadata.manager.listener.name = null\n\tremote.log.reader.max.pending.tasks = 100\n\tremote.log.reader.threads = 10\n\tremote.log.storage.manager.class.name = null\n\tremote.log.storage.manager.class.path = null\n\tremote.log.storage.manager.impl.prefix = rsm.config.\n\tremote.log.storage.system.enable = false\n\treplica.fetch.backoff.ms = 1000\n\treplica.fetch.max.bytes = 1048576\n\treplica.fetch.min.bytes = 1\n\treplica.fetch.response.max.bytes = 10485760\n\treplica.fetch.wait.max.ms = 500\n\treplica.high.watermark.checkpoint.interval.ms = 9223372036854775807\n\treplica.lag.time.max.ms = 30000\n\treplica.selector.class = null\n\treplica.socket.receive.buffer.bytes = 65536\n\treplica.socket.timeout.ms = 1000\n\treplication.quota.window.num = 11\n\treplication.quota.window.size.seconds = 1\n\trequest.timeout.ms = 30000\n\treserved.broker.max.id = 1000\n\tsasl.client.callback.handler.class = null\n\tsasl.enabled.mechanisms = [GSSAPI]\n\tsasl.jaas.config = null\n\tsasl.kerberos.kinit.cmd = /usr/bin/kinit\n\tsasl.kerberos.min.time.before.relogin = 60000\n\tsasl.kerberos.principal.to.local.rules = [DEFAULT]\n\tsasl.kerberos.service.name = null\n\tsasl.kerberos.ticket.renew.jitter = 0.05\n\tsasl.kerberos.ticket.renew.window.factor = 0.8\n\tsasl.login.callback.handler.class = null\n\tsasl.login.class = null\n\tsasl.login.connect.timeout.ms = null\n\tsasl.login.read.timeout.ms = null\n\tsasl.login.refresh.buffer.seconds = 300\n\tsasl.login.refresh.min.period.seconds = 60\n\tsasl.login.refresh.window.factor = 0.8\n\tsasl.login.refresh.window.jitter = 0.05\n\tsasl.login.retry.backoff.max.ms = 10000\n\tsasl.login.retry.backoff.ms = 100\n\tsasl.mechanism.controller.protocol = GSSAPI\n\tsasl.mechanism.inter.broker.protocol = GSSAPI\n\tsasl.oauthbearer.clock.skew.seconds = 30\n\tsasl.oauthbearer.expected.audience = null\n\tsasl.oauthbearer.expected.issuer = null\n\tsasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000\n\tsasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000\n\tsasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100\n\tsasl.oauthbearer.jwks.endpoint.url = null\n\tsasl.oauthbearer.scope.claim.name = scope\n\tsasl.oauthbearer.sub.claim.name = sub\n\tsasl.oauthbearer.token.endpoint.url = null\n\tsasl.server.callback.handler.class = null\n\tsasl.server.max.receive.size = 524288\n\tsecurity.inter.broker.protocol = PLAINTEXT\n\tsecurity.providers = null\n\tserver.max.startup.time.ms = 9223372036854775807\n\tsocket.connection.setup.timeout.max.ms = 30000\n\tsocket.connection.setup.timeout.ms = 10000\n\tsocket.listen.backlog.size = 50\n\tsocket.receive.buffer.bytes = 102400\n\tsocket.request.max.bytes = 104857600\n\tsocket.send.buffer.bytes = 102400\n\tssl.cipher.suites = []\n\tssl.client.auth = none\n\tssl.enabled.protocols = [TLSv1.2, TLSv1.3]\n\tssl.endpoint.identification.algorithm = https\n\tssl.engine.factory.class = null\n\tssl.key.password = null\n\tssl.keymanager.algorithm = SunX509\n\tssl.keystore.certificate.chain = null\n\tssl.keystore.key = null\n\tssl.keystore.location = null\n\tssl.keystore.password = null\n\tssl.keystore.type = JKS\n\tssl.principal.mapping.rules = DEFAULT\n\tssl.protocol = TLSv1.3\n\tssl.provider = null\n\tssl.secure.random.implementation = null\n\tssl.trustmanager.algorithm = PKIX\n\tssl.truststore.certificates = null\n\tssl.truststore.location = null\n\tssl.truststore.password = null\n\tssl.truststore.type = JKS\n\ttransaction.abort.timed.out.transaction.cleanup.interval.ms = 10000\n\ttransaction.max.timeout.ms = 900000\n\ttransaction.partition.verification.enable = true\n\ttransaction.remove.expired.transaction.cleanup.interval.ms = 3600000\n\ttransaction.state.log.load.buffer.size = 5242880\n\ttransaction.state.log.min.isr = 2\n\ttransaction.state.log.num.partitions = 50\n\ttransaction.state.log.replication.factor = 3\n\ttransaction.state.log.segment.bytes = 104857600\n\ttransactional.id.expiration.ms = 604800000\n\tunclean.leader.election.enable = false\n\tunstable.api.versions.enable = false\n\tzookeeper.clientCnxnSocket = null\n\tzookeeper.connect = 127.0.0.1:41533\n\tzookeeper.connection.timeout.ms = 10000\n\tzookeeper.max.in.flight.requests = 10\n\tzookeeper.metadata.migration.enable = false\n\tzookeeper.session.timeout.ms = 18000\n\tzookeeper.set.acl = false\n\tzookeeper.ssl.cipher.suites = null\n\tzookeeper.ssl.client.enable = false\n\tzookeeper.ssl.crl.enable = false\n\tzookeeper.ssl.enabled.protocols = null\n\tzookeeper.ssl.endpoint.identification.algorithm = HTTPS\n\tzookeeper.ssl.keystore.location = null\n\tzookeeper.ssl.keystore.password = null\n\tzookeeper.ssl.keystore.type = null\n\tzookeeper.ssl.ocsp.enable = false\n\tzookeeper.ssl.protocol = TLSv1.2\n\tzookeeper.ssl.truststore.location = null\n\tzookeeper.ssl.truststore.password = null\n\tzookeeper.ssl.truststore.type = null\n\n05:48:52.179 [main] INFO kafka.server.KafkaServer -- starting\n05:48:52.179 [main] INFO kafka.server.KafkaServer -- Connecting to zookeeper on 127.0.0.1:41533\n05:48:52.179 [main] INFO kafka.zookeeper.ZooKeeperClient -- [ZooKeeperClient Kafka server] Initializing a new session to 127.0.0.1:41533.\n05:48:52.179 [main] INFO org.apache.zookeeper.ZooKeeper -- Initiating client connection, connectString=127.0.0.1:41533 sessionTimeout=18000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@5434e40c\n05:48:52.179 [main] INFO org.apache.zookeeper.ClientCnxnSocket -- jute.maxbuffer value is 4194304 Bytes\n05:48:52.179 [main] INFO org.apache.zookeeper.ClientCnxn -- zookeeper.request.timeout value is 0. feature enabled=false\n05:48:52.180 [main] INFO kafka.zookeeper.ZooKeeperClient -- [ZooKeeperClient Kafka server] Waiting until connected.\n05:48:52.180 [main-SendThread(127.0.0.1:41533)] INFO org.apache.zookeeper.ClientCnxn -- Opening socket connection to server /127.0.0.1:41533.\n05:48:52.180 [main-SendThread(127.0.0.1:41533)] INFO org.apache.zookeeper.ClientCnxn -- Socket connection established, initiating session, client: /127.0.0.1:47140, server: /127.0.0.1:41533\n05:48:52.182 [main-SendThread(127.0.0.1:41533)] INFO org.apache.zookeeper.ClientCnxn -- Session establishment complete on server /127.0.0.1:41533, session id = 0x1001b05f8640001, negotiated timeout = 16000\n05:48:52.182 [main] INFO kafka.zookeeper.ZooKeeperClient -- [ZooKeeperClient Kafka server] Connected.\n05:48:52.200 [main] INFO kafka.server.KafkaServer -- Cluster ID = Rfe_PJqIS1WPJ2Ba9HwwNQ\n05:48:52.201 [main] WARN kafka.server.BrokerMetadataCheckpoint -- No meta.properties file under dir /tmp/spring.kafka.69cecbcc-b8aa-44e4-b673-a5243dcc9dee11809573343017669845/meta.properties\n05:48:52.213 [main] INFO kafka.server.KafkaConfig -- KafkaConfig values: \n\tadvertised.listeners = null\n\talter.config.policy.class.name = null\n\talter.log.dirs.replication.quota.window.num = 11\n\talter.log.dirs.replication.quota.window.size.seconds = 1\n\tauthorizer.class.name = \n\tauto.create.topics.enable = true\n\tauto.include.jmx.reporter = true\n\tauto.leader.rebalance.enable = true\n\tbackground.threads = 2\n\tbroker.heartbeat.interval.ms = 2000\n\tbroker.id = 1\n\tbroker.id.generation.enable = true\n\tbroker.rack = null\n\tbroker.session.timeout.ms = 9000\n\tclient.quota.callback.class = null\n\tcompression.type = producer\n\tconnection.failed.authentication.delay.ms = 100\n\tconnections.max.idle.ms = 600000\n\tconnections.max.reauth.ms = 0\n\tcontrol.plane.listener.name = null\n\tcontrolled.shutdown.enable = true\n\tcontrolled.shutdown.max.retries = 3\n\tcontrolled.shutdown.retry.backoff.ms = 100\n\tcontroller.listener.names = null\n\tcontroller.quorum.append.linger.ms = 25\n\tcontroller.quorum.election.backoff.max.ms = 1000\n\tcontroller.quorum.election.timeout.ms = 1000\n\tcontroller.quorum.fetch.timeout.ms = 2000\n\tcontroller.quorum.request.timeout.ms = 2000\n\tcontroller.quorum.retry.backoff.ms = 20\n\tcontroller.quorum.voters = []\n\tcontroller.quota.window.num = 11\n\tcontroller.quota.window.size.seconds = 1\n\tcontroller.socket.timeout.ms = 1000\n\tcreate.topic.policy.class.name = null\n\tdefault.replication.factor = 2\n\tdelegation.token.expiry.check.interval.ms = 3600000\n\tdelegation.token.expiry.time.ms = 86400000\n\tdelegation.token.master.key = null\n\tdelegation.token.max.lifetime.ms = 604800000\n\tdelegation.token.secret.key = null\n\tdelete.records.purgatory.purge.interval.requests = 1\n\tdelete.topic.enable = true\n\tearly.start.listeners = null\n\tfetch.max.bytes = 57671680\n\tfetch.purgatory.purge.interval.requests = 1000\n\tgroup.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.RangeAssignor]\n\tgroup.consumer.heartbeat.interval.ms = 5000\n\tgroup.consumer.max.heartbeat.interval.ms = 15000\n\tgroup.consumer.max.session.timeout.ms = 60000\n\tgroup.consumer.max.size = 2147483647\n\tgroup.consumer.min.heartbeat.interval.ms = 5000\n\tgroup.consumer.min.session.timeout.ms = 45000\n\tgroup.consumer.session.timeout.ms = 45000\n\tgroup.coordinator.new.enable = false\n\tgroup.coordinator.threads = 1\n\tgroup.initial.rebalance.delay.ms = 0\n\tgroup.max.session.timeout.ms = 1800000\n\tgroup.max.size = 2147483647\n\tgroup.min.session.timeout.ms = 6000\n\tinitial.broker.registration.timeout.ms = 60000\n\tinter.broker.listener.name = null\n\tinter.broker.protocol.version = 3.6-IV2\n\tkafka.metrics.polling.interval.secs = 10\n\tkafka.metrics.reporters = []\n\tleader.imbalance.check.interval.seconds = 300\n\tleader.imbalance.per.broker.percentage = 10\n\tlistener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL\n\tlisteners = PLAINTEXT://localhost:0\n\tlog.cleaner.backoff.ms = 15000\n\tlog.cleaner.dedupe.buffer.size = 2097152\n\tlog.cleaner.delete.retention.ms = 86400000\n\tlog.cleaner.enable = true\n\tlog.cleaner.io.buffer.load.factor = 0.9\n\tlog.cleaner.io.buffer.size = 524288\n\tlog.cleaner.io.max.bytes.per.second = 1.7976931348623157E308\n\tlog.cleaner.max.compaction.lag.ms = 9223372036854775807\n\tlog.cleaner.min.cleanable.ratio = 0.5\n\tlog.cleaner.min.compaction.lag.ms = 0\n\tlog.cleaner.threads = 1\n\tlog.cleanup.policy = [delete]\n\tlog.dir = /tmp/spring.kafka.69cecbcc-b8aa-44e4-b673-a5243dcc9dee11809573343017669845\n\tlog.dirs = null\n\tlog.flush.interval.messages = 9223372036854775807\n\tlog.flush.interval.ms = null\n\tlog.flush.offset.checkpoint.interval.ms = 60000\n\tlog.flush.scheduler.interval.ms = 9223372036854775807\n\tlog.flush.start.offset.checkpoint.interval.ms = 60000\n\tlog.index.interval.bytes = 4096\n\tlog.index.size.max.bytes = 10485760\n\tlog.local.retention.bytes = -2\n\tlog.local.retention.ms = -2\n\tlog.message.downconversion.enable = true\n\tlog.message.format.version = 3.0-IV1\n\tlog.message.timestamp.after.max.ms = 9223372036854775807\n\tlog.message.timestamp.before.max.ms = 9223372036854775807\n\tlog.message.timestamp.difference.max.ms = 9223372036854775807\n\tlog.message.timestamp.type = CreateTime\n\tlog.preallocate = false\n\tlog.retention.bytes = -1\n\tlog.retention.check.interval.ms = 300000\n\tlog.retention.hours = 168\n\tlog.retention.minutes = null\n\tlog.retention.ms = null\n\tlog.roll.hours = 168\n\tlog.roll.jitter.hours = 0\n\tlog.roll.jitter.ms = null\n\tlog.roll.ms = null\n\tlog.segment.bytes = 1073741824\n\tlog.segment.delete.delay.ms = 1000\n\tmax.connection.creation.rate = 2147483647\n\tmax.connections = 2147483647\n\tmax.connections.per.ip = 2147483647\n\tmax.connections.per.ip.overrides = \n\tmax.incremental.fetch.session.cache.slots = 1000\n\tmessage.max.bytes = 1048588\n\tmetadata.log.dir = null\n\tmetadata.log.max.record.bytes.between.snapshots = 20971520\n\tmetadata.log.max.snapshot.interval.ms = 3600000\n\tmetadata.log.segment.bytes = 1073741824\n\tmetadata.log.segment.min.bytes = 8388608\n\tmetadata.log.segment.ms = 604800000\n\tmetadata.max.idle.interval.ms = 500\n\tmetadata.max.retention.bytes = 104857600\n\tmetadata.max.retention.ms = 604800000\n\tmetric.reporters = []\n\tmetrics.num.samples = 2\n\tmetrics.recording.level = INFO\n\tmetrics.sample.window.ms = 30000\n\tmin.insync.replicas = 1\n\tnode.id = 1\n\tnum.io.threads = 8\n\tnum.network.threads = 2\n\tnum.partitions = 2\n\tnum.recovery.threads.per.data.dir = 1\n\tnum.replica.alter.log.dirs.threads = null\n\tnum.replica.fetchers = 1\n\toffset.metadata.max.bytes = 4096\n\toffsets.commit.required.acks = -1\n\toffsets.commit.timeout.ms = 5000\n\toffsets.load.buffer.size = 5242880\n\toffsets.retention.check.interval.ms = 600000\n\toffsets.retention.minutes = 10080\n\toffsets.topic.compression.codec = 0\n\toffsets.topic.num.partitions = 5\n\toffsets.topic.replication.factor = 1\n\toffsets.topic.segment.bytes = 104857600\n\tpassword.encoder.cipher.algorithm = AES/CBC/PKCS5Padding\n\tpassword.encoder.iterations = 4096\n\tpassword.encoder.key.length = 128\n\tpassword.encoder.keyfactory.algorithm = null\n\tpassword.encoder.old.secret = null\n\tpassword.encoder.secret = null\n\tprincipal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder\n\tprocess.roles = []\n\tproducer.id.expiration.check.interval.ms = 600000\n\tproducer.id.expiration.ms = 86400000\n\tproducer.purgatory.purge.interval.requests = 1000\n\tqueued.max.request.bytes = -1\n\tqueued.max.requests = 500\n\tquota.window.num = 11\n\tquota.window.size.seconds = 1\n\tremote.log.index.file.cache.total.size.bytes = 1073741824\n\tremote.log.manager.task.interval.ms = 30000\n\tremote.log.manager.task.retry.backoff.max.ms = 30000\n\tremote.log.manager.task.retry.backoff.ms = 500\n\tremote.log.manager.task.retry.jitter = 0.2\n\tremote.log.manager.thread.pool.size = 10\n\tremote.log.metadata.custom.metadata.max.bytes = 128\n\tremote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager\n\tremote.log.metadata.manager.class.path = null\n\tremote.log.metadata.manager.impl.prefix = rlmm.config.\n\tremote.log.metadata.manager.listener.name = null\n\tremote.log.reader.max.pending.tasks = 100\n\tremote.log.reader.threads = 10\n\tremote.log.storage.manager.class.name = null\n\tremote.log.storage.manager.class.path = null\n\tremote.log.storage.manager.impl.prefix = rsm.config.\n\tremote.log.storage.system.enable = false\n\treplica.fetch.backoff.ms = 1000\n\treplica.fetch.max.bytes = 1048576\n\treplica.fetch.min.bytes = 1\n\treplica.fetch.response.max.bytes = 10485760\n\treplica.fetch.wait.max.ms = 500\n\treplica.high.watermark.checkpoint.interval.ms = 9223372036854775807\n\treplica.lag.time.max.ms = 30000\n\treplica.selector.class = null\n\treplica.socket.receive.buffer.bytes = 65536\n\treplica.socket.timeout.ms = 1000\n\treplication.quota.window.num = 11\n\treplication.quota.window.size.seconds = 1\n\trequest.timeout.ms = 30000\n\treserved.broker.max.id = 1000\n\tsasl.client.callback.handler.class = null\n\tsasl.enabled.mechanisms = [GSSAPI]\n\tsasl.jaas.config = null\n\tsasl.kerberos.kinit.cmd = /usr/bin/kinit\n\tsasl.kerberos.min.time.before.relogin = 60000\n\tsasl.kerberos.principal.to.local.rules = [DEFAULT]\n\tsasl.kerberos.service.name = null\n\tsasl.kerberos.ticket.renew.jitter = 0.05\n\tsasl.kerberos.ticket.renew.window.factor = 0.8\n\tsasl.login.callback.handler.class = null\n\tsasl.login.class = null\n\tsasl.login.connect.timeout.ms = null\n\tsasl.login.read.timeout.ms = null\n\tsasl.login.refresh.buffer.seconds = 300\n\tsasl.login.refresh.min.period.seconds = 60\n\tsasl.login.refresh.window.factor = 0.8\n\tsasl.login.refresh.window.jitter = 0.05\n\tsasl.login.retry.backoff.max.ms = 10000\n\tsasl.login.retry.backoff.ms = 100\n\tsasl.mechanism.controller.protocol = GSSAPI\n\tsasl.mechanism.inter.broker.protocol = GSSAPI\n\tsasl.oauthbearer.clock.skew.seconds = 30\n\tsasl.oauthbearer.expected.audience = null\n\tsasl.oauthbearer.expected.issuer = null\n\tsasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000\n\tsasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000\n\tsasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100\n\tsasl.oauthbearer.jwks.endpoint.url = null\n\tsasl.oauthbearer.scope.claim.name = scope\n\tsasl.oauthbearer.sub.claim.name = sub\n\tsasl.oauthbearer.token.endpoint.url = null\n\tsasl.server.callback.handler.class = null\n\tsasl.server.max.receive.size = 524288\n\tsecurity.inter.broker.protocol = PLAINTEXT\n\tsecurity.providers = null\n\tserver.max.startup.time.ms = 9223372036854775807\n\tsocket.connection.setup.timeout.max.ms = 30000\n\tsocket.connection.setup.timeout.ms = 10000\n\tsocket.listen.backlog.size = 50\n\tsocket.receive.buffer.bytes = 102400\n\tsocket.request.max.bytes = 104857600\n\tsocket.send.buffer.bytes = 102400\n\tssl.cipher.suites = []\n\tssl.client.auth = none\n\tssl.enabled.protocols = [TLSv1.2, TLSv1.3]\n\tssl.endpoint.identification.algorithm = https\n\tssl.engine.factory.class = null\n\tssl.key.password = null\n\tssl.keymanager.algorithm = SunX509\n\tssl.keystore.certificate.chain = null\n\tssl.keystore.key = null\n\tssl.keystore.location = null\n\tssl.keystore.password = null\n\tssl.keystore.type = JKS\n\tssl.principal.mapping.rules = DEFAULT\n\tssl.protocol = TLSv1.3\n\tssl.provider = null\n\tssl.secure.random.implementation = null\n\tssl.trustmanager.algorithm = PKIX\n\tssl.truststore.certificates = null\n\tssl.truststore.location = null\n\tssl.truststore.password = null\n\tssl.truststore.type = JKS\n\ttransaction.abort.timed.out.transaction.cleanup.interval.ms = 10000\n\ttransaction.max.timeout.ms = 900000\n\ttransaction.partition.verification.enable = true\n\ttransaction.remove.expired.transaction.cleanup.interval.ms = 3600000\n\ttransaction.state.log.load.buffer.size = 5242880\n\ttransaction.state.log.min.isr = 2\n\ttransaction.state.log.num.partitions = 50\n\ttransaction.state.log.replication.factor = 3\n\ttransaction.state.log.segment.bytes = 104857600\n\ttransactional.id.expiration.ms = 604800000\n\tunclean.leader.election.enable = false\n\tunstable.api.versions.enable = false\n\tzookeeper.clientCnxnSocket = null\n\tzookeeper.connect = 127.0.0.1:41533\n\tzookeeper.connection.timeout.ms = 10000\n\tzookeeper.max.in.flight.requests = 10\n\tzookeeper.metadata.migration.enable = false\n\tzookeeper.session.timeout.ms = 18000\n\tzookeeper.set.acl = false\n\tzookeeper.ssl.cipher.suites = null\n\tzookeeper.ssl.client.enable = false\n\tzookeeper.ssl.crl.enable = false\n\tzookeeper.ssl.enabled.protocols = null\n\tzookeeper.ssl.endpoint.identification.algorithm = HTTPS\n\tzookeeper.ssl.keystore.location = null\n\tzookeeper.ssl.keystore.password = null\n\tzookeeper.ssl.keystore.type = null\n\tzookeeper.ssl.ocsp.enable = false\n\tzookeeper.ssl.protocol = TLSv1.2\n\tzookeeper.ssl.truststore.location = null\n\tzookeeper.ssl.truststore.password = null\n\tzookeeper.ssl.truststore.type = null\n\n05:48:52.224 [zk-broker-0-to-controller-alter-partition-channel-manager] INFO kafka.server.BrokerToControllerRequestThread -- [zk-broker-0-to-controller-alter-partition-channel-manager]: Recorded new controller, from now on will use node localhost:35013 (id: 0 rack: null)\n05:48:52.224 [zk-broker-0-to-controller-forwarding-channel-manager] INFO kafka.server.BrokerToControllerRequestThread -- [zk-broker-0-to-controller-forwarding-channel-manager]: Recorded new controller, from now on will use node localhost:35013 (id: 0 rack: null)\n05:48:52.231 [ThrottledChannelReaper-Fetch] INFO kafka.server.ClientQuotaManager$ThrottledChannelReaper -- [ThrottledChannelReaper-Fetch]: Starting\n05:48:52.231 [ThrottledChannelReaper-Produce] INFO kafka.server.ClientQuotaManager$ThrottledChannelReaper -- [ThrottledChannelReaper-Produce]: Starting\n05:48:52.231 [ThrottledChannelReaper-Request] INFO kafka.server.ClientQuotaManager$ThrottledChannelReaper -- [ThrottledChannelReaper-Request]: Starting\n05:48:52.231 [ThrottledChannelReaper-ControllerMutation] INFO kafka.server.ClientQuotaManager$ThrottledChannelReaper -- [ThrottledChannelReaper-ControllerMutation]: Starting\n05:48:52.234 [main] INFO kafka.log.LogManager -- Loading logs from log dirs ArraySeq(/tmp/spring.kafka.69cecbcc-b8aa-44e4-b673-a5243dcc9dee11809573343017669845)\n05:48:52.235 [main] INFO kafka.log.LogManager -- No logs found to be loaded in /tmp/spring.kafka.69cecbcc-b8aa-44e4-b673-a5243dcc9dee11809573343017669845\n05:48:52.236 [main] INFO kafka.log.LogManager -- Loaded 0 logs in 1ms\n05:48:52.236 [main] INFO kafka.log.LogManager -- Starting log cleanup with a period of 300000 ms.\n05:48:52.237 [main] INFO kafka.log.LogManager -- Starting log flusher with a default period of 9223372036854775807 ms.\n05:48:52.237 [main] INFO kafka.log.LogCleaner -- Starting the log cleaner\n05:48:52.239 [kafka-log-cleaner-thread-0] INFO kafka.log.LogCleaner$CleanerThread -- [kafka-log-cleaner-thread-0]: Starting\n05:48:52.240 [feature-zk-node-event-process-thread] INFO kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread -- [feature-zk-node-event-process-thread]: Starting\n05:48:52.242 [feature-zk-node-event-process-thread] INFO kafka.server.metadata.ZkMetadataCache -- [MetadataCache brokerId=1] Updated cache from existing None to latest Features(version=3.6-IV2, finalizedFeatures={}, finalizedFeaturesEpoch=0).\n05:48:52.244 [zk-broker-1-to-controller-forwarding-channel-manager] INFO kafka.server.BrokerToControllerRequestThread -- [zk-broker-1-to-controller-forwarding-channel-manager]: Starting\n05:48:52.260 [main] INFO kafka.network.ConnectionQuotas -- Updated connection-accept-rate max connection creation rate to 2147483647\n05:48:52.261 [main] INFO kafka.network.DataPlaneAcceptor -- Awaiting socket connections on localhost:41935.\n05:48:52.261 [main] INFO kafka.network.DataPlaneAcceptor -- Opened wildcard endpoint localhost:41935\n05:48:52.263 [main] INFO kafka.network.SocketServer -- [SocketServer listenerType=ZK_BROKER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT)\n05:48:52.264 [zk-broker-1-to-controller-alter-partition-channel-manager] INFO kafka.server.BrokerToControllerRequestThread -- [zk-broker-1-to-controller-alter-partition-channel-manager]: Starting\n05:48:52.265 [ExpirationReaper-1-Produce] INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper -- [ExpirationReaper-1-Produce]: Starting\n05:48:52.266 [ExpirationReaper-1-Fetch] INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper -- [ExpirationReaper-1-Fetch]: Starting\n05:48:52.266 [ExpirationReaper-1-DeleteRecords] INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper -- [ExpirationReaper-1-DeleteRecords]: Starting\n05:48:52.267 [ExpirationReaper-1-ElectLeader] INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper -- [ExpirationReaper-1-ElectLeader]: Starting\n05:48:52.267 [ExpirationReaper-1-RemoteFetch] INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper -- [ExpirationReaper-1-RemoteFetch]: Starting\n05:48:52.268 [LogDirFailureHandler] INFO kafka.server.ReplicaManager$LogDirFailureHandler -- [LogDirFailureHandler]: Starting\n05:48:52.268 [AddPartitionsToTxnSenderThread-1] INFO kafka.server.AddPartitionsToTxnManager -- [AddPartitionsToTxnSenderThread-1]: Starting\n05:48:52.275 [main] INFO kafka.zk.KafkaZkClient -- Creating /brokers/ids/1 (is it secure? false)\n05:48:52.278 [main] INFO kafka.zk.KafkaZkClient -- Stat of the created znode at /brokers/ids/1 is: 45,45,1769665732276,1769665732276,1,0,0,72087306494017537,204,0,45\n\n05:48:52.278 [main] INFO kafka.zk.KafkaZkClient -- Registered broker 1 at path /brokers/ids/1 with addresses: PLAINTEXT://localhost:41935, czxid (broker epoch): 45\n05:48:52.284 [controller-event-thread] INFO kafka.controller.ControllerEventManager$ControllerEventThread -- [ControllerEventThread controllerId=1] Starting\n05:48:52.284 [controller-event-thread] INFO kafka.controller.KafkaController -- [Controller id=0] Newly added brokers: 1, deleted brokers: , bounced brokers: , all live brokers: 0,1\n05:48:52.284 [ExpirationReaper-1-topic] INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper -- [ExpirationReaper-1-topic]: Starting\n05:48:52.285 [ExpirationReaper-1-Heartbeat] INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper -- [ExpirationReaper-1-Heartbeat]: Starting\n05:48:52.286 [ExpirationReaper-1-Rebalance] INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper -- [ExpirationReaper-1-Rebalance]: Starting\n05:48:52.286 [main] INFO kafka.coordinator.group.GroupCoordinator -- [GroupCoordinator 1]: Starting up.\n05:48:52.287 [Controller-0-to-broker-1-send-thread] INFO kafka.controller.RequestSendThread -- [RequestSendThread controllerId=0] Starting\n05:48:52.288 [main] INFO kafka.coordinator.group.GroupCoordinator -- [GroupCoordinator 1]: Startup complete.\n05:48:52.288 [controller-event-thread] INFO kafka.controller.KafkaController -- [Controller id=0] New broker startup callback for 1\n05:48:52.288 [controller-event-thread] INFO state.change.logger -- [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 0 partitions\n05:48:52.289 [main] INFO kafka.coordinator.transaction.TransactionCoordinator -- [TransactionCoordinator id=1] Starting up.\n05:48:52.289 [controller-event-thread] INFO state.change.logger -- [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(1) for 0 partitions\n05:48:52.290 [Controller-0-to-broker-1-send-thread] INFO kafka.controller.RequestSendThread -- [RequestSendThread controllerId=0] Controller 0 connected to localhost:41935 (id: 1 rack: null) for sending state change requests\n05:48:52.291 [main] INFO kafka.coordinator.transaction.TransactionCoordinator -- [TransactionCoordinator id=1] Startup complete.\n05:48:52.291 [TxnMarkerSenderThread-1] INFO kafka.coordinator.transaction.TransactionMarkerChannelManager -- [TxnMarkerSenderThread-1]: Starting\n05:48:52.292 [ExpirationReaper-1-AlterAcls] INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper -- [ExpirationReaper-1-AlterAcls]: Starting\n05:48:52.293 [controller-event-thread] INFO kafka.controller.KafkaController -- [Controller id=0] Updated broker epochs cache: HashMap(0 -> 25, 1 -> 45)\n05:48:52.295 [/config/changes-event-process-thread] INFO kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread -- [/config/changes-event-process-thread]: Starting\n05:48:52.300 [main] INFO kafka.network.SocketServer -- [SocketServer listenerType=ZK_BROKER, nodeId=1] Enabling request processing.\n05:48:52.301 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- Kafka version: 3.6.1\n05:48:52.301 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- Kafka commitId: 5e3c2b738d253ff5\n05:48:52.301 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- Kafka startTimeMs: 1769665732301\n05:48:52.302 [main] INFO kafka.server.KafkaServer -- [KafkaServer id=1] started\n05:48:52.314 [main] INFO org.apache.kafka.clients.admin.AdminClientConfig -- AdminClientConfig values: \n\tauto.include.jmx.reporter = true\n\tbootstrap.servers = [127.0.0.1:35013, 127.0.0.1:41935]\n\tclient.dns.lookup = use_all_dns_ips\n\tclient.id = \n\tconnections.max.idle.ms = 300000\n\tdefault.api.timeout.ms = 60000\n\tmetadata.max.age.ms = 300000\n\tmetric.reporters = []\n\tmetrics.num.samples = 2\n\tmetrics.recording.level = INFO\n\tmetrics.sample.window.ms = 30000\n\treceive.buffer.bytes = 65536\n\treconnect.backoff.max.ms = 1000\n\treconnect.backoff.ms = 50\n\trequest.timeout.ms = 30000\n\tretries = 2147483647\n\tretry.backoff.ms = 100\n\tsasl.client.callback.handler.class = null\n\tsasl.jaas.config = null\n\tsasl.kerberos.kinit.cmd = /usr/bin/kinit\n\tsasl.kerberos.min.time.before.relogin = 60000\n\tsasl.kerberos.service.name = null\n\tsasl.kerberos.ticket.renew.jitter = 0.05\n\tsasl.kerberos.ticket.renew.window.factor = 0.8\n\tsasl.login.callback.handler.class = null\n\tsasl.login.class = null\n\tsasl.login.connect.timeout.ms = null\n\tsasl.login.read.timeout.ms = null\n\tsasl.login.refresh.buffer.seconds = 300\n\tsasl.login.refresh.min.period.seconds = 60\n\tsasl.login.refresh.window.factor = 0.8\n\tsasl.login.refresh.window.jitter = 0.05\n\tsasl.login.retry.backoff.max.ms = 10000\n\tsasl.login.retry.backoff.ms = 100\n\tsasl.mechanism = GSSAPI\n\tsasl.oauthbearer.clock.skew.seconds = 30\n\tsasl.oauthbearer.expected.audience = null\n\tsasl.oauthbearer.expected.issuer = null\n\tsasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000\n\tsasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000\n\tsasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100\n\tsasl.oauthbearer.jwks.endpoint.url = null\n\tsasl.oauthbearer.scope.claim.name = scope\n\tsasl.oauthbearer.sub.claim.name = sub\n\tsasl.oauthbearer.token.endpoint.url = null\n\tsecurity.protocol = PLAINTEXT\n\tsecurity.providers = null\n\tsend.buffer.bytes = 131072\n\tsocket.connection.setup.timeout.max.ms = 30000\n\tsocket.connection.setup.timeout.ms = 10000\n\tssl.cipher.suites = null\n\tssl.enabled.protocols = [TLSv1.2, TLSv1.3]\n\tssl.endpoint.identification.algorithm = https\n\tssl.engine.factory.class = null\n\tssl.key.password = null\n\tssl.keymanager.algorithm = SunX509\n\tssl.keystore.certificate.chain = null\n\tssl.keystore.key = null\n\tssl.keystore.location = null\n\tssl.keystore.password = null\n\tssl.keystore.type = JKS\n\tssl.protocol = TLSv1.3\n\tssl.provider = null\n\tssl.secure.random.implementation = null\n\tssl.trustmanager.algorithm = PKIX\n\tssl.truststore.certificates = null\n\tssl.truststore.location = null\n\tssl.truststore.password = null\n\tssl.truststore.type = JKS\n\n05:48:52.338 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- Kafka version: 3.6.1\n05:48:52.338 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- Kafka commitId: 5e3c2b738d253ff5\n05:48:52.338 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- Kafka startTimeMs: 1769665732338\n05:48:52.344 [zk-broker-1-to-controller-forwarding-channel-manager] INFO kafka.server.BrokerToControllerRequestThread -- [zk-broker-1-to-controller-forwarding-channel-manager]: Recorded new controller, from now on will use node localhost:35013 (id: 0 rack: null)\n05:48:52.364 [zk-broker-1-to-controller-alter-partition-channel-manager] INFO kafka.server.BrokerToControllerRequestThread -- [zk-broker-1-to-controller-alter-partition-channel-manager]: Recorded new controller, from now on will use node localhost:35013 (id: 0 rack: null)\n05:48:52.392 [data-plane-kafka-request-handler-5] INFO kafka.zk.AdminZkClient -- Creating topic messages with configuration {} and initial partition assignment HashMap(0 -> ArrayBuffer(1, 0), 1 -> ArrayBuffer(0, 1))\n05:48:52.410 [controller-event-thread] INFO kafka.controller.KafkaController -- [Controller id=0] New topics: [Set(messages)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(messages,Some(TAgOnGN8TCiusnCpThFLJQ),Map(messages-0 -> ReplicaAssignment(replicas=1,0, addingReplicas=, removingReplicas=), messages-1 -> ReplicaAssignment(replicas=0,1, addingReplicas=, removingReplicas=))))]\n05:48:52.411 [controller-event-thread] INFO kafka.controller.KafkaController -- [Controller id=0] New partition creation callback for messages-0,messages-1\n05:48:52.413 [controller-event-thread] INFO state.change.logger -- [Controller id=0 epoch=1] Changed partition messages-0 state from NonExistentPartition to NewPartition with assigned replicas 1,0\n05:48:52.413 [controller-event-thread] INFO state.change.logger -- [Controller id=0 epoch=1] Changed partition messages-1 state from NonExistentPartition to NewPartition with assigned replicas 0,1\n05:48:52.413 [controller-event-thread] INFO state.change.logger -- [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions\n05:48:52.416 [controller-event-thread] INFO state.change.logger -- [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions\n05:48:52.432 [controller-event-thread] INFO state.change.logger -- [Controller id=0 epoch=1] Changed partition messages-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=1, brokerEpoch=-1), BrokerState(brokerId=0, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0)\n05:48:52.432 [controller-event-thread] INFO state.change.logger -- [Controller id=0 epoch=1] Changed partition messages-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=0, brokerEpoch=-1), BrokerState(brokerId=1, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0)\n05:48:52.434 [controller-event-thread] INFO state.change.logger -- [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 1 become-leader and 1 become-follower partitions\n05:48:52.436 [controller-event-thread] INFO state.change.logger -- [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 1 with 1 become-leader and 1 become-follower partitions\n05:48:52.436 [controller-event-thread] INFO state.change.logger -- [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0, 1) for 2 partitions\n05:48:52.437 [controller-event-thread] INFO state.change.logger -- [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions\n05:48:52.441 [data-plane-kafka-request-handler-0] INFO state.change.logger -- [Broker id=0] Handling LeaderAndIsr request correlationId 2 from controller 0 for 2 partitions\n05:48:52.441 [data-plane-kafka-request-handler-1] INFO state.change.logger -- [Broker id=1] Handling LeaderAndIsr request correlationId 1 from controller 0 for 2 partitions\n05:48:52.461 [data-plane-kafka-request-handler-0] INFO kafka.server.ReplicaFetcherManager -- [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(messages-1)\n05:48:52.461 [data-plane-kafka-request-handler-1] INFO kafka.server.ReplicaFetcherManager -- [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(messages-0)\n05:48:52.462 [data-plane-kafka-request-handler-0] INFO state.change.logger -- [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 2 from controller 0 epoch 1 as part of the become-leader transition for 1 partitions\n05:48:52.463 [data-plane-kafka-request-handler-1] INFO state.change.logger -- [Broker id=1] Stopped fetchers as part of LeaderAndIsr request correlationId 1 from controller 0 epoch 1 as part of the become-leader transition for 1 partitions\n05:48:52.505 [data-plane-kafka-request-handler-0] INFO kafka.log.UnifiedLog$ -- [LogLoader partition=messages-1, dir=/tmp/spring.kafka.bca7ba27-5e93-400e-9aae-86960892bff71860864339420333442] Loading producer state till offset 0 with message format version 2\n05:48:52.505 [data-plane-kafka-request-handler-1] INFO kafka.log.UnifiedLog$ -- [LogLoader partition=messages-0, dir=/tmp/spring.kafka.69cecbcc-b8aa-44e4-b673-a5243dcc9dee11809573343017669845] Loading producer state till offset 0 with message format version 2\n05:48:52.515 [data-plane-kafka-request-handler-0] INFO kafka.log.LogManager -- Created log for partition messages-1 in /tmp/spring.kafka.bca7ba27-5e93-400e-9aae-86960892bff71860864339420333442/messages-1 with properties {}\n05:48:52.515 [data-plane-kafka-request-handler-1] INFO kafka.log.LogManager -- Created log for partition messages-0 in /tmp/spring.kafka.69cecbcc-b8aa-44e4-b673-a5243dcc9dee11809573343017669845/messages-0 with properties {}\n05:48:52.516 [data-plane-kafka-request-handler-1] INFO kafka.cluster.Partition -- [Partition messages-0 broker=1] No checkpointed highwatermark is found for partition messages-0\n05:48:52.517 [data-plane-kafka-request-handler-0] INFO kafka.cluster.Partition -- [Partition messages-1 broker=0] No checkpointed highwatermark is found for partition messages-1\n05:48:52.517 [data-plane-kafka-request-handler-1] INFO kafka.cluster.Partition -- [Partition messages-0 broker=1] Log loaded for partition messages-0 with initial high watermark 0\n05:48:52.517 [data-plane-kafka-request-handler-0] INFO kafka.cluster.Partition -- [Partition messages-1 broker=0] Log loaded for partition messages-1 with initial high watermark 0\n05:48:52.519 [data-plane-kafka-request-handler-1] INFO state.change.logger -- [Broker id=1] Leader messages-0 with topic id Some(TAgOnGN8TCiusnCpThFLJQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1,0], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1.\n05:48:52.520 [data-plane-kafka-request-handler-0] INFO state.change.logger -- [Broker id=0] Leader messages-1 with topic id Some(TAgOnGN8TCiusnCpThFLJQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0,1], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1.\n05:48:52.535 [data-plane-kafka-request-handler-1] INFO kafka.log.UnifiedLog$ -- [LogLoader partition=messages-1, dir=/tmp/spring.kafka.69cecbcc-b8aa-44e4-b673-a5243dcc9dee11809573343017669845] Loading producer state till offset 0 with message format version 2\n05:48:52.535 [data-plane-kafka-request-handler-0] INFO kafka.log.UnifiedLog$ -- [LogLoader partition=messages-0, dir=/tmp/spring.kafka.bca7ba27-5e93-400e-9aae-86960892bff71860864339420333442] Loading producer state till offset 0 with message format version 2\n05:48:52.536 [data-plane-kafka-request-handler-1] INFO kafka.log.LogManager -- Created log for partition messages-1 in /tmp/spring.kafka.69cecbcc-b8aa-44e4-b673-a5243dcc9dee11809573343017669845/messages-1 with properties {}\n05:48:52.536 [data-plane-kafka-request-handler-0] INFO kafka.log.LogManager -- Created log for partition messages-0 in /tmp/spring.kafka.bca7ba27-5e93-400e-9aae-86960892bff71860864339420333442/messages-0 with properties {}\n05:48:52.536 [data-plane-kafka-request-handler-1] INFO kafka.cluster.Partition -- [Partition messages-1 broker=1] No checkpointed highwatermark is found for partition messages-1\n05:48:52.536 [data-plane-kafka-request-handler-0] INFO kafka.cluster.Partition -- [Partition messages-0 broker=0] No checkpointed highwatermark is found for partition messages-0\n05:48:52.536 [data-plane-kafka-request-handler-1] INFO kafka.cluster.Partition -- [Partition messages-1 broker=1] Log loaded for partition messages-1 with initial high watermark 0\n05:48:52.536 [data-plane-kafka-request-handler-0] INFO kafka.cluster.Partition -- [Partition messages-0 broker=0] Log loaded for partition messages-0 with initial high watermark 0\n05:48:52.537 [data-plane-kafka-request-handler-1] INFO state.change.logger -- [Broker id=1] Follower messages-1 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 0. Previous leader Some(0) and previous leader epoch was 0.\n05:48:52.537 [data-plane-kafka-request-handler-0] INFO state.change.logger -- [Broker id=0] Follower messages-0 starts at leader epoch 0 from offset 0 with partition epoch 0 and high watermark 0. Current leader is 1. Previous leader Some(1) and previous leader epoch was 0.\n05:48:52.537 [data-plane-kafka-request-handler-1] INFO kafka.server.ReplicaFetcherManager -- [ReplicaFetcherManager on broker 1] Removed fetcher for partitions HashSet(messages-1)\n05:48:52.537 [data-plane-kafka-request-handler-0] INFO kafka.server.ReplicaFetcherManager -- [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(messages-0)\n05:48:52.537 [data-plane-kafka-request-handler-1] INFO state.change.logger -- [Broker id=1] Stopped fetchers as part of become-follower request from controller 0 epoch 1 with correlation id 1 for 1 partitions\n05:48:52.537 [data-plane-kafka-request-handler-0] INFO state.change.logger -- [Broker id=0] Stopped fetchers as part of become-follower request from controller 0 epoch 1 with correlation id 2 for 1 partitions\n05:48:52.554 [ReplicaFetcherThread-0-0] INFO kafka.server.ReplicaFetcherThread -- [ReplicaFetcherThread-0-0]: Starting\n05:48:52.554 [ReplicaFetcherThread-0-1] INFO kafka.server.ReplicaFetcherThread -- [ReplicaFetcherThread-0-1]: Starting\n05:48:52.556 [data-plane-kafka-request-handler-1] INFO kafka.server.ReplicaFetcherManager -- [ReplicaFetcherManager on broker 1] Added fetcher to broker 0 for partitions Map(messages-1 -> InitialFetchState(Some(TAgOnGN8TCiusnCpThFLJQ),BrokerEndPoint(id=0, host=localhost:35013),0,0))\n05:48:52.556 [data-plane-kafka-request-handler-0] INFO kafka.server.ReplicaFetcherManager -- [ReplicaFetcherManager on broker 0] Added fetcher to broker 1 for partitions Map(messages-0 -> InitialFetchState(Some(TAgOnGN8TCiusnCpThFLJQ),BrokerEndPoint(id=1, host=localhost:41935),0,0))\n05:48:52.558 [ReplicaFetcherThread-0-0] INFO kafka.server.ReplicaFetcherThread -- [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Truncating partition messages-1 with TruncationState(offset=0, completed=true) due to local high watermark 0\n05:48:52.558 [ReplicaFetcherThread-0-1] INFO kafka.server.ReplicaFetcherThread -- [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition messages-0 with TruncationState(offset=0, completed=true) due to local high watermark 0\n05:48:52.559 [ReplicaFetcherThread-0-0] INFO kafka.log.UnifiedLog -- [UnifiedLog partition=messages-1, dir=/tmp/spring.kafka.69cecbcc-b8aa-44e4-b673-a5243dcc9dee11809573343017669845] Truncating to 0 has no effect as the largest offset in the log is -1\n05:48:52.559 [ReplicaFetcherThread-0-1] INFO kafka.log.UnifiedLog -- [UnifiedLog partition=messages-0, dir=/tmp/spring.kafka.bca7ba27-5e93-400e-9aae-86960892bff71860864339420333442] Truncating to 0 has no effect as the largest offset in the log is -1\n05:48:52.567 [data-plane-kafka-request-handler-1] INFO state.change.logger -- [Broker id=1] Finished LeaderAndIsr request in 127ms correlationId 1 from controller 0 for 2 partitions\n05:48:52.567 [data-plane-kafka-request-handler-0] INFO state.change.logger -- [Broker id=0] Finished LeaderAndIsr request in 127ms correlationId 2 from controller 0 for 2 partitions\n05:48:52.574 [data-plane-kafka-request-handler-1] INFO state.change.logger -- [Broker id=0] Add 2 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 3\n05:48:52.574 [data-plane-kafka-request-handler-3] INFO state.change.logger -- [Broker id=1] Add 2 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 2\n05:48:52.580 [kafka-admin-client-thread | adminclient-1] INFO org.apache.kafka.common.utils.AppInfoParser -- App info kafka.admin.client for adminclient-1 unregistered\n05:48:52.583 [kafka-admin-client-thread | adminclient-1] INFO org.apache.kafka.common.metrics.Metrics -- Metrics scheduler closed\n05:48:52.583 [kafka-admin-client-thread | adminclient-1] INFO org.apache.kafka.common.metrics.Metrics -- Closing reporter org.apache.kafka.common.metrics.JmxReporter\n05:48:52.583 [kafka-admin-client-thread | adminclient-1] INFO org.apache.kafka.common.metrics.Metrics -- Metrics reporters closed\n05:48:52.595 [ReplicaFetcherThread-0-0] WARN kafka.server.ReplicaFetcherThread -- [ReplicaFetcher replicaId=1, leaderId=0, fetcherId=0] Received UNKNOWN_TOPIC_ID from the leader for partition messages-1. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.\n05:48:52.595 [ReplicaFetcherThread-0-1] WARN kafka.server.ReplicaFetcherThread -- [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Received UNKNOWN_TOPIC_ID from the leader for partition messages-0. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.\n05:48:52.732 [main] INFO org.apache.kafka.clients.consumer.ConsumerConfig -- ConsumerConfig values: \n\tallow.auto.create.topics = true\n\tauto.commit.interval.ms = 10\n\tauto.include.jmx.reporter = true\n\tauto.offset.reset = earliest\n\tbootstrap.servers = [127.0.0.1:35013, 127.0.0.1:41935]\n\tcheck.crcs = true\n\tclient.dns.lookup = use_all_dns_ips\n\tclient.id = consumer-sampleConsumer-1\n\tclient.rack = \n\tconnections.max.idle.ms = 540000\n\tdefault.api.timeout.ms = 60000\n\tenable.auto.commit = false\n\texclude.internal.topics = true\n\tfetch.max.bytes = 52428800\n\tfetch.max.wait.ms = 500\n\tfetch.min.bytes = 1\n\tgroup.id = sampleConsumer\n\tgroup.instance.id = null\n\theartbeat.interval.ms = 3000\n\tinterceptor.classes = []\n\tinternal.leave.group.on.close = true\n\tinternal.throw.on.fetch.stable.offset.unsupported = false\n\tisolation.level = read_uncommitted\n\tkey.deserializer = class org.apache.kafka.common.serialization.IntegerDeserializer\n\tmax.partition.fetch.bytes = 1048576\n\tmax.poll.interval.ms = 300000\n\tmax.poll.records = 500\n\tmetadata.max.age.ms = 300000\n\tmetric.reporters = []\n\tmetrics.num.samples = 2\n\tmetrics.recording.level = INFO\n\tmetrics.sample.window.ms = 30000\n\tpartition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]\n\treceive.buffer.bytes = 65536\n\treconnect.backoff.max.ms = 1000\n\treconnect.backoff.ms = 50\n\trequest.timeout.ms = 30000\n\tretry.backoff.ms = 100\n\tsasl.client.callback.handler.class = null\n\tsasl.jaas.config = null\n\tsasl.kerberos.kinit.cmd = /usr/bin/kinit\n\tsasl.kerberos.min.time.before.relogin = 60000\n\tsasl.kerberos.service.name = null\n\tsasl.kerberos.ticket.renew.jitter = 0.05\n\tsasl.kerberos.ticket.renew.window.factor = 0.8\n\tsasl.login.callback.handler.class = null\n\tsasl.login.class = null\n\tsasl.login.connect.timeout.ms = null\n\tsasl.login.read.timeout.ms = null\n\tsasl.login.refresh.buffer.seconds = 300\n\tsasl.login.refresh.min.period.seconds = 60\n\tsasl.login.refresh.window.factor = 0.8\n\tsasl.login.refresh.window.jitter = 0.05\n\tsasl.login.retry.backoff.max.ms = 10000\n\tsasl.login.retry.backoff.ms = 100\n\tsasl.mechanism = GSSAPI\n\tsasl.oauthbearer.clock.skew.seconds = 30\n\tsasl.oauthbearer.expected.audience = null\n\tsasl.oauthbearer.expected.issuer = null\n\tsasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000\n\tsasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000\n\tsasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100\n\tsasl.oauthbearer.jwks.endpoint.url = null\n\tsasl.oauthbearer.scope.claim.name = scope\n\tsasl.oauthbearer.sub.claim.name = sub\n\tsasl.oauthbearer.token.endpoint.url = null\n\tsecurity.protocol = PLAINTEXT\n\tsecurity.providers = null\n\tsend.buffer.bytes = 131072\n\tsession.timeout.ms = 60000\n\tsocket.connection.setup.timeout.max.ms = 30000\n\tsocket.connection.setup.timeout.ms = 10000\n\tssl.cipher.suites = null\n\tssl.enabled.protocols = [TLSv1.2, TLSv1.3]\n\tssl.endpoint.identification.algorithm = https\n\tssl.engine.factory.class = null\n\tssl.key.password = null\n\tssl.keymanager.algorithm = SunX509\n\tssl.keystore.certificate.chain = null\n\tssl.keystore.key = null\n\tssl.keystore.location = null\n\tssl.keystore.password = null\n\tssl.keystore.type = JKS\n\tssl.protocol = TLSv1.3\n\tssl.provider = null\n\tssl.secure.random.implementation = null\n\tssl.trustmanager.algorithm = PKIX\n\tssl.truststore.certificates = null\n\tssl.truststore.location = null\n\tssl.truststore.password = null\n\tssl.truststore.type = JKS\n\tvalue.deserializer = class org.apache.kafka.common.serialization.StringDeserializer\n\n05:48:52.789 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- Kafka version: 3.6.1\n05:48:52.790 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- Kafka commitId: 5e3c2b738d253ff5\n05:48:52.790 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- Kafka startTimeMs: 1769665732789\n05:48:52.793 [main] INFO org.springframework.kafka.core.DefaultKafkaConsumerFactory$ExtendedKafkaConsumer -- [Consumer clientId=consumer-sampleConsumer-1, groupId=sampleConsumer] Subscribed to topic(s): messages\n05:48:52.833 [sampleConsumer-C-1] INFO org.apache.kafka.clients.Metadata -- [Consumer clientId=consumer-sampleConsumer-1, groupId=sampleConsumer] Cluster ID: Rfe_PJqIS1WPJ2Ba9HwwNQ\n05:48:52.837 [data-plane-kafka-request-handler-2] INFO kafka.zk.AdminZkClient -- Creating topic __consumer_offsets with configuration {compression.type=producer, cleanup.policy=compact, segment.bytes=104857600} and initial partition assignment HashMap(0 -> ArrayBuffer(1), 1 -> ArrayBuffer(0), 2 -> ArrayBuffer(1), 3 -> ArrayBuffer(0), 4 -> ArrayBuffer(1))\n05:48:52.843 [controller-event-thread] INFO kafka.controller.KafkaController -- [Controller id=0] New topics: [Set(__consumer_offsets)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(__consumer_offsets,Some(D2P7Z7ZtQEybYd_QpFAodA),HashMap(__consumer_offsets-4 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-3 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-2 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-0 -> ReplicaAssignment(replicas=1, addingReplicas=, removingReplicas=), __consumer_offsets-1 -> ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))]\n05:48:52.844 [controller-event-thread] INFO kafka.controller.KafkaController -- [Controller id=0] New partition creation callback for __consumer_offsets-4,__consumer_offsets-3,__consumer_offsets-2,__consumer_offsets-0,__consumer_offsets-1\n05:48:52.844 [controller-event-thread] INFO state.change.logger -- [Controller id=0 epoch=1] Changed partition __consumer_offsets-4 state from NonExistentPartition to NewPartition with assigned replicas 1\n05:48:52.844 [controller-event-thread] INFO state.change.logger -- [Controller id=0 epoch=1] Changed partition __consumer_offsets-3 state from NonExistentPartition to NewPartition with assigned replicas 0\n05:48:52.844 [controller-event-thread] INFO state.change.logger -- [Controller id=0 epoch=1] Changed partition __consumer_offsets-2 state from NonExistentPartition to NewPartition with assigned replicas 1\n05:48:52.844 [controller-event-thread] INFO state.change.logger -- [Controller id=0 epoch=1] Changed partition __consumer_offsets-0 state from NonExistentPartition to NewPartition with assigned replicas 1\n05:48:52.844 [controller-event-thread] INFO state.change.logger -- [Controller id=0 epoch=1] Changed partition __consumer_offsets-1 state from NonExistentPartition to NewPartition with assigned replicas 0\n05:48:52.844 [controller-event-thread] INFO state.change.logger -- [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions\n05:48:52.844 [controller-event-thread] INFO state.change.logger -- [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions\n05:48:52.853 [controller-event-thread] INFO state.change.logger -- [Controller id=0 epoch=1] Changed partition __consumer_offsets-4 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=1, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0)\n05:48:52.853 [controller-event-thread] INFO state.change.logger -- [Controller id=0 epoch=1] Changed partition __consumer_offsets-3 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=0, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0)\n05:48:52.853 [controller-event-thread] INFO state.change.logger -- [Controller id=0 epoch=1] Changed partition __consumer_offsets-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=1, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0)\n05:48:52.853 [controller-event-thread] INFO state.change.logger -- [Controller id=0 epoch=1] Changed partition __consumer_offsets-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=1, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0)\n05:48:52.853 [controller-event-thread] INFO state.change.logger -- [Controller id=0 epoch=1] Changed partition __consumer_offsets-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isrWithBrokerEpoch=List(BrokerState(brokerId=0, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=0)\n05:48:52.853 [controller-event-thread] INFO state.change.logger -- [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 2 become-leader and 0 become-follower partitions\n05:48:52.854 [controller-event-thread] INFO state.change.logger -- [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 1 with 3 become-leader and 0 become-follower partitions\n05:48:52.854 [controller-event-thread] INFO state.change.logger -- [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0, 1) for 5 partitions\n05:48:52.854 [controller-event-thread] INFO state.change.logger -- [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions\n05:48:52.855 [data-plane-kafka-request-handler-0] INFO state.change.logger -- [Broker id=0] Handling LeaderAndIsr request correlationId 4 from controller 0 for 2 partitions\n05:48:52.855 [data-plane-kafka-request-handler-4] INFO state.change.logger -- [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 0 for 3 partitions\n05:48:52.857 [data-plane-kafka-request-handler-4] INFO kafka.server.ReplicaFetcherManager -- [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(__consumer_offsets-2, __consumer_offsets-4, __consumer_offsets-0)\n05:48:52.857 [data-plane-kafka-request-handler-0] INFO kafka.server.ReplicaFetcherManager -- [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(__consumer_offsets-3, __consumer_offsets-1)\n05:48:52.857 [data-plane-kafka-request-handler-4] INFO state.change.logger -- [Broker id=1] Stopped fetchers as part of LeaderAndIsr request correlationId 3 from controller 0 epoch 1 as part of the become-leader transition for 3 partitions\n05:48:52.857 [data-plane-kafka-request-handler-0] INFO state.change.logger -- [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 4 from controller 0 epoch 1 as part of the become-leader transition for 2 partitions\n05:48:52.861 [data-plane-kafka-request-handler-4] INFO kafka.log.UnifiedLog$ -- [LogLoader partition=__consumer_offsets-2, dir=/tmp/spring.kafka.69cecbcc-b8aa-44e4-b673-a5243dcc9dee11809573343017669845] Loading producer state till offset 0 with message format version 2\n05:48:52.862 [data-plane-kafka-request-handler-4] INFO kafka.log.LogManager -- Created log for partition __consumer_offsets-2 in /tmp/spring.kafka.69cecbcc-b8aa-44e4-b673-a5243dcc9dee11809573343017669845/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600}\n05:48:52.862 [data-plane-kafka-request-handler-4] INFO kafka.cluster.Partition -- [Partition __consumer_offsets-2 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-2\n05:48:52.862 [data-plane-kafka-request-handler-4] INFO kafka.cluster.Partition -- [Partition __consumer_offsets-2 broker=1] Log loaded for partition __consumer_offsets-2 with initial high watermark 0\n05:48:52.862 [data-plane-kafka-request-handler-4] INFO state.change.logger -- [Broker id=1] Leader __consumer_offsets-2 with topic id Some(D2P7Z7ZtQEybYd_QpFAodA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1.\n05:48:52.866 [main] INFO org.apache.kafka.clients.producer.ProducerConfig -- ProducerConfig values: \n\tacks = -1\n\tauto.include.jmx.reporter = true\n\tbatch.size = 16384\n\tbootstrap.servers = [127.0.0.1:35013, 127.0.0.1:41935]\n\tbuffer.memory = 33554432\n\tclient.dns.lookup = use_all_dns_ips\n\tclient.id = producer-1\n\tcompression.type = none\n\tconnections.max.idle.ms = 540000\n\tdelivery.timeout.ms = 120000\n\tenable.idempotence = true\n\tinterceptor.classes = []\n\tkey.serializer = class org.apache.kafka.common.serialization.IntegerSerializer\n\tlinger.ms = 1\n\tmax.block.ms = 60000\n\tmax.in.flight.requests.per.connection = 5\n\tmax.request.size = 1048576\n\tmetadata.max.age.ms = 300000\n\tmetadata.max.idle.ms = 300000\n\tmetric.reporters = []\n\tmetrics.num.samples = 2\n\tmetrics.recording.level = INFO\n\tmetrics.sample.window.ms = 30000\n\tpartitioner.adaptive.partitioning.enable = true\n\tpartitioner.availability.timeout.ms = 0\n\tpartitioner.class = null\n\tpartitioner.ignore.keys = false\n\treceive.buffer.bytes = 32768\n\treconnect.backoff.max.ms = 1000\n\treconnect.backoff.ms = 50\n\trequest.timeout.ms = 30000\n\tretries = 2147483647\n\tretry.backoff.ms = 100\n\tsasl.client.callback.handler.class = null\n\tsasl.jaas.config = null\n\tsasl.kerberos.kinit.cmd = /usr/bin/kinit\n\tsasl.kerberos.min.time.before.relogin = 60000\n\tsasl.kerberos.service.name = null\n\tsasl.kerberos.ticket.renew.jitter = 0.05\n\tsasl.kerberos.ticket.renew.window.factor = 0.8\n\tsasl.login.callback.handler.class = null\n\tsasl.login.class = null\n\tsasl.login.connect.timeout.ms = null\n\tsasl.login.read.timeout.ms = null\n\tsasl.login.refresh.buffer.seconds = 300\n\tsasl.login.refresh.min.period.seconds = 60\n\tsasl.login.refresh.window.factor = 0.8\n\tsasl.login.refresh.window.jitter = 0.05\n\tsasl.login.retry.backoff.max.ms = 10000\n\tsasl.login.retry.backoff.ms = 100\n\tsasl.mechanism = GSSAPI\n\tsasl.oauthbearer.clock.skew.seconds = 30\n\tsasl.oauthbearer.expected.audience = null\n\tsasl.oauthbearer.expected.issuer = null\n\tsasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000\n\tsasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000\n\tsasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100\n\tsasl.oauthbearer.jwks.endpoint.url = null\n\tsasl.oauthbearer.scope.claim.name = scope\n\tsasl.oauthbearer.sub.claim.name = sub\n\tsasl.oauthbearer.token.endpoint.url = null\n\tsecurity.protocol = PLAINTEXT\n\tsecurity.providers = null\n\tsend.buffer.bytes = 131072\n\tsocket.connection.setup.timeout.max.ms = 30000\n\tsocket.connection.setup.timeout.ms = 10000\n\tssl.cipher.suites = null\n\tssl.enabled.protocols = [TLSv1.2, TLSv1.3]\n\tssl.endpoint.identification.algorithm = https\n\tssl.engine.factory.class = null\n\tssl.key.password = null\n\tssl.keymanager.algorithm = SunX509\n\tssl.keystore.certificate.chain = null\n\tssl.keystore.key = null\n\tssl.keystore.location = null\n\tssl.keystore.password = null\n\tssl.keystore.type = JKS\n\tssl.protocol = TLSv1.3\n\tssl.provider = null\n\tssl.secure.random.implementation = null\n\tssl.trustmanager.algorithm = PKIX\n\tssl.truststore.certificates = null\n\tssl.truststore.location = null\n\tssl.truststore.password = null\n\tssl.truststore.type = JKS\n\ttransaction.timeout.ms = 60000\n\ttransactional.id = null\n\tvalue.serializer = class org.apache.kafka.common.serialization.StringSerializer\n\n05:48:52.873 [data-plane-kafka-request-handler-0] INFO kafka.log.UnifiedLog$ -- [LogLoader partition=__consumer_offsets-3, dir=/tmp/spring.kafka.bca7ba27-5e93-400e-9aae-86960892bff71860864339420333442] Loading producer state till offset 0 with message format version 2\n05:48:52.873 [data-plane-kafka-request-handler-4] INFO kafka.log.UnifiedLog$ -- [LogLoader partition=__consumer_offsets-4, dir=/tmp/spring.kafka.69cecbcc-b8aa-44e4-b673-a5243dcc9dee11809573343017669845] Loading producer state till offset 0 with message format version 2\n05:48:52.874 [data-plane-kafka-request-handler-0] INFO kafka.log.LogManager -- Created log for partition __consumer_offsets-3 in /tmp/spring.kafka.bca7ba27-5e93-400e-9aae-86960892bff71860864339420333442/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600}\n05:48:52.874 [data-plane-kafka-request-handler-0] INFO kafka.cluster.Partition -- [Partition __consumer_offsets-3 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-3\n05:48:52.874 [data-plane-kafka-request-handler-0] INFO kafka.cluster.Partition -- [Partition __consumer_offsets-3 broker=0] Log loaded for partition __consumer_offsets-3 with initial high watermark 0\n05:48:52.874 [data-plane-kafka-request-handler-4] INFO kafka.log.LogManager -- Created log for partition __consumer_offsets-4 in /tmp/spring.kafka.69cecbcc-b8aa-44e4-b673-a5243dcc9dee11809573343017669845/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600}\n05:48:52.874 [data-plane-kafka-request-handler-0] INFO state.change.logger -- [Broker id=0] Leader __consumer_offsets-3 with topic id Some(D2P7Z7ZtQEybYd_QpFAodA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1.\n05:48:52.874 [data-plane-kafka-request-handler-4] INFO kafka.cluster.Partition -- [Partition __consumer_offsets-4 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-4\n05:48:52.874 [data-plane-kafka-request-handler-4] INFO kafka.cluster.Partition -- [Partition __consumer_offsets-4 broker=1] Log loaded for partition __consumer_offsets-4 with initial high watermark 0\n05:48:52.875 [data-plane-kafka-request-handler-4] INFO state.change.logger -- [Broker id=1] Leader __consumer_offsets-4 with topic id Some(D2P7Z7ZtQEybYd_QpFAodA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1.\n05:48:52.875 [main] INFO org.apache.kafka.clients.producer.KafkaProducer -- [Producer clientId=producer-1] Instantiated an idempotent producer.\n05:48:52.882 [data-plane-kafka-request-handler-0] INFO kafka.log.UnifiedLog$ -- [LogLoader partition=__consumer_offsets-1, dir=/tmp/spring.kafka.bca7ba27-5e93-400e-9aae-86960892bff71860864339420333442] Loading producer state till offset 0 with message format version 2\n05:48:52.882 [data-plane-kafka-request-handler-0] INFO kafka.log.LogManager -- Created log for partition __consumer_offsets-1 in /tmp/spring.kafka.bca7ba27-5e93-400e-9aae-86960892bff71860864339420333442/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600}\n05:48:52.882 [data-plane-kafka-request-handler-0] INFO kafka.cluster.Partition -- [Partition __consumer_offsets-1 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-1\n05:48:52.882 [data-plane-kafka-request-handler-0] INFO kafka.cluster.Partition -- [Partition __consumer_offsets-1 broker=0] Log loaded for partition __consumer_offsets-1 with initial high watermark 0\n05:48:52.882 [data-plane-kafka-request-handler-0] INFO state.change.logger -- [Broker id=0] Leader __consumer_offsets-1 with topic id Some(D2P7Z7ZtQEybYd_QpFAodA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [0], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1.\n05:48:52.884 [data-plane-kafka-request-handler-4] INFO kafka.log.UnifiedLog$ -- [LogLoader partition=__consumer_offsets-0, dir=/tmp/spring.kafka.69cecbcc-b8aa-44e4-b673-a5243dcc9dee11809573343017669845] Loading producer state till offset 0 with message format version 2\n05:48:52.886 [data-plane-kafka-request-handler-4] INFO kafka.log.LogManager -- Created log for partition __consumer_offsets-0 in /tmp/spring.kafka.69cecbcc-b8aa-44e4-b673-a5243dcc9dee11809573343017669845/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type=\"producer\", segment.bytes=104857600}\n05:48:52.886 [data-plane-kafka-request-handler-4] INFO kafka.cluster.Partition -- [Partition __consumer_offsets-0 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-0\n05:48:52.886 [data-plane-kafka-request-handler-4] INFO kafka.cluster.Partition -- [Partition __consumer_offsets-0 broker=1] Log loaded for partition __consumer_offsets-0 with initial high watermark 0\n05:48:52.886 [data-plane-kafka-request-handler-4] INFO state.change.logger -- [Broker id=1] Leader __consumer_offsets-0 with topic id Some(D2P7Z7ZtQEybYd_QpFAodA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [] and removing replicas [] . Previous leader None and previous leader epoch was -1.\n05:48:52.886 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- Kafka version: 3.6.1\n05:48:52.887 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- Kafka commitId: 5e3c2b738d253ff5\n05:48:52.887 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- Kafka startTimeMs: 1769665732886\n05:48:52.889 [data-plane-kafka-request-handler-0] INFO kafka.coordinator.group.GroupCoordinator -- [GroupCoordinator 0]: Elected as the group coordinator for partition 3 in epoch 0\n05:48:52.890 [data-plane-kafka-request-handler-0] INFO kafka.coordinator.group.GroupMetadataManager -- [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-3 for epoch 0\n05:48:52.891 [data-plane-kafka-request-handler-0] INFO kafka.coordinator.group.GroupCoordinator -- [GroupCoordinator 0]: Elected as the group coordinator for partition 1 in epoch 0\n05:48:52.891 [data-plane-kafka-request-handler-0] INFO kafka.coordinator.group.GroupMetadataManager -- [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 0\n05:48:52.891 [data-plane-kafka-request-handler-0] INFO state.change.logger -- [Broker id=0] Finished LeaderAndIsr request in 36ms correlationId 4 from controller 0 for 2 partitions\n05:48:52.891 [data-plane-kafka-request-handler-4] INFO kafka.coordinator.group.GroupCoordinator -- [GroupCoordinator 1]: Elected as the group coordinator for partition 2 in epoch 0\n05:48:52.891 [data-plane-kafka-request-handler-4] INFO kafka.coordinator.group.GroupMetadataManager -- [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 0\n05:48:52.891 [data-plane-kafka-request-handler-4] INFO kafka.coordinator.group.GroupCoordinator -- [GroupCoordinator 1]: Elected as the group coordinator for partition 4 in epoch 0\n05:48:52.892 [data-plane-kafka-request-handler-4] INFO kafka.coordinator.group.GroupMetadataManager -- [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 0\n05:48:52.892 [data-plane-kafka-request-handler-4] INFO kafka.coordinator.group.GroupCoordinator -- [GroupCoordinator 1]: Elected as the group coordinator for partition 0 in epoch 0\n05:48:52.892 [data-plane-kafka-request-handler-4] INFO kafka.coordinator.group.GroupMetadataManager -- [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0 for epoch 0\n05:48:52.892 [data-plane-kafka-request-handler-4] INFO state.change.logger -- [Broker id=1] Finished LeaderAndIsr request in 37ms correlationId 3 from controller 0 for 3 partitions\n05:48:52.893 [data-plane-kafka-request-handler-4] INFO state.change.logger -- [Broker id=0] Add 5 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 5\n05:48:52.894 [kafka-producer-network-thread | producer-1] INFO org.apache.kafka.clients.Metadata -- [Producer clientId=producer-1] Cluster ID: Rfe_PJqIS1WPJ2Ba9HwwNQ\n05:48:52.895 [data-plane-kafka-request-handler-0] INFO state.change.logger -- [Broker id=1] Add 5 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 4\n05:48:52.899 [group-metadata-manager-0] INFO kafka.coordinator.group.GroupMetadataManager -- [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-2 in 5 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler.\n05:48:52.899 [group-metadata-manager-0] INFO kafka.coordinator.group.GroupMetadataManager -- [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-3 in 6 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler.\n05:48:52.900 [group-metadata-manager-0] INFO kafka.coordinator.group.GroupMetadataManager -- [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-4 in 7 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler.\n05:48:52.900 [group-metadata-manager-0] INFO kafka.coordinator.group.GroupMetadataManager -- [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-1 in 9 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler.\n05:48:52.900 [group-metadata-manager-0] INFO kafka.coordinator.group.GroupMetadataManager -- [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 8 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler.\n05:48:52.920 [controller-event-thread] INFO kafka.controller.KafkaController -- [Controller id=0] Acquired new producerId block ProducerIdsBlock(assignedBrokerId=0, firstProducerId=0, size=1000) by writing to Zk with path version 1\n05:48:52.944 [sampleConsumer-C-1] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-sampleConsumer-1, groupId=sampleConsumer] Discovered group coordinator localhost:41935 (id: 2147483646 rack: null)\n05:48:52.948 [sampleConsumer-C-1] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-sampleConsumer-1, groupId=sampleConsumer] (Re-)joining group\n05:48:52.982 [data-plane-kafka-request-handler-2] INFO kafka.coordinator.group.GroupCoordinator -- [GroupCoordinator 1]: Dynamic member with unknown member id joins group sampleConsumer in Empty state. Created a new member id consumer-sampleConsumer-1-5c03fd55-2c18-4b5e-a1d3-bb143e1707fc and request the member to rejoin with this id.\n05:48:52.985 [sampleConsumer-C-1] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-sampleConsumer-1, groupId=sampleConsumer] Request joining group due to: need to re-join with the given member-id: consumer-sampleConsumer-1-5c03fd55-2c18-4b5e-a1d3-bb143e1707fc\n05:48:52.986 [sampleConsumer-C-1] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-sampleConsumer-1, groupId=sampleConsumer] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)\n05:48:52.986 [sampleConsumer-C-1] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-sampleConsumer-1, groupId=sampleConsumer] (Re-)joining group\n05:48:52.991 [data-plane-kafka-request-handler-4] INFO kafka.coordinator.group.GroupCoordinator -- [GroupCoordinator 1]: Preparing to rebalance group sampleConsumer in state PreparingRebalance with old generation 0 (__consumer_offsets-2) (reason: Adding new member consumer-sampleConsumer-1-5c03fd55-2c18-4b5e-a1d3-bb143e1707fc with group instance id None; client reason: rebalance failed due to MemberIdRequiredException)\n05:48:52.996 [executor-Rebalance] INFO kafka.coordinator.group.GroupCoordinator -- [GroupCoordinator 1]: Stabilized group sampleConsumer generation 1 (__consumer_offsets-2) with 1 members\n05:48:52.998 [sampleConsumer-C-1] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-sampleConsumer-1, groupId=sampleConsumer] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sampleConsumer-1-5c03fd55-2c18-4b5e-a1d3-bb143e1707fc', protocol='range'}\n05:48:53.003 [sampleConsumer-C-1] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-sampleConsumer-1, groupId=sampleConsumer] Finished assignment for group at generation 1: {consumer-sampleConsumer-1-5c03fd55-2c18-4b5e-a1d3-bb143e1707fc=Assignment(partitions=[messages-0, messages-1])}\n05:48:53.008 [data-plane-kafka-request-handler-5] INFO kafka.coordinator.group.GroupCoordinator -- [GroupCoordinator 1]: Assignment received from leader consumer-sampleConsumer-1-5c03fd55-2c18-4b5e-a1d3-bb143e1707fc for group sampleConsumer for generation 1. The group has 1 members, 0 of which are static.\n05:48:53.013 [controller-event-thread] INFO kafka.controller.KafkaController -- [Controller id=0] Acquired new producerId block ProducerIdsBlock(assignedBrokerId=1, firstProducerId=1000, size=1000) by writing to Zk with path version 2\n05:48:53.037 [sampleConsumer-C-1] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-sampleConsumer-1, groupId=sampleConsumer] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sampleConsumer-1-5c03fd55-2c18-4b5e-a1d3-bb143e1707fc', protocol='range'}\n05:48:53.038 [sampleConsumer-C-1] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-sampleConsumer-1, groupId=sampleConsumer] Notifying assignor about the new Assignment(partitions=[messages-0, messages-1])\n05:48:53.039 [sampleConsumer-C-1] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-sampleConsumer-1, groupId=sampleConsumer] Adding newly assigned partitions: messages-0, messages-1\n05:48:53.040 [sampleConsumer-C-1] INFO org.springframework.kafka.listener.KafkaMessageListenerContainer -- sampleConsumer: partitions assigned: [messages-0, messages-1]\n05:48:53.052 [sampleConsumer-C-1] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-sampleConsumer-1, groupId=sampleConsumer] Found no committed offset for partition messages-0\n05:48:53.052 [sampleConsumer-C-1] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-sampleConsumer-1, groupId=sampleConsumer] Found no committed offset for partition messages-1\n05:48:53.063 [sampleConsumer-C-1] INFO org.apache.kafka.clients.consumer.internals.SubscriptionState -- [Consumer clientId=consumer-sampleConsumer-1, groupId=sampleConsumer] Resetting offset for partition messages-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41935 (id: 1 rack: null)], epoch=0}}.\n05:48:53.063 [sampleConsumer-C-1] INFO org.apache.kafka.clients.consumer.internals.SubscriptionState -- [Consumer clientId=consumer-sampleConsumer-1, groupId=sampleConsumer] Resetting offset for partition messages-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35013 (id: 0 rack: null)], epoch=0}}.\n05:48:53.110 [kafka-producer-network-thread | producer-1] INFO org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-1] ProducerId set to 1000 with epoch 0\n05:48:53.626 [sampleConsumer-C-1] INFO com.zen.lab.services.program.consumer.LiveFeedPojoListenerTest -- Receiving: ConsumerRecord(topic = messages, partition = 0, leaderEpoch = 0, offset = 0, CreateTime = 1769665732895, serialized key size = 4, serialized value size = 8, headers = RecordHeaders(headers = [], isReadOnly = false), key = 0, value = message1)\n05:48:53.827 [sampleConsumer-C-1] INFO com.zen.lab.services.program.consumer.LiveFeedPojoListenerTest -- Receiving: ConsumerRecord(topic = messages, partition = 0, leaderEpoch = 0, offset = 1, CreateTime = 1769665732912, serialized key size = 4, serialized value size = 8, headers = RecordHeaders(headers = [], isReadOnly = false), key = 1, value = message2)\n05:48:54.027 [sampleConsumer-C-1] INFO com.zen.lab.services.program.consumer.LiveFeedPojoListenerTest -- Receiving: ConsumerRecord(topic = messages, partition = 1, leaderEpoch = 0, offset = 0, CreateTime = 1769665732912, serialized key size = 4, serialized value size = 8, headers = RecordHeaders(headers = [], isReadOnly = false), key = 2, value = message3)\n05:48:54.227 [sampleConsumer-C-1] INFO com.zen.lab.services.program.consumer.LiveFeedPojoListenerTest -- Receiving: ConsumerRecord(topic = messages, partition = 1, leaderEpoch = 0, offset = 1, CreateTime = 1769665732912, serialized key size = 4, serialized value size = 8, headers = RecordHeaders(headers = [], isReadOnly = false), key = 3, value = message4)\n05:48:54.492 [sampleConsumer-C-1] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-sampleConsumer-1, groupId=sampleConsumer] Revoke previously assigned partitions messages-0, messages-1\n05:48:54.493 [sampleConsumer-C-1] INFO org.springframework.kafka.listener.KafkaMessageListenerContainer -- sampleConsumer: partitions revoked: [messages-0, messages-1]\n05:48:54.493 [sampleConsumer-C-1] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-sampleConsumer-1, groupId=sampleConsumer] Member consumer-sampleConsumer-1-5c03fd55-2c18-4b5e-a1d3-bb143e1707fc sending LeaveGroup request to coordinator localhost:41935 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics\n05:48:54.494 [sampleConsumer-C-1] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-sampleConsumer-1, groupId=sampleConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group\n05:48:54.494 [sampleConsumer-C-1] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-sampleConsumer-1, groupId=sampleConsumer] Request joining group due to: consumer pro-actively leaving the group\n05:48:54.494 [sampleConsumer-C-1] INFO org.springframework.kafka.core.DefaultKafkaConsumerFactory$ExtendedKafkaConsumer -- [Consumer clientId=consumer-sampleConsumer-1, groupId=sampleConsumer] Unsubscribed all topics or patterns and assigned partitions\n05:48:54.495 [sampleConsumer-C-1] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-sampleConsumer-1, groupId=sampleConsumer] Resetting generation and member id due to: consumer pro-actively leaving the group\n05:48:54.495 [sampleConsumer-C-1] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-sampleConsumer-1, groupId=sampleConsumer] Request joining group due to: consumer pro-actively leaving the group\n05:48:54.497 [data-plane-kafka-request-handler-0] INFO kafka.coordinator.group.GroupCoordinator -- [GroupCoordinator 1]: Preparing to rebalance group sampleConsumer in state PreparingRebalance with old generation 1 (__consumer_offsets-2) (reason: Removing member consumer-sampleConsumer-1-5c03fd55-2c18-4b5e-a1d3-bb143e1707fc on LeaveGroup; client reason: the consumer unsubscribed from all topics)\n05:48:54.498 [data-plane-kafka-request-handler-0] INFO kafka.coordinator.group.GroupCoordinator -- [GroupCoordinator 1]: Group sampleConsumer with generation 2 is now empty (__consumer_offsets-2)\n05:48:54.501 [data-plane-kafka-request-handler-0] INFO kafka.coordinator.group.GroupCoordinator -- [GroupCoordinator 1]: Member MemberMetadata(memberId=consumer-sampleConsumer-1-5c03fd55-2c18-4b5e-a1d3-bb143e1707fc, groupInstanceId=None, clientId=consumer-sampleConsumer-1, clientHost=/127.0.0.1, sessionTimeoutMs=60000, rebalanceTimeoutMs=300000, supportedProtocols=List(range, cooperative-sticky)) has left group sampleConsumer through explicit `LeaveGroup`; client reason: the consumer unsubscribed from all topics\n05:48:54.947 [sampleConsumer-C-1] INFO org.apache.kafka.common.metrics.Metrics -- Metrics scheduler closed\n05:48:54.947 [sampleConsumer-C-1] INFO org.apache.kafka.common.metrics.Metrics -- Closing reporter org.apache.kafka.common.metrics.JmxReporter\n05:48:54.948 [sampleConsumer-C-1] INFO org.apache.kafka.common.metrics.Metrics -- Metrics reporters closed\n05:48:54.949 [sampleConsumer-C-1] INFO org.apache.kafka.common.utils.AppInfoParser -- App info kafka.consumer for consumer-sampleConsumer-1 unregistered\n05:48:54.949 [sampleConsumer-C-1] INFO org.springframework.kafka.listener.KafkaMessageListenerContainer -- sampleConsumer: Consumer stopped\n05:48:54.954 [main] INFO org.apache.kafka.clients.producer.ProducerConfig -- ProducerConfig values: \n\tacks = -1\n\tauto.include.jmx.reporter = true\n\tbatch.size = 16384\n\tbootstrap.servers = [127.0.0.1:35013, 127.0.0.1:41935]\n\tbuffer.memory = 33554432\n\tclient.dns.lookup = use_all_dns_ips\n\tclient.id = producer-2\n\tcompression.type = none\n\tconnections.max.idle.ms = 540000\n\tdelivery.timeout.ms = 120000\n\tenable.idempotence = true\n\tinterceptor.classes = []\n\tkey.serializer = class org.apache.kafka.common.serialization.IntegerSerializer\n\tlinger.ms = 1\n\tmax.block.ms = 60000\n\tmax.in.flight.requests.per.connection = 5\n\tmax.request.size = 1048576\n\tmetadata.max.age.ms = 300000\n\tmetadata.max.idle.ms = 300000\n\tmetric.reporters = []\n\tmetrics.num.samples = 2\n\tmetrics.recording.level = INFO\n\tmetrics.sample.window.ms = 30000\n\tpartitioner.adaptive.partitioning.enable = true\n\tpartitioner.availability.timeout.ms = 0\n\tpartitioner.class = null\n\tpartitioner.ignore.keys = false\n\treceive.buffer.bytes = 32768\n\treconnect.backoff.max.ms = 1000\n\treconnect.backoff.ms = 50\n\trequest.timeout.ms = 30000\n\tretries = 2147483647\n\tretry.backoff.ms = 100\n\tsasl.client.callback.handler.class = null\n\tsasl.jaas.config = null\n\tsasl.kerberos.kinit.cmd = /usr/bin/kinit\n\tsasl.kerberos.min.time.before.relogin = 60000\n\tsasl.kerberos.service.name = null\n\tsasl.kerberos.ticket.renew.jitter = 0.05\n\tsasl.kerberos.ticket.renew.window.factor = 0.8\n\tsasl.login.callback.handler.class = null\n\tsasl.login.class = null\n\tsasl.login.connect.timeout.ms = null\n\tsasl.login.read.timeout.ms = null\n\tsasl.login.refresh.buffer.seconds = 300\n\tsasl.login.refresh.min.period.seconds = 60\n\tsasl.login.refresh.window.factor = 0.8\n\tsasl.login.refresh.window.jitter = 0.05\n\tsasl.login.retry.backoff.max.ms = 10000\n\tsasl.login.retry.backoff.ms = 100\n\tsasl.mechanism = GSSAPI\n\tsasl.oauthbearer.clock.skew.seconds = 30\n\tsasl.oauthbearer.expected.audience = null\n\tsasl.oauthbearer.expected.issuer = null\n\tsasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000\n\tsasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000\n\tsasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100\n\tsasl.oauthbearer.jwks.endpoint.url = null\n\tsasl.oauthbearer.scope.claim.name = scope\n\tsasl.oauthbearer.sub.claim.name = sub\n\tsasl.oauthbearer.token.endpoint.url = null\n\tsecurity.protocol = PLAINTEXT\n\tsecurity.providers = null\n\tsend.buffer.bytes = 131072\n\tsocket.connection.setup.timeout.max.ms = 30000\n\tsocket.connection.setup.timeout.ms = 10000\n\tssl.cipher.suites = null\n\tssl.enabled.protocols = [TLSv1.2, TLSv1.3]\n\tssl.endpoint.identification.algorithm = https\n\tssl.engine.factory.class = null\n\tssl.key.password = null\n\tssl.keymanager.algorithm = SunX509\n\tssl.keystore.certificate.chain = null\n\tssl.keystore.key = null\n\tssl.keystore.location = null\n\tssl.keystore.password = null\n\tssl.keystore.type = JKS\n\tssl.protocol = TLSv1.3\n\tssl.provider = null\n\tssl.secure.random.implementation = null\n\tssl.trustmanager.algorithm = PKIX\n\tssl.truststore.certificates = null\n\tssl.truststore.location = null\n\tssl.truststore.password = null\n\tssl.truststore.type = JKS\n\ttransaction.timeout.ms = 60000\n\ttransactional.id = null\n\tvalue.serializer = class org.apache.kafka.common.serialization.StringSerializer\n\n05:48:54.954 [main] INFO org.apache.kafka.clients.producer.KafkaProducer -- [Producer clientId=producer-2] Instantiated an idempotent producer.\n05:48:54.955 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- Kafka version: 3.6.1\n05:48:54.956 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- Kafka commitId: 5e3c2b738d253ff5\n05:48:54.956 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- Kafka startTimeMs: 1769665734955\n05:48:54.959 [kafka-producer-network-thread | producer-2] INFO org.apache.kafka.clients.Metadata -- [Producer clientId=producer-2] Cluster ID: Rfe_PJqIS1WPJ2Ba9HwwNQ\n05:48:54.959 [kafka-producer-network-thread | producer-2] INFO org.apache.kafka.clients.producer.internals.TransactionManager -- [Producer clientId=producer-2] ProducerId set to 0 with epoch 0\n05:48:54.982 [pool-14-thread-1] INFO org.apache.kafka.clients.consumer.ConsumerConfig -- ConsumerConfig values: \n\tallow.auto.create.topics = true\n\tauto.commit.interval.ms = 10\n\tauto.include.jmx.reporter = true\n\tauto.offset.reset = earliest\n\tbootstrap.servers = [127.0.0.1:35013, 127.0.0.1:41935]\n\tcheck.crcs = true\n\tclient.dns.lookup = use_all_dns_ips\n\tclient.id = consumer-sampleRawConsumer-2\n\tclient.rack = \n\tconnections.max.idle.ms = 540000\n\tdefault.api.timeout.ms = 60000\n\tenable.auto.commit = false\n\texclude.internal.topics = true\n\tfetch.max.bytes = 52428800\n\tfetch.max.wait.ms = 500\n\tfetch.min.bytes = 1\n\tgroup.id = sampleRawConsumer\n\tgroup.instance.id = null\n\theartbeat.interval.ms = 3000\n\tinterceptor.classes = []\n\tinternal.leave.group.on.close = true\n\tinternal.throw.on.fetch.stable.offset.unsupported = false\n\tisolation.level = read_uncommitted\n\tkey.deserializer = class org.apache.kafka.common.serialization.IntegerDeserializer\n\tmax.partition.fetch.bytes = 1048576\n\tmax.poll.interval.ms = 300000\n\tmax.poll.records = 500\n\tmetadata.max.age.ms = 300000\n\tmetric.reporters = []\n\tmetrics.num.samples = 2\n\tmetrics.recording.level = INFO\n\tmetrics.sample.window.ms = 30000\n\tpartition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]\n\treceive.buffer.bytes = 65536\n\treconnect.backoff.max.ms = 1000\n\treconnect.backoff.ms = 50\n\trequest.timeout.ms = 30000\n\tretry.backoff.ms = 100\n\tsasl.client.callback.handler.class = null\n\tsasl.jaas.config = null\n\tsasl.kerberos.kinit.cmd = /usr/bin/kinit\n\tsasl.kerberos.min.time.before.relogin = 60000\n\tsasl.kerberos.service.name = null\n\tsasl.kerberos.ticket.renew.jitter = 0.05\n\tsasl.kerberos.ticket.renew.window.factor = 0.8\n\tsasl.login.callback.handler.class = null\n\tsasl.login.class = null\n\tsasl.login.connect.timeout.ms = null\n\tsasl.login.read.timeout.ms = null\n\tsasl.login.refresh.buffer.seconds = 300\n\tsasl.login.refresh.min.period.seconds = 60\n\tsasl.login.refresh.window.factor = 0.8\n\tsasl.login.refresh.window.jitter = 0.05\n\tsasl.login.retry.backoff.max.ms = 10000\n\tsasl.login.retry.backoff.ms = 100\n\tsasl.mechanism = GSSAPI\n\tsasl.oauthbearer.clock.skew.seconds = 30\n\tsasl.oauthbearer.expected.audience = null\n\tsasl.oauthbearer.expected.issuer = null\n\tsasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000\n\tsasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000\n\tsasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100\n\tsasl.oauthbearer.jwks.endpoint.url = null\n\tsasl.oauthbearer.scope.claim.name = scope\n\tsasl.oauthbearer.sub.claim.name = sub\n\tsasl.oauthbearer.token.endpoint.url = null\n\tsecurity.protocol = PLAINTEXT\n\tsecurity.providers = null\n\tsend.buffer.bytes = 131072\n\tsession.timeout.ms = 60000\n\tsocket.connection.setup.timeout.max.ms = 30000\n\tsocket.connection.setup.timeout.ms = 10000\n\tssl.cipher.suites = null\n\tssl.enabled.protocols = [TLSv1.2, TLSv1.3]\n\tssl.endpoint.identification.algorithm = https\n\tssl.engine.factory.class = null\n\tssl.key.password = null\n\tssl.keymanager.algorithm = SunX509\n\tssl.keystore.certificate.chain = null\n\tssl.keystore.key = null\n\tssl.keystore.location = null\n\tssl.keystore.password = null\n\tssl.keystore.type = JKS\n\tssl.protocol = TLSv1.3\n\tssl.provider = null\n\tssl.secure.random.implementation = null\n\tssl.trustmanager.algorithm = PKIX\n\tssl.truststore.certificates = null\n\tssl.truststore.location = null\n\tssl.truststore.password = null\n\tssl.truststore.type = JKS\n\tvalue.deserializer = class org.apache.kafka.common.serialization.StringDeserializer\n\n05:48:54.984 [pool-14-thread-1] INFO org.apache.kafka.common.utils.AppInfoParser -- Kafka version: 3.6.1\n05:48:54.984 [pool-14-thread-1] INFO org.apache.kafka.common.utils.AppInfoParser -- Kafka commitId: 5e3c2b738d253ff5\n05:48:54.984 [pool-14-thread-1] INFO org.apache.kafka.common.utils.AppInfoParser -- Kafka startTimeMs: 1769665734984\n05:48:54.984 [pool-14-thread-1] INFO org.apache.kafka.clients.consumer.KafkaConsumer -- [Consumer clientId=consumer-sampleRawConsumer-2, groupId=sampleRawConsumer] Subscribed to topic(s): messages\n05:48:54.987 [pool-14-thread-1] INFO org.apache.kafka.clients.Metadata -- [Consumer clientId=consumer-sampleRawConsumer-2, groupId=sampleRawConsumer] Cluster ID: Rfe_PJqIS1WPJ2Ba9HwwNQ\n05:48:54.987 [pool-14-thread-1] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-sampleRawConsumer-2, groupId=sampleRawConsumer] Discovered group coordinator localhost:41935 (id: 2147483646 rack: null)\n05:48:54.988 [pool-14-thread-1] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-sampleRawConsumer-2, groupId=sampleRawConsumer] (Re-)joining group\n05:48:54.989 [data-plane-kafka-request-handler-0] INFO kafka.coordinator.group.GroupCoordinator -- [GroupCoordinator 1]: Dynamic member with unknown member id joins group sampleRawConsumer in Empty state. Created a new member id consumer-sampleRawConsumer-2-55013b00-6524-4857-880c-25a15adeb70e and request the member to rejoin with this id.\n05:48:54.989 [pool-14-thread-1] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-sampleRawConsumer-2, groupId=sampleRawConsumer] Request joining group due to: need to re-join with the given member-id: consumer-sampleRawConsumer-2-55013b00-6524-4857-880c-25a15adeb70e\n05:48:54.990 [pool-14-thread-1] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-sampleRawConsumer-2, groupId=sampleRawConsumer] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)\n05:48:54.990 [pool-14-thread-1] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-sampleRawConsumer-2, groupId=sampleRawConsumer] (Re-)joining group\n05:48:54.990 [data-plane-kafka-request-handler-1] INFO kafka.coordinator.group.GroupCoordinator -- [GroupCoordinator 1]: Preparing to rebalance group sampleRawConsumer in state PreparingRebalance with old generation 0 (__consumer_offsets-0) (reason: Adding new member consumer-sampleRawConsumer-2-55013b00-6524-4857-880c-25a15adeb70e with group instance id None; client reason: rebalance failed due to MemberIdRequiredException)\n05:48:54.990 [executor-Rebalance] INFO kafka.coordinator.group.GroupCoordinator -- [GroupCoordinator 1]: Stabilized group sampleRawConsumer generation 1 (__consumer_offsets-0) with 1 members\n05:48:54.991 [pool-14-thread-1] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-sampleRawConsumer-2, groupId=sampleRawConsumer] Successfully joined group with generation Generation{generationId=1, memberId='consumer-sampleRawConsumer-2-55013b00-6524-4857-880c-25a15adeb70e', protocol='range'}\n05:48:54.991 [pool-14-thread-1] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-sampleRawConsumer-2, groupId=sampleRawConsumer] Finished assignment for group at generation 1: {consumer-sampleRawConsumer-2-55013b00-6524-4857-880c-25a15adeb70e=Assignment(partitions=[messages-0, messages-1])}\n05:48:54.991 [data-plane-kafka-request-handler-3] INFO kafka.coordinator.group.GroupCoordinator -- [GroupCoordinator 1]: Assignment received from leader consumer-sampleRawConsumer-2-55013b00-6524-4857-880c-25a15adeb70e for group sampleRawConsumer for generation 1. The group has 1 members, 0 of which are static.\n05:48:54.993 [pool-14-thread-1] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-sampleRawConsumer-2, groupId=sampleRawConsumer] Successfully synced group in generation Generation{generationId=1, memberId='consumer-sampleRawConsumer-2-55013b00-6524-4857-880c-25a15adeb70e', protocol='range'}\n05:48:54.993 [pool-14-thread-1] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-sampleRawConsumer-2, groupId=sampleRawConsumer] Notifying assignor about the new Assignment(partitions=[messages-0, messages-1])\n05:48:54.993 [pool-14-thread-1] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-sampleRawConsumer-2, groupId=sampleRawConsumer] Adding newly assigned partitions: messages-0, messages-1\n05:48:54.994 [pool-14-thread-1] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-sampleRawConsumer-2, groupId=sampleRawConsumer] Found no committed offset for partition messages-0\n05:48:54.994 [pool-14-thread-1] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-sampleRawConsumer-2, groupId=sampleRawConsumer] Found no committed offset for partition messages-1\n05:48:54.996 [pool-14-thread-1] INFO org.apache.kafka.clients.consumer.internals.SubscriptionState -- [Consumer clientId=consumer-sampleRawConsumer-2, groupId=sampleRawConsumer] Resetting offset for partition messages-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:41935 (id: 1 rack: null)], epoch=0}}.\n05:48:54.997 [pool-14-thread-1] INFO org.apache.kafka.clients.consumer.internals.SubscriptionState -- [Consumer clientId=consumer-sampleRawConsumer-2, groupId=sampleRawConsumer] Resetting offset for partition messages-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:35013 (id: 0 rack: null)], epoch=0}}.\n05:48:54.998 [pool-14-thread-1] INFO com.zen.lab.services.program.consumer.LiveFeedPojoListenerTest -- consuming from topic = messages, partition = 0, offset = 0, key = 0, value = message1\n05:48:54.998 [pool-14-thread-1] INFO com.zen.lab.services.program.consumer.LiveFeedPojoListenerTest -- consuming from topic = messages, partition = 0, offset = 1, key = 1, value = message2\n05:48:54.998 [pool-14-thread-1] INFO com.zen.lab.services.program.consumer.LiveFeedPojoListenerTest -- consuming from topic = messages, partition = 0, offset = 2, key = 0, value = message0\n05:48:54.998 [pool-14-thread-1] INFO com.zen.lab.services.program.consumer.LiveFeedPojoListenerTest -- consuming from topic = messages, partition = 0, offset = 3, key = 1, value = message1\n05:48:54.999 [pool-14-thread-1] INFO com.zen.lab.services.program.consumer.LiveFeedPojoListenerTest -- consuming from topic = messages, partition = 1, offset = 0, key = 2, value = message3\n05:48:54.999 [pool-14-thread-1] INFO com.zen.lab.services.program.consumer.LiveFeedPojoListenerTest -- consuming from topic = messages, partition = 1, offset = 1, key = 3, value = message4\n05:48:54.999 [pool-14-thread-1] INFO com.zen.lab.services.program.consumer.LiveFeedPojoListenerTest -- consuming from topic = messages, partition = 1, offset = 2, key = 2, value = message2\n05:48:54.999 [pool-14-thread-1] INFO com.zen.lab.services.program.consumer.LiveFeedPojoListenerTest -- consuming from topic = messages, partition = 1, offset = 3, key = 3, value = message3\n05:48:54.999 [main] INFO kafka.server.KafkaServer -- [KafkaServer id=0] shutting down\n05:48:55.000 [main] INFO kafka.server.KafkaServer -- [KafkaServer id=0] Starting controlled shutdown\n05:48:55.003 [controller-event-thread] INFO kafka.controller.KafkaController -- [Controller id=0] Shutting down broker 0\n05:48:55.016 [controller-event-thread] INFO state.change.logger -- [Controller id=0 epoch=1] Changed partition messages-1 from OnlinePartition to OnlinePartition with state LeaderAndIsr(leader=1, leaderEpoch=1, isrWithBrokerEpoch=List(BrokerState(brokerId=1, brokerEpoch=-1)), leaderRecoveryState=RECOVERED, partitionEpoch=1)\n05:48:55.016 [controller-event-thread] INFO state.change.logger -- [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 0 become-leader and 1 become-follower partitions\n05:48:55.016 [controller-event-thread] INFO state.change.logger -- [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 1 with 1 become-leader and 0 become-follower partitions\n05:48:55.017 [controller-event-thread] INFO state.change.logger -- [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0, 1) for 1 partitions\n05:48:55.017 [data-plane-kafka-request-handler-1] INFO state.change.logger -- [Broker id=0] Handling LeaderAndIsr request correlationId 6 from controller 0 for 1 partitions\n05:48:55.017 [data-plane-kafka-request-handler-0] INFO state.change.logger -- [Broker id=1] Handling LeaderAndIsr request correlationId 5 from controller 0 for 1 partitions\n05:48:55.017 [data-plane-kafka-request-handler-0] INFO kafka.server.ReplicaFetcherManager -- [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(messages-1)\n05:48:55.017 [data-plane-kafka-request-handler-0] INFO state.change.logger -- [Broker id=1] Stopped fetchers as part of LeaderAndIsr request correlationId 5 from controller 0 epoch 1 as part of the become-leader transition for 1 partitions\n05:48:55.018 [data-plane-kafka-request-handler-1] INFO state.change.logger -- [Broker id=0] Follower messages-1 starts at leader epoch 1 from offset 4 with partition epoch 1 and high watermark 4. Current leader is 1. Previous leader Some(1) and previous leader epoch was 1.\n05:48:55.018 [data-plane-kafka-request-handler-0] INFO state.change.logger -- [Broker id=1] Leader messages-1 with topic id Some(TAgOnGN8TCiusnCpThFLJQ) starts at leader epoch 1 from offset 4 with partition epoch 1, high watermark 3, ISR [1], adding replicas [] and removing replicas [] . Previous leader Some(0) and previous leader epoch was 0.\n05:48:55.019 [data-plane-kafka-request-handler-1] INFO kafka.server.ReplicaFetcherManager -- [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(messages-1)\n05:48:55.019 [data-plane-kafka-request-handler-1] INFO state.change.logger -- [Broker id=0] Stopped fetchers as part of become-follower request from controller 0 epoch 1 with correlation id 6 for 1 partitions\n05:48:55.020 [controller-event-thread] INFO state.change.logger -- [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions\n05:48:55.021 [controller-event-thread] INFO state.change.logger -- [Controller id=0 epoch=1] Sending StopReplica request for 1 replicas to broker 0\n05:48:55.022 [data-plane-kafka-request-handler-1] INFO state.change.logger -- [Broker id=0] Finished LeaderAndIsr request in 5ms correlationId 6 from controller 0 for 1 partitions\n05:48:55.023 [data-plane-kafka-request-handler-4] INFO state.change.logger -- [Broker id=0] Add 1 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 7\n05:48:55.024 [data-plane-kafka-request-handler-5] INFO state.change.logger -- [Broker id=0] Handling StopReplica request correlationId 8 from controller 0 for 1 partitions\n05:48:55.026 [data-plane-kafka-request-handler-0] INFO kafka.server.ReplicaFetcherThread -- [ReplicaFetcherThread-0-0]: Shutting down\n05:48:55.026 [data-plane-kafka-request-handler-5] INFO kafka.server.ReplicaFetcherManager -- [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(messages-0)\n05:48:55.026 [data-plane-kafka-request-handler-5] INFO kafka.server.ReplicaAlterLogDirsManager -- [ReplicaAlterLogDirsManager on broker 0] Removed fetcher for partitions Set(messages-0)\n05:48:55.026 [ReplicaFetcherThread-0-0] INFO kafka.server.ReplicaFetcherThread -- [ReplicaFetcherThread-0-0]: Stopped\n05:48:55.026 [data-plane-kafka-request-handler-0] INFO kafka.server.ReplicaFetcherThread -- [ReplicaFetcherThread-0-0]: Shutdown completed\n05:48:55.027 [data-plane-kafka-request-handler-0] INFO state.change.logger -- [Broker id=1] Finished LeaderAndIsr request in 10ms correlationId 5 from controller 0 for 1 partitions\n05:48:55.028 [data-plane-kafka-request-handler-1] INFO state.change.logger -- [Broker id=1] Add 1 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 6\n05:48:55.028 [controller-event-thread] INFO state.change.logger -- [Controller id=0 epoch=1] Partition messages-0 state changed to (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:1,ZkVersion:1,ControllerEpoch:1) after removing replica 0 from the ISR as part of transition to OfflineReplica\n05:48:55.029 [data-plane-kafka-request-handler-5] INFO kafka.server.ReplicaFetcherThread -- [ReplicaFetcherThread-0-1]: Shutting down\n05:48:55.029 [ReplicaFetcherThread-0-1] INFO org.apache.kafka.clients.NetworkClient -- [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Client requested connection close from node 1\n05:48:55.029 [ReplicaFetcherThread-0-1] INFO org.apache.kafka.clients.NetworkClient -- [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Cancelled in-flight FETCH request with correlation id 7 due to node 1 being disconnected (elapsed time since creation: 60ms, elapsed time since send: 60ms, request timeout: 30000ms)\n05:48:55.030 [controller-event-thread] INFO state.change.logger -- [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 1 with 1 become-leader and 0 become-follower partitions\n05:48:55.030 [controller-event-thread] INFO state.change.logger -- [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0, 1) for 1 partitions\n05:48:55.030 [controller-event-thread] INFO state.change.logger -- [Controller id=0 epoch=1] Sending StopReplica request for 1 replicas to broker 0\n05:48:55.030 [data-plane-kafka-request-handler-3] INFO state.change.logger -- [Broker id=1] Handling LeaderAndIsr request correlationId 7 from controller 0 for 1 partitions\n05:48:55.030 [data-plane-kafka-request-handler-3] INFO kafka.server.ReplicaFetcherManager -- [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(messages-0)\n05:48:55.030 [data-plane-kafka-request-handler-3] INFO state.change.logger -- [Broker id=1] Stopped fetchers as part of LeaderAndIsr request correlationId 7 from controller 0 epoch 1 as part of the become-leader transition for 1 partitions\n05:48:55.030 [data-plane-kafka-request-handler-7] INFO state.change.logger -- [Broker id=0] Add 1 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 9\n05:48:55.030 [data-plane-kafka-request-handler-3] INFO state.change.logger -- [Broker id=1] Leader messages-0 with topic id Some(TAgOnGN8TCiusnCpThFLJQ) starts at leader epoch 1 from offset 4 with partition epoch 1, high watermark 4, ISR [1], adding replicas [] and removing replicas [] . Previous leader Some(1) and previous leader epoch was 0.\n05:48:55.029 [ReplicaFetcherThread-0-1] INFO org.apache.kafka.clients.FetchSessionHandler -- [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error sending fetch request (sessionId=5236946, epoch=5) to node 1:\njava.io.IOException: Client was shutdown before response was read\n\tat org.apache.kafka.clients.NetworkClientUtils.sendAndReceive(NetworkClientUtils.java:108)\n\tat kafka.server.BrokerBlockingSender.sendRequest(BrokerBlockingSender.scala:113)\n\tat kafka.server.RemoteLeaderEndPoint.fetch(RemoteLeaderEndPoint.scala:79)\n\tat kafka.server.AbstractFetcherThread.processFetchRequest(AbstractFetcherThread.scala:316)\n\tat kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3(AbstractFetcherThread.scala:130)\n\tat kafka.server.AbstractFetcherThread.$anonfun$maybeFetch$3$adapted(AbstractFetcherThread.scala:129)\n\tat scala.Option.foreach(Option.scala:437)\n\tat kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:129)\n\tat kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:112)\n\tat kafka.server.ReplicaFetcherThread.doWork(ReplicaFetcherThread.scala:98)\n\tat org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:130)\n05:48:55.031 [ReplicaFetcherThread-0-1] INFO kafka.server.ReplicaFetcherThread -- [ReplicaFetcherThread-0-1]: Stopped\n05:48:55.031 [data-plane-kafka-request-handler-5] INFO kafka.server.ReplicaFetcherThread -- [ReplicaFetcherThread-0-1]: Shutdown completed\n05:48:55.031 [data-plane-kafka-request-handler-0] INFO state.change.logger -- [Broker id=0] Handling StopReplica request correlationId 10 from controller 0 for 1 partitions\n05:48:55.031 [data-plane-kafka-request-handler-0] INFO kafka.server.ReplicaFetcherManager -- [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(messages-0)\n05:48:55.031 [data-plane-kafka-request-handler-0] INFO kafka.server.ReplicaAlterLogDirsManager -- [ReplicaAlterLogDirsManager on broker 0] Removed fetcher for partitions Set(messages-0)\n05:48:55.031 [main] INFO kafka.server.KafkaServer -- [KafkaServer id=0] Controlled shutdown request returned successfully after 30ms\n05:48:55.033 [main] INFO kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread -- [/config/changes-event-process-thread]: Shutting down\n05:48:55.033 [main] INFO kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread -- [/config/changes-event-process-thread]: Shutdown completed\n05:48:55.033 [/config/changes-event-process-thread] INFO kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread -- [/config/changes-event-process-thread]: Stopped\n05:48:55.034 [main] INFO kafka.network.SocketServer -- [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopping socket server request processors\n05:48:55.035 [data-plane-kafka-request-handler-3] INFO state.change.logger -- [Broker id=1] Finished LeaderAndIsr request in 5ms correlationId 7 from controller 0 for 1 partitions\n05:48:55.035 [kafka-producer-network-thread | producer-1] INFO org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-1] Node -1 disconnected.\n05:48:55.035 [zk-broker-1-to-controller-forwarding-channel-manager] INFO org.apache.kafka.clients.NetworkClient -- [BrokerToControllerChannelManager id=1 name=forwarding] Node 0 disconnected.\n05:48:55.035 [pool-14-thread-1] INFO org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-sampleRawConsumer-2, groupId=sampleRawConsumer] Node 0 disconnected.\n05:48:55.035 [kafka-producer-network-thread | producer-2] INFO org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-2] Node 0 disconnected.\n05:48:55.035 [pool-14-thread-1] INFO org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-sampleRawConsumer-2, groupId=sampleRawConsumer] Cancelled in-flight FETCH request with correlation id 30 due to node 0 being disconnected (elapsed time since creation: 1ms, elapsed time since send: 1ms, request timeout: 30000ms)\n05:48:55.036 [zk-broker-0-to-controller-forwarding-channel-manager] INFO org.apache.kafka.clients.NetworkClient -- [BrokerToControllerChannelManager id=0 name=forwarding] Node 0 disconnected.\n05:48:55.036 [data-plane-kafka-request-handler-2] INFO state.change.logger -- [Broker id=1] Add 1 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 8\n05:48:55.036 [pool-14-thread-1] INFO org.apache.kafka.clients.FetchSessionHandler -- [Consumer clientId=consumer-sampleRawConsumer-2, groupId=sampleRawConsumer] Error sending fetch request (sessionId=1318414656, epoch=16) to node 0:\norg.apache.kafka.common.errors.DisconnectException: null\n05:48:55.036 [kafka-producer-network-thread | producer-2] INFO org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-2] Node -1 disconnected.\n05:48:55.036 [kafka-producer-network-thread | producer-1] INFO org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-1] Node 0 disconnected.\n05:48:55.037 [main] INFO kafka.network.SocketServer -- [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopped socket server request processors\n05:48:55.037 [main] INFO kafka.server.KafkaRequestHandlerPool -- [data-plane Kafka Request Handler on Broker 0], shutting down\n05:48:55.038 [main] INFO kafka.server.KafkaRequestHandlerPool -- [data-plane Kafka Request Handler on Broker 0], shut down completely\n05:48:55.039 [main] INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper -- [ExpirationReaper-0-AlterAcls]: Shutting down\n05:48:55.039 [ExpirationReaper-0-AlterAcls] INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper -- [ExpirationReaper-0-AlterAcls]: Stopped\n05:48:55.039 [main] INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper -- [ExpirationReaper-0-AlterAcls]: Shutdown completed\n05:48:55.039 [main] INFO kafka.server.KafkaApis -- [KafkaApi-0] Shutdown complete.\n05:48:55.040 [main] INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper -- [ExpirationReaper-0-topic]: Shutting down\n05:48:55.040 [ExpirationReaper-0-topic] INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper -- [ExpirationReaper-0-topic]: Stopped\n05:48:55.040 [main] INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper -- [ExpirationReaper-0-topic]: Shutdown completed\n05:48:55.041 [main] INFO kafka.coordinator.transaction.TransactionCoordinator -- [TransactionCoordinator id=0] Shutting down.\n05:48:55.041 [main] INFO kafka.coordinator.transaction.TransactionStateManager -- [Transaction State Manager 0]: Shutdown complete\n05:48:55.041 [main] INFO kafka.coordinator.transaction.TransactionMarkerChannelManager -- [TxnMarkerSenderThread-0]: Shutting down\n05:48:55.041 [TxnMarkerSenderThread-0] INFO kafka.coordinator.transaction.TransactionMarkerChannelManager -- [TxnMarkerSenderThread-0]: Stopped\n05:48:55.041 [main] INFO kafka.coordinator.transaction.TransactionMarkerChannelManager -- [TxnMarkerSenderThread-0]: Shutdown completed\n05:48:55.042 [main] INFO kafka.coordinator.transaction.TransactionCoordinator -- [TransactionCoordinator id=0] Shutdown complete.\n05:48:55.042 [main] INFO kafka.coordinator.group.GroupCoordinator -- [GroupCoordinator 0]: Shutting down.\n05:48:55.042 [main] INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper -- [ExpirationReaper-0-Heartbeat]: Shutting down\n05:48:55.042 [main] INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper -- [ExpirationReaper-0-Heartbeat]: Shutdown completed\n05:48:55.042 [ExpirationReaper-0-Heartbeat] INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper -- [ExpirationReaper-0-Heartbeat]: Stopped\n05:48:55.042 [main] INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper -- [ExpirationReaper-0-Rebalance]: Shutting down\n05:48:55.043 [ExpirationReaper-0-Rebalance] INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper -- [ExpirationReaper-0-Rebalance]: Stopped\n05:48:55.043 [main] INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper -- [ExpirationReaper-0-Rebalance]: Shutdown completed\n05:48:55.043 [main] INFO kafka.coordinator.group.GroupCoordinator -- [GroupCoordinator 0]: Shutdown complete.\n05:48:55.043 [main] INFO kafka.server.ReplicaManager -- [ReplicaManager broker=0] Shutting down\n05:48:55.044 [main] INFO kafka.server.ReplicaManager$LogDirFailureHandler -- [LogDirFailureHandler]: Shutting down\n05:48:55.044 [LogDirFailureHandler] INFO kafka.server.ReplicaManager$LogDirFailureHandler -- [LogDirFailureHandler]: Stopped\n05:48:55.044 [main] INFO kafka.server.ReplicaManager$LogDirFailureHandler -- [LogDirFailureHandler]: Shutdown completed\n05:48:55.044 [main] INFO kafka.server.ReplicaFetcherManager -- [ReplicaFetcherManager on broker 0] shutting down\n05:48:55.045 [main] INFO kafka.server.ReplicaFetcherManager -- [ReplicaFetcherManager on broker 0] shutdown completed\n05:48:55.045 [main] INFO kafka.server.ReplicaAlterLogDirsManager -- [ReplicaAlterLogDirsManager on broker 0] shutting down\n05:48:55.045 [main] INFO kafka.server.ReplicaAlterLogDirsManager -- [ReplicaAlterLogDirsManager on broker 0] shutdown completed\n05:48:55.045 [main] INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper -- [ExpirationReaper-0-Fetch]: Shutting down\n05:48:55.045 [ExpirationReaper-0-Fetch] INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper -- [ExpirationReaper-0-Fetch]: Stopped\n05:48:55.045 [main] INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper -- [ExpirationReaper-0-Fetch]: Shutdown completed\n05:48:55.045 [main] INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper -- [ExpirationReaper-0-RemoteFetch]: Shutting down\n05:48:55.045 [ExpirationReaper-0-RemoteFetch] INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper -- [ExpirationReaper-0-RemoteFetch]: Stopped\n05:48:55.045 [main] INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper -- [ExpirationReaper-0-RemoteFetch]: Shutdown completed\n05:48:55.045 [main] INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper -- [ExpirationReaper-0-Produce]: Shutting down\n05:48:55.046 [main] INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper -- [ExpirationReaper-0-Produce]: Shutdown completed\n05:48:55.046 [ExpirationReaper-0-Produce] INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper -- [ExpirationReaper-0-Produce]: Stopped\n05:48:55.046 [main] INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper -- [ExpirationReaper-0-DeleteRecords]: Shutting down\n05:48:55.046 [ExpirationReaper-0-DeleteRecords] INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper -- [ExpirationReaper-0-DeleteRecords]: Stopped\n05:48:55.046 [main] INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper -- [ExpirationReaper-0-DeleteRecords]: Shutdown completed\n05:48:55.046 [main] INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper -- [ExpirationReaper-0-ElectLeader]: Shutting down\n05:48:55.046 [ExpirationReaper-0-ElectLeader] INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper -- [ExpirationReaper-0-ElectLeader]: Stopped\n05:48:55.046 [main] INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper -- [ExpirationReaper-0-ElectLeader]: Shutdown completed\n05:48:55.052 [main] INFO kafka.server.AddPartitionsToTxnManager -- [AddPartitionsToTxnSenderThread-0]: Shutting down\n05:48:55.052 [AddPartitionsToTxnSenderThread-0] INFO kafka.server.AddPartitionsToTxnManager -- [AddPartitionsToTxnSenderThread-0]: Stopped\n05:48:55.052 [main] INFO kafka.server.AddPartitionsToTxnManager -- [AddPartitionsToTxnSenderThread-0]: Shutdown completed\n05:48:55.052 [main] INFO kafka.server.ReplicaManager -- [ReplicaManager broker=0] Shut down completely\n05:48:55.052 [main] INFO kafka.server.BrokerToControllerRequestThread -- [zk-broker-0-to-controller-alter-partition-channel-manager]: Shutting down\n05:48:55.052 [zk-broker-0-to-controller-alter-partition-channel-manager] INFO kafka.server.BrokerToControllerRequestThread -- [zk-broker-0-to-controller-alter-partition-channel-manager]: Stopped\n05:48:55.052 [main] INFO kafka.server.BrokerToControllerRequestThread -- [zk-broker-0-to-controller-alter-partition-channel-manager]: Shutdown completed\n05:48:55.053 [main] INFO kafka.server.BrokerToControllerChannelManagerImpl -- Broker to controller channel manager for alter-partition shutdown\n05:48:55.053 [main] INFO kafka.server.BrokerToControllerRequestThread -- [zk-broker-0-to-controller-forwarding-channel-manager]: Shutting down\n05:48:55.053 [zk-broker-0-to-controller-forwarding-channel-manager] INFO kafka.server.BrokerToControllerRequestThread -- [zk-broker-0-to-controller-forwarding-channel-manager]: Stopped\n05:48:55.053 [main] INFO kafka.server.BrokerToControllerRequestThread -- [zk-broker-0-to-controller-forwarding-channel-manager]: Shutdown completed\n05:48:55.054 [main] INFO kafka.server.BrokerToControllerChannelManagerImpl -- Broker to controller channel manager for forwarding shutdown\n05:48:55.054 [main] INFO kafka.log.LogManager -- Shutting down.\n05:48:55.055 [main] INFO kafka.log.LogCleaner -- Shutting down the log cleaner.\n05:48:55.055 [main] INFO kafka.log.LogCleaner$CleanerThread -- [kafka-log-cleaner-thread-0]: Shutting down\n05:48:55.055 [kafka-log-cleaner-thread-0] INFO kafka.log.LogCleaner$CleanerThread -- [kafka-log-cleaner-thread-0]: Stopped\n05:48:55.055 [main] INFO kafka.log.LogCleaner$CleanerThread -- [kafka-log-cleaner-thread-0]: Shutdown completed\n05:48:55.067 [log-closing-/tmp/spring.kafka.bca7ba27-5e93-400e-9aae-86960892bff71860864339420333442] INFO org.apache.kafka.storage.internals.log.ProducerStateManager -- [ProducerStateManager partition=messages-1]Wrote producer snapshot at offset 4 with 2 producer ids in 2 ms.\n05:48:55.076 [log-closing-/tmp/spring.kafka.bca7ba27-5e93-400e-9aae-86960892bff71860864339420333442] INFO org.apache.kafka.storage.internals.log.ProducerStateManager -- [ProducerStateManager partition=messages-0]Wrote producer snapshot at offset 4 with 2 producer ids in 1 ms.\n05:48:55.086 [main] INFO kafka.log.LogManager -- Shutdown complete.\n05:48:55.086 [main] INFO kafka.controller.ControllerEventManager$ControllerEventThread -- [ControllerEventThread controllerId=0] Shutting down\n05:48:55.087 [controller-event-thread] INFO kafka.controller.ControllerEventManager$ControllerEventThread -- [ControllerEventThread controllerId=0] Stopped\n05:48:55.087 [main] INFO kafka.controller.ControllerEventManager$ControllerEventThread -- [ControllerEventThread controllerId=0] Shutdown completed\n05:48:55.088 [main] INFO kafka.controller.ZkPartitionStateMachine -- [PartitionStateMachine controllerId=0] Stopped partition state machine\n05:48:55.088 [pool-14-thread-1] INFO org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-sampleRawConsumer-2, groupId=sampleRawConsumer] Node 0 disconnected.\n05:48:55.088 [pool-14-thread-1] WARN org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-sampleRawConsumer-2, groupId=sampleRawConsumer] Connection to node 0 (localhost/127.0.0.1:35013) could not be established. Broker may not be available.\n05:48:55.088 [main] INFO kafka.controller.ZkReplicaStateMachine -- [ReplicaStateMachine controllerId=0] Stopped replica state machine\n05:48:55.088 [pool-14-thread-1] INFO org.apache.kafka.clients.FetchSessionHandler -- [Consumer clientId=consumer-sampleRawConsumer-2, groupId=sampleRawConsumer] Error sending fetch request (sessionId=1318414656, epoch=INITIAL) to node 0:\norg.apache.kafka.common.errors.DisconnectException: null\n05:48:55.089 [main] INFO kafka.controller.RequestSendThread -- [RequestSendThread controllerId=0] Shutting down\n05:48:55.089 [Controller-0-to-broker-0-send-thread] INFO kafka.controller.RequestSendThread -- [RequestSendThread controllerId=0] Stopped\n05:48:55.089 [main] INFO kafka.controller.RequestSendThread -- [RequestSendThread controllerId=0] Shutdown completed\n05:48:55.091 [main] INFO kafka.controller.RequestSendThread -- [RequestSendThread controllerId=0] Shutting down\n05:48:55.091 [Controller-0-to-broker-1-send-thread] INFO kafka.controller.RequestSendThread -- [RequestSendThread controllerId=0] Stopped\n05:48:55.091 [main] INFO kafka.controller.RequestSendThread -- [RequestSendThread controllerId=0] Shutdown completed\n05:48:55.091 [main] INFO kafka.controller.KafkaController -- [Controller id=0] Resigned\n05:48:55.092 [main] INFO kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread -- [feature-zk-node-event-process-thread]: Shutting down\n05:48:55.092 [main] INFO kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread -- [feature-zk-node-event-process-thread]: Shutdown completed\n05:48:55.092 [feature-zk-node-event-process-thread] INFO kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread -- [feature-zk-node-event-process-thread]: Stopped\n05:48:55.093 [main] INFO kafka.zookeeper.ZooKeeperClient -- [ZooKeeperClient Kafka server] Closing.\n05:48:55.099 [controller-event-thread] INFO kafka.controller.KafkaController -- [Controller id=1] 1 successfully elected as the controller. Epoch incremented to 2 and epoch zk version is now 2\n05:48:55.100 [controller-event-thread] INFO kafka.controller.KafkaController -- [Controller id=1] Registering handlers\n05:48:55.102 [controller-event-thread] INFO kafka.controller.KafkaController -- [Controller id=1] Deleting log dir event notifications\n05:48:55.102 [controller-event-thread] INFO kafka.controller.KafkaController -- [Controller id=1] Deleting isr change notifications\n05:48:55.103 [controller-event-thread] INFO kafka.controller.KafkaController -- [Controller id=1] Initializing controller context\n05:48:55.105 [controller-event-thread] INFO kafka.controller.KafkaController -- [Controller id=1] Initialized broker epochs cache: HashMap(1 -> 45)\n05:48:55.114 [Controller-1-to-broker-1-send-thread] INFO kafka.controller.RequestSendThread -- [RequestSendThread controllerId=1] Starting\n05:48:55.114 [controller-event-thread] INFO kafka.controller.KafkaController -- [Controller id=1] Currently active brokers in the cluster: Set(1)\n05:48:55.114 [controller-event-thread] INFO kafka.controller.KafkaController -- [Controller id=1] Currently shutting brokers in the cluster: HashSet()\n05:48:55.114 [controller-event-thread] INFO kafka.controller.KafkaController -- [Controller id=1] Current list of topics in the cluster: HashSet(messages, __consumer_offsets)\n05:48:55.114 [controller-event-thread] INFO kafka.controller.KafkaController -- [Controller id=1] Fetching topic deletions in progress\n05:48:55.116 [controller-event-thread] INFO kafka.controller.KafkaController -- [Controller id=1] List of topics to be deleted: \n05:48:55.116 [controller-event-thread] INFO kafka.controller.KafkaController -- [Controller id=1] List of topics ineligible for deletion: messages,__consumer_offsets\n05:48:55.116 [controller-event-thread] INFO kafka.controller.KafkaController -- [Controller id=1] Initializing topic deletion manager\n05:48:55.116 [controller-event-thread] INFO kafka.controller.TopicDeletionManager -- [Topic Deletion Manager 1] Initializing manager with initial deletions: Set(), initial ineligible deletions: HashSet(messages, __consumer_offsets)\n05:48:55.116 [controller-event-thread] INFO kafka.controller.KafkaController -- [Controller id=1] Sending update metadata request\n05:48:55.116 [controller-event-thread] INFO state.change.logger -- [Controller id=1 epoch=2] Sending UpdateMetadata request to brokers HashSet(1) for 0 partitions\n05:48:55.116 [controller-event-thread] INFO kafka.controller.ZkReplicaStateMachine -- [ReplicaStateMachine controllerId=1] Initializing replica state\n05:48:55.116 [Controller-1-to-broker-1-send-thread] INFO kafka.controller.RequestSendThread -- [RequestSendThread controllerId=1] Controller 1 connected to localhost:41935 (id: 1 rack: null) for sending state change requests\n05:48:55.117 [controller-event-thread] INFO kafka.controller.ZkReplicaStateMachine -- [ReplicaStateMachine controllerId=1] Triggering online replica state changes\n05:48:55.119 [controller-event-thread] INFO state.change.logger -- [Controller id=1 epoch=2] Sending LeaderAndIsr request to broker 1 with 5 become-leader and 0 become-follower partitions\n05:48:55.119 [controller-event-thread] INFO state.change.logger -- [Controller id=1 epoch=2] Sending UpdateMetadata request to brokers HashSet(1) for 5 partitions\n05:48:55.119 [controller-event-thread] INFO kafka.controller.ZkReplicaStateMachine -- [ReplicaStateMachine controllerId=1] Triggering offline replica state changes\n05:48:55.119 [data-plane-kafka-request-handler-7] INFO state.change.logger -- [Broker id=1] Handling LeaderAndIsr request correlationId 1 from controller 1 for 5 partitions\n05:48:55.120 [data-plane-kafka-request-handler-7] INFO state.change.logger -- [Broker id=1] Ignoring LeaderAndIsr request from controller 1 with correlation id 1 epoch 2 for partition messages-0 since its associated leader epoch 1 matches the current leader epoch\n05:48:55.120 [data-plane-kafka-request-handler-7] INFO state.change.logger -- [Broker id=1] Ignoring LeaderAndIsr request from controller 1 with correlation id 1 epoch 2 for partition messages-1 since its associated leader epoch 1 matches the current leader epoch\n05:48:55.120 [data-plane-kafka-request-handler-7] INFO state.change.logger -- [Broker id=1] Ignoring LeaderAndIsr request from controller 1 with correlation id 1 epoch 2 for partition __consumer_offsets-0 since its associated leader epoch 0 matches the current leader epoch\n05:48:55.120 [data-plane-kafka-request-handler-7] INFO state.change.logger -- [Broker id=1] Ignoring LeaderAndIsr request from controller 1 with correlation id 1 epoch 2 for partition __consumer_offsets-4 since its associated leader epoch 0 matches the current leader epoch\n05:48:55.120 [data-plane-kafka-request-handler-7] INFO state.change.logger -- [Broker id=1] Ignoring LeaderAndIsr request from controller 1 with correlation id 1 epoch 2 for partition __consumer_offsets-2 since its associated leader epoch 0 matches the current leader epoch\n05:48:55.121 [data-plane-kafka-request-handler-7] INFO state.change.logger -- [Broker id=1] Finished LeaderAndIsr request in 2ms correlationId 1 from controller 1 for 5 partitions\n05:48:55.122 [data-plane-kafka-request-handler-0] INFO state.change.logger -- [Broker id=1] Add 5 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 2 with correlation id 2\n05:48:55.124 [controller-event-thread] INFO state.change.logger -- [Controller id=1 epoch=2] Partition messages-0 state changed to (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:1,ZkVersion:1,ControllerEpoch:2) after removing replica 0 from the ISR as part of transition to OfflineReplica\n05:48:55.124 [controller-event-thread] INFO state.change.logger -- [Controller id=1 epoch=2] Partition messages-1 state changed to (Leader:1,ISR:1,LeaderRecoveryState:RECOVERED,LeaderEpoch:1,ZkVersion:1,ControllerEpoch:2) after removing replica 0 from the ISR as part of transition to OfflineReplica\n05:48:55.124 [controller-event-thread] INFO state.change.logger -- [Controller id=1 epoch=2] Partition __consumer_offsets-3 state changed to (Leader:-1,ISR:0,LeaderRecoveryState:RECOVERED,LeaderEpoch:1,ZkVersion:1,ControllerEpoch:2) after removing replica 0 from the ISR as part of transition to OfflineReplica\n05:48:55.124 [controller-event-thread] INFO state.change.logger -- [Controller id=1 epoch=2] Partition __consumer_offsets-1 state changed to (Leader:-1,ISR:0,LeaderRecoveryState:RECOVERED,LeaderEpoch:1,ZkVersion:1,ControllerEpoch:2) after removing replica 0 from the ISR as part of transition to OfflineReplica\n05:48:55.124 [controller-event-thread] INFO state.change.logger -- [Controller id=1 epoch=2] Sending LeaderAndIsr request to broker 1 with 2 become-leader and 0 become-follower partitions\n05:48:55.124 [controller-event-thread] INFO state.change.logger -- [Controller id=1 epoch=2] Sending UpdateMetadata request to brokers HashSet(1) for 4 partitions\n05:48:55.125 [controller-event-thread] INFO kafka.controller.ZkPartitionStateMachine -- [PartitionStateMachine controllerId=1] Initializing partition state\n05:48:55.125 [data-plane-kafka-request-handler-1] INFO state.change.logger -- [Broker id=1] Handling LeaderAndIsr request correlationId 3 from controller 1 for 2 partitions\n05:48:55.125 [data-plane-kafka-request-handler-1] INFO state.change.logger -- [Broker id=1] Ignoring LeaderAndIsr request from controller 1 with correlation id 3 epoch 2 for partition messages-0 since its associated leader epoch 1 matches the current leader epoch\n05:48:55.125 [data-plane-kafka-request-handler-1] INFO state.change.logger -- [Broker id=1] Ignoring LeaderAndIsr request from controller 1 with correlation id 3 epoch 2 for partition messages-1 since its associated leader epoch 1 matches the current leader epoch\n05:48:55.125 [controller-event-thread] INFO kafka.controller.ZkPartitionStateMachine -- [PartitionStateMachine controllerId=1] Triggering online partition state changes\n05:48:55.125 [data-plane-kafka-request-handler-1] INFO state.change.logger -- [Broker id=1] Finished LeaderAndIsr request in 0ms correlationId 3 from controller 1 for 2 partitions\n05:48:55.126 [data-plane-kafka-request-handler-3] INFO state.change.logger -- [Broker id=1] Add 4 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 2 with correlation id 4\n05:48:55.132 [controller-event-thread] ERROR state.change.logger -- [Controller id=1 epoch=2] Controller 1 epoch 2 failed to change state for partition __consumer_offsets-3 from OfflinePartition to OnlinePartition\nkafka.common.StateChangeFailedException: Failed to elect leader for partition __consumer_offsets-3 under strategy OfflinePartitionLeaderElectionStrategy(false)\n\tat kafka.controller.ZkPartitionStateMachine.$anonfun$doElectLeaderForPartitions$7(PartitionStateMachine.scala:433)\n\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:576)\n\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:574)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:933)\n\tat kafka.controller.ZkPartitionStateMachine.doElectLeaderForPartitions(PartitionStateMachine.scala:430)\n\tat kafka.controller.ZkPartitionStateMachine.electLeaderForPartitions(PartitionStateMachine.scala:336)\n\tat kafka.controller.ZkPartitionStateMachine.doHandleStateChanges(PartitionStateMachine.scala:242)\n\tat kafka.controller.ZkPartitionStateMachine.handleStateChanges(PartitionStateMachine.scala:162)\n\tat kafka.controller.PartitionStateMachine.triggerOnlineStateChangeForPartitions(PartitionStateMachine.scala:76)\n\tat kafka.controller.PartitionStateMachine.triggerOnlinePartitionStateChange(PartitionStateMachine.scala:61)\n\tat kafka.controller.PartitionStateMachine.startup(PartitionStateMachine.scala:44)\n\tat kafka.controller.KafkaController.onControllerFailover(KafkaController.scala:317)\n\tat kafka.controller.KafkaController.elect(KafkaController.scala:1568)\n\tat kafka.controller.KafkaController.processReelect(KafkaController.scala:2581)\n\tat kafka.controller.KafkaController.process(KafkaController.scala:2626)\n\tat kafka.controller.QueuedEvent.process(ControllerEventManager.scala:54)\n\tat kafka.controller.ControllerEventManager$ControllerEventThread.process$1(ControllerEventManager.scala:138)\n\tat kafka.controller.ControllerEventManager$ControllerEventThread.$anonfun$doWork$1(ControllerEventManager.scala:141)\n\tat kafka.controller.ControllerEventManager$ControllerEventThread.$anonfun$doWork$1$adapted(ControllerEventManager.scala:141)\n\tat com.yammer.metrics.core.Timer.time(Timer.java:91)\n\tat kafka.controller.ControllerEventManager$ControllerEventThread.doWork(ControllerEventManager.scala:141)\n\tat org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:130)\n05:48:55.132 [controller-event-thread] ERROR state.change.logger -- [Controller id=1 epoch=2] Controller 1 epoch 2 failed to change state for partition __consumer_offsets-1 from OfflinePartition to OnlinePartition\nkafka.common.StateChangeFailedException: Failed to elect leader for partition __consumer_offsets-1 under strategy OfflinePartitionLeaderElectionStrategy(false)\n\tat kafka.controller.ZkPartitionStateMachine.$anonfun$doElectLeaderForPartitions$7(PartitionStateMachine.scala:433)\n\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:576)\n\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:574)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:933)\n\tat kafka.controller.ZkPartitionStateMachine.doElectLeaderForPartitions(PartitionStateMachine.scala:430)\n\tat kafka.controller.ZkPartitionStateMachine.electLeaderForPartitions(PartitionStateMachine.scala:336)\n\tat kafka.controller.ZkPartitionStateMachine.doHandleStateChanges(PartitionStateMachine.scala:242)\n\tat kafka.controller.ZkPartitionStateMachine.handleStateChanges(PartitionStateMachine.scala:162)\n\tat kafka.controller.PartitionStateMachine.triggerOnlineStateChangeForPartitions(PartitionStateMachine.scala:76)\n\tat kafka.controller.PartitionStateMachine.triggerOnlinePartitionStateChange(PartitionStateMachine.scala:61)\n\tat kafka.controller.PartitionStateMachine.startup(PartitionStateMachine.scala:44)\n\tat kafka.controller.KafkaController.onControllerFailover(KafkaController.scala:317)\n\tat kafka.controller.KafkaController.elect(KafkaController.scala:1568)\n\tat kafka.controller.KafkaController.processReelect(KafkaController.scala:2581)\n\tat kafka.controller.KafkaController.process(KafkaController.scala:2626)\n\tat kafka.controller.QueuedEvent.process(ControllerEventManager.scala:54)\n\tat kafka.controller.ControllerEventManager$ControllerEventThread.process$1(ControllerEventManager.scala:138)\n\tat kafka.controller.ControllerEventManager$ControllerEventThread.$anonfun$doWork$1(ControllerEventManager.scala:141)\n\tat kafka.controller.ControllerEventManager$ControllerEventThread.$anonfun$doWork$1$adapted(ControllerEventManager.scala:141)\n\tat com.yammer.metrics.core.Timer.time(Timer.java:91)\n\tat kafka.controller.ControllerEventManager$ControllerEventThread.doWork(ControllerEventManager.scala:141)\n\tat org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:130)\n05:48:55.132 [controller-event-thread] INFO state.change.logger -- [Controller id=1 epoch=2] Sending UpdateMetadata request to brokers HashSet() for 0 partitions\n05:48:55.132 [controller-event-thread] INFO kafka.controller.KafkaController -- [Controller id=1] Ready to serve as the new controller with epoch 2\n05:48:55.134 [controller-event-thread] INFO kafka.controller.KafkaController -- [Controller id=1] Partitions undergoing preferred replica election: \n05:48:55.134 [controller-event-thread] INFO kafka.controller.KafkaController -- [Controller id=1] Partitions that completed preferred replica election: \n05:48:55.134 [controller-event-thread] INFO kafka.controller.KafkaController -- [Controller id=1] Skipping preferred replica election for partitions due to topic deletion: \n05:48:55.134 [controller-event-thread] INFO kafka.controller.KafkaController -- [Controller id=1] Resuming preferred replica election for partitions: \n05:48:55.134 [controller-event-thread] INFO kafka.controller.KafkaController -- [Controller id=1] Starting replica leader election (PREFERRED) for partitions  triggered by ZkTriggered\n05:48:55.135 [controller-event-thread] INFO kafka.controller.KafkaController -- [Controller id=1] Starting the controller scheduler\n05:48:55.196 [main] INFO org.apache.zookeeper.ZooKeeper -- Session: 0x1001b05f8640000 closed\n05:48:55.196 [main-EventThread] INFO org.apache.zookeeper.ClientCnxn -- EventThread shut down for session: 0x1001b05f8640000\n05:48:55.196 [main] INFO kafka.zookeeper.ZooKeeperClient -- [ZooKeeperClient Kafka server] Closed.\n05:48:55.197 [main] INFO kafka.server.ClientQuotaManager$ThrottledChannelReaper -- [ThrottledChannelReaper-Fetch]: Shutting down\n05:48:55.198 [main] INFO kafka.server.ClientQuotaManager$ThrottledChannelReaper -- [ThrottledChannelReaper-Fetch]: Shutdown completed\n05:48:55.198 [ThrottledChannelReaper-Fetch] INFO kafka.server.ClientQuotaManager$ThrottledChannelReaper -- [ThrottledChannelReaper-Fetch]: Stopped\n05:48:55.198 [main] INFO kafka.server.ClientQuotaManager$ThrottledChannelReaper -- [ThrottledChannelReaper-Produce]: Shutting down\n05:48:55.198 [main] INFO kafka.server.ClientQuotaManager$ThrottledChannelReaper -- [ThrottledChannelReaper-Produce]: Shutdown completed\n05:48:55.198 [ThrottledChannelReaper-Produce] INFO kafka.server.ClientQuotaManager$ThrottledChannelReaper -- [ThrottledChannelReaper-Produce]: Stopped\n05:48:55.198 [main] INFO kafka.server.ClientQuotaManager$ThrottledChannelReaper -- [ThrottledChannelReaper-Request]: Shutting down\n05:48:55.198 [main] INFO kafka.server.ClientQuotaManager$ThrottledChannelReaper -- [ThrottledChannelReaper-Request]: Shutdown completed\n05:48:55.198 [ThrottledChannelReaper-Request] INFO kafka.server.ClientQuotaManager$ThrottledChannelReaper -- [ThrottledChannelReaper-Request]: Stopped\n05:48:55.198 [main] INFO kafka.server.ClientQuotaManager$ThrottledChannelReaper -- [ThrottledChannelReaper-ControllerMutation]: Shutting down\n05:48:55.198 [main] INFO kafka.server.ClientQuotaManager$ThrottledChannelReaper -- [ThrottledChannelReaper-ControllerMutation]: Shutdown completed\n05:48:55.198 [ThrottledChannelReaper-ControllerMutation] INFO kafka.server.ClientQuotaManager$ThrottledChannelReaper -- [ThrottledChannelReaper-ControllerMutation]: Stopped\n05:48:55.198 [main] INFO kafka.network.SocketServer -- [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutting down socket server\n05:48:55.208 [main] INFO kafka.network.SocketServer -- [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutdown completed\n05:48:55.209 [main] INFO org.apache.kafka.common.metrics.Metrics -- Metrics scheduler closed\n05:48:55.209 [main] INFO org.apache.kafka.common.metrics.Metrics -- Closing reporter org.apache.kafka.common.metrics.JmxReporter\n05:48:55.209 [main] INFO org.apache.kafka.common.metrics.Metrics -- Metrics reporters closed\n05:48:55.210 [main] INFO kafka.server.BrokerTopicStats -- Broker and topic stats closed\n05:48:55.210 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- App info kafka.server for 0 unregistered\n05:48:55.210 [main] INFO kafka.server.KafkaServer -- [KafkaServer id=0] shut down completed\n05:48:55.213 [main] INFO kafka.server.KafkaServer -- [KafkaServer id=1] shutting down\n05:48:55.213 [main] INFO kafka.server.KafkaServer -- [KafkaServer id=1] Starting controlled shutdown\n05:48:55.215 [controller-event-thread] INFO kafka.controller.KafkaController -- [Controller id=1] Shutting down broker 1\n05:48:55.217 [controller-event-thread] ERROR state.change.logger -- [Controller id=1 epoch=2] Controller 1 epoch 2 failed to change state for partition messages-0 from OnlinePartition to OnlinePartition\nkafka.common.StateChangeFailedException: Failed to elect leader for partition messages-0 under strategy ControlledShutdownPartitionLeaderElectionStrategy\n\tat kafka.controller.ZkPartitionStateMachine.$anonfun$doElectLeaderForPartitions$7(PartitionStateMachine.scala:433)\n\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:576)\n\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:574)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:933)\n\tat kafka.controller.ZkPartitionStateMachine.doElectLeaderForPartitions(PartitionStateMachine.scala:430)\n\tat kafka.controller.ZkPartitionStateMachine.electLeaderForPartitions(PartitionStateMachine.scala:336)\n\tat kafka.controller.ZkPartitionStateMachine.doHandleStateChanges(PartitionStateMachine.scala:242)\n\tat kafka.controller.ZkPartitionStateMachine.handleStateChanges(PartitionStateMachine.scala:162)\n\tat kafka.controller.KafkaController.doControlledShutdown(KafkaController.scala:1394)\n\tat kafka.controller.KafkaController.$anonfun$processControlledShutdown$1(KafkaController.scala:1356)\n\tat scala.util.Try$.apply(Try.scala:210)\n\tat kafka.controller.KafkaController.processControlledShutdown(KafkaController.scala:1356)\n\tat kafka.controller.KafkaController.process(KafkaController.scala:2612)\n\tat kafka.controller.QueuedEvent.process(ControllerEventManager.scala:54)\n\tat kafka.controller.ControllerEventManager$ControllerEventThread.process$1(ControllerEventManager.scala:138)\n\tat kafka.controller.ControllerEventManager$ControllerEventThread.$anonfun$doWork$1(ControllerEventManager.scala:141)\n\tat kafka.controller.ControllerEventManager$ControllerEventThread.$anonfun$doWork$1$adapted(ControllerEventManager.scala:141)\n\tat com.yammer.metrics.core.Timer.time(Timer.java:91)\n\tat kafka.controller.ControllerEventManager$ControllerEventThread.doWork(ControllerEventManager.scala:141)\n\tat org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:130)\n05:48:55.217 [controller-event-thread] ERROR state.change.logger -- [Controller id=1 epoch=2] Controller 1 epoch 2 failed to change state for partition messages-1 from OnlinePartition to OnlinePartition\nkafka.common.StateChangeFailedException: Failed to elect leader for partition messages-1 under strategy ControlledShutdownPartitionLeaderElectionStrategy\n\tat kafka.controller.ZkPartitionStateMachine.$anonfun$doElectLeaderForPartitions$7(PartitionStateMachine.scala:433)\n\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:576)\n\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:574)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:933)\n\tat kafka.controller.ZkPartitionStateMachine.doElectLeaderForPartitions(PartitionStateMachine.scala:430)\n\tat kafka.controller.ZkPartitionStateMachine.electLeaderForPartitions(PartitionStateMachine.scala:336)\n\tat kafka.controller.ZkPartitionStateMachine.doHandleStateChanges(PartitionStateMachine.scala:242)\n\tat kafka.controller.ZkPartitionStateMachine.handleStateChanges(PartitionStateMachine.scala:162)\n\tat kafka.controller.KafkaController.doControlledShutdown(KafkaController.scala:1394)\n\tat kafka.controller.KafkaController.$anonfun$processControlledShutdown$1(KafkaController.scala:1356)\n\tat scala.util.Try$.apply(Try.scala:210)\n\tat kafka.controller.KafkaController.processControlledShutdown(KafkaController.scala:1356)\n\tat kafka.controller.KafkaController.process(KafkaController.scala:2612)\n\tat kafka.controller.QueuedEvent.process(ControllerEventManager.scala:54)\n\tat kafka.controller.ControllerEventManager$ControllerEventThread.process$1(ControllerEventManager.scala:138)\n\tat kafka.controller.ControllerEventManager$ControllerEventThread.$anonfun$doWork$1(ControllerEventManager.scala:141)\n\tat kafka.controller.ControllerEventManager$ControllerEventThread.$anonfun$doWork$1$adapted(ControllerEventManager.scala:141)\n\tat com.yammer.metrics.core.Timer.time(Timer.java:91)\n\tat kafka.controller.ControllerEventManager$ControllerEventThread.doWork(ControllerEventManager.scala:141)\n\tat org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:130)\n05:48:55.218 [controller-event-thread] INFO state.change.logger -- [Controller id=1 epoch=2] Sending UpdateMetadata request to brokers HashSet() for 0 partitions\n05:48:55.218 [controller-event-thread] INFO state.change.logger -- [Controller id=1 epoch=2] Sending UpdateMetadata request to brokers HashSet() for 0 partitions\n05:48:55.219 [main] INFO kafka.server.KafkaServer -- [KafkaServer id=1] Controlled shutdown request returned after 4ms with 2 partitions remaining to move\n05:48:55.301 [pool-14-thread-1] INFO org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-sampleRawConsumer-2, groupId=sampleRawConsumer] Node 0 disconnected.\n05:48:55.302 [pool-14-thread-1] WARN org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-sampleRawConsumer-2, groupId=sampleRawConsumer] Connection to node 0 (localhost/127.0.0.1:35013) could not be established. Broker may not be available.\n05:48:55.302 [pool-14-thread-1] INFO org.apache.kafka.clients.FetchSessionHandler -- [Consumer clientId=consumer-sampleRawConsumer-2, groupId=sampleRawConsumer] Error sending fetch request (sessionId=1318414656, epoch=INITIAL) to node 0:\norg.apache.kafka.common.errors.DisconnectException: null\n05:48:55.319 [main] INFO kafka.server.KafkaServer -- [KafkaServer id=1] Retrying controlled shutdown (2 retries remaining)\n05:48:55.320 [controller-event-thread] INFO kafka.controller.KafkaController -- [Controller id=1] Shutting down broker 1\n05:48:55.322 [controller-event-thread] ERROR state.change.logger -- [Controller id=1 epoch=2] Controller 1 epoch 2 failed to change state for partition messages-0 from OnlinePartition to OnlinePartition\nkafka.common.StateChangeFailedException: Failed to elect leader for partition messages-0 under strategy ControlledShutdownPartitionLeaderElectionStrategy\n\tat kafka.controller.ZkPartitionStateMachine.$anonfun$doElectLeaderForPartitions$7(PartitionStateMachine.scala:433)\n\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:576)\n\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:574)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:933)\n\tat kafka.controller.ZkPartitionStateMachine.doElectLeaderForPartitions(PartitionStateMachine.scala:430)\n\tat kafka.controller.ZkPartitionStateMachine.electLeaderForPartitions(PartitionStateMachine.scala:336)\n\tat kafka.controller.ZkPartitionStateMachine.doHandleStateChanges(PartitionStateMachine.scala:242)\n\tat kafka.controller.ZkPartitionStateMachine.handleStateChanges(PartitionStateMachine.scala:162)\n\tat kafka.controller.KafkaController.doControlledShutdown(KafkaController.scala:1394)\n\tat kafka.controller.KafkaController.$anonfun$processControlledShutdown$1(KafkaController.scala:1356)\n\tat scala.util.Try$.apply(Try.scala:210)\n\tat kafka.controller.KafkaController.processControlledShutdown(KafkaController.scala:1356)\n\tat kafka.controller.KafkaController.process(KafkaController.scala:2612)\n\tat kafka.controller.QueuedEvent.process(ControllerEventManager.scala:54)\n\tat kafka.controller.ControllerEventManager$ControllerEventThread.process$1(ControllerEventManager.scala:138)\n\tat kafka.controller.ControllerEventManager$ControllerEventThread.$anonfun$doWork$1(ControllerEventManager.scala:141)\n\tat kafka.controller.ControllerEventManager$ControllerEventThread.$anonfun$doWork$1$adapted(ControllerEventManager.scala:141)\n\tat com.yammer.metrics.core.Timer.time(Timer.java:91)\n\tat kafka.controller.ControllerEventManager$ControllerEventThread.doWork(ControllerEventManager.scala:141)\n\tat org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:130)\n05:48:55.322 [controller-event-thread] ERROR state.change.logger -- [Controller id=1 epoch=2] Controller 1 epoch 2 failed to change state for partition messages-1 from OnlinePartition to OnlinePartition\nkafka.common.StateChangeFailedException: Failed to elect leader for partition messages-1 under strategy ControlledShutdownPartitionLeaderElectionStrategy\n\tat kafka.controller.ZkPartitionStateMachine.$anonfun$doElectLeaderForPartitions$7(PartitionStateMachine.scala:433)\n\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:576)\n\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:574)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:933)\n\tat kafka.controller.ZkPartitionStateMachine.doElectLeaderForPartitions(PartitionStateMachine.scala:430)\n\tat kafka.controller.ZkPartitionStateMachine.electLeaderForPartitions(PartitionStateMachine.scala:336)\n\tat kafka.controller.ZkPartitionStateMachine.doHandleStateChanges(PartitionStateMachine.scala:242)\n\tat kafka.controller.ZkPartitionStateMachine.handleStateChanges(PartitionStateMachine.scala:162)\n\tat kafka.controller.KafkaController.doControlledShutdown(KafkaController.scala:1394)\n\tat kafka.controller.KafkaController.$anonfun$processControlledShutdown$1(KafkaController.scala:1356)\n\tat scala.util.Try$.apply(Try.scala:210)\n\tat kafka.controller.KafkaController.processControlledShutdown(KafkaController.scala:1356)\n\tat kafka.controller.KafkaController.process(KafkaController.scala:2612)\n\tat kafka.controller.QueuedEvent.process(ControllerEventManager.scala:54)\n\tat kafka.controller.ControllerEventManager$ControllerEventThread.process$1(ControllerEventManager.scala:138)\n\tat kafka.controller.ControllerEventManager$ControllerEventThread.$anonfun$doWork$1(ControllerEventManager.scala:141)\n\tat kafka.controller.ControllerEventManager$ControllerEventThread.$anonfun$doWork$1$adapted(ControllerEventManager.scala:141)\n\tat com.yammer.metrics.core.Timer.time(Timer.java:91)\n\tat kafka.controller.ControllerEventManager$ControllerEventThread.doWork(ControllerEventManager.scala:141)\n\tat org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:130)\n05:48:55.322 [controller-event-thread] INFO state.change.logger -- [Controller id=1 epoch=2] Sending UpdateMetadata request to brokers HashSet() for 0 partitions\n05:48:55.322 [controller-event-thread] INFO state.change.logger -- [Controller id=1 epoch=2] Sending UpdateMetadata request to brokers HashSet() for 0 partitions\n05:48:55.323 [main] INFO kafka.server.KafkaServer -- [KafkaServer id=1] Controlled shutdown request returned after 4ms with 2 partitions remaining to move\n05:48:55.423 [main] INFO kafka.server.KafkaServer -- [KafkaServer id=1] Retrying controlled shutdown (1 retries remaining)\n05:48:55.424 [controller-event-thread] INFO kafka.controller.KafkaController -- [Controller id=1] Shutting down broker 1\n05:48:55.426 [controller-event-thread] ERROR state.change.logger -- [Controller id=1 epoch=2] Controller 1 epoch 2 failed to change state for partition messages-0 from OnlinePartition to OnlinePartition\nkafka.common.StateChangeFailedException: Failed to elect leader for partition messages-0 under strategy ControlledShutdownPartitionLeaderElectionStrategy\n\tat kafka.controller.ZkPartitionStateMachine.$anonfun$doElectLeaderForPartitions$7(PartitionStateMachine.scala:433)\n\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:576)\n\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:574)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:933)\n\tat kafka.controller.ZkPartitionStateMachine.doElectLeaderForPartitions(PartitionStateMachine.scala:430)\n\tat kafka.controller.ZkPartitionStateMachine.electLeaderForPartitions(PartitionStateMachine.scala:336)\n\tat kafka.controller.ZkPartitionStateMachine.doHandleStateChanges(PartitionStateMachine.scala:242)\n\tat kafka.controller.ZkPartitionStateMachine.handleStateChanges(PartitionStateMachine.scala:162)\n\tat kafka.controller.KafkaController.doControlledShutdown(KafkaController.scala:1394)\n\tat kafka.controller.KafkaController.$anonfun$processControlledShutdown$1(KafkaController.scala:1356)\n\tat scala.util.Try$.apply(Try.scala:210)\n\tat kafka.controller.KafkaController.processControlledShutdown(KafkaController.scala:1356)\n\tat kafka.controller.KafkaController.process(KafkaController.scala:2612)\n\tat kafka.controller.QueuedEvent.process(ControllerEventManager.scala:54)\n\tat kafka.controller.ControllerEventManager$ControllerEventThread.process$1(ControllerEventManager.scala:138)\n\tat kafka.controller.ControllerEventManager$ControllerEventThread.$anonfun$doWork$1(ControllerEventManager.scala:141)\n\tat kafka.controller.ControllerEventManager$ControllerEventThread.$anonfun$doWork$1$adapted(ControllerEventManager.scala:141)\n\tat com.yammer.metrics.core.Timer.time(Timer.java:91)\n\tat kafka.controller.ControllerEventManager$ControllerEventThread.doWork(ControllerEventManager.scala:141)\n\tat org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:130)\n05:48:55.426 [controller-event-thread] ERROR state.change.logger -- [Controller id=1 epoch=2] Controller 1 epoch 2 failed to change state for partition messages-1 from OnlinePartition to OnlinePartition\nkafka.common.StateChangeFailedException: Failed to elect leader for partition messages-1 under strategy ControlledShutdownPartitionLeaderElectionStrategy\n\tat kafka.controller.ZkPartitionStateMachine.$anonfun$doElectLeaderForPartitions$7(PartitionStateMachine.scala:433)\n\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:576)\n\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:574)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:933)\n\tat kafka.controller.ZkPartitionStateMachine.doElectLeaderForPartitions(PartitionStateMachine.scala:430)\n\tat kafka.controller.ZkPartitionStateMachine.electLeaderForPartitions(PartitionStateMachine.scala:336)\n\tat kafka.controller.ZkPartitionStateMachine.doHandleStateChanges(PartitionStateMachine.scala:242)\n\tat kafka.controller.ZkPartitionStateMachine.handleStateChanges(PartitionStateMachine.scala:162)\n\tat kafka.controller.KafkaController.doControlledShutdown(KafkaController.scala:1394)\n\tat kafka.controller.KafkaController.$anonfun$processControlledShutdown$1(KafkaController.scala:1356)\n\tat scala.util.Try$.apply(Try.scala:210)\n\tat kafka.controller.KafkaController.processControlledShutdown(KafkaController.scala:1356)\n\tat kafka.controller.KafkaController.process(KafkaController.scala:2612)\n\tat kafka.controller.QueuedEvent.process(ControllerEventManager.scala:54)\n\tat kafka.controller.ControllerEventManager$ControllerEventThread.process$1(ControllerEventManager.scala:138)\n\tat kafka.controller.ControllerEventManager$ControllerEventThread.$anonfun$doWork$1(ControllerEventManager.scala:141)\n\tat kafka.controller.ControllerEventManager$ControllerEventThread.$anonfun$doWork$1$adapted(ControllerEventManager.scala:141)\n\tat com.yammer.metrics.core.Timer.time(Timer.java:91)\n\tat kafka.controller.ControllerEventManager$ControllerEventThread.doWork(ControllerEventManager.scala:141)\n\tat org.apache.kafka.server.util.ShutdownableThread.run(ShutdownableThread.java:130)\n05:48:55.426 [controller-event-thread] INFO state.change.logger -- [Controller id=1 epoch=2] Sending UpdateMetadata request to brokers HashSet() for 0 partitions\n05:48:55.426 [controller-event-thread] INFO state.change.logger -- [Controller id=1 epoch=2] Sending UpdateMetadata request to brokers HashSet() for 0 partitions\n05:48:55.426 [main] INFO kafka.server.KafkaServer -- [KafkaServer id=1] Controlled shutdown request returned after 3ms with 2 partitions remaining to move\n05:48:55.428 [main] WARN kafka.server.KafkaServer -- [KafkaServer id=1] Proceeding to do an unclean shutdown as all the controlled shutdown attempts failed\n05:48:55.428 [main] INFO kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread -- [/config/changes-event-process-thread]: Shutting down\n05:48:55.428 [main] INFO kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread -- [/config/changes-event-process-thread]: Shutdown completed\n05:48:55.428 [/config/changes-event-process-thread] INFO kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread -- [/config/changes-event-process-thread]: Stopped\n05:48:55.428 [main] INFO kafka.network.SocketServer -- [SocketServer listenerType=ZK_BROKER, nodeId=1] Stopping socket server request processors\n05:48:55.428 [kafka-producer-network-thread | producer-2] INFO org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-2] Node 1 disconnected.\n05:48:55.428 [pool-14-thread-1] INFO org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-sampleRawConsumer-2, groupId=sampleRawConsumer] Node 2147483646 disconnected.\n05:48:55.428 [kafka-producer-network-thread | producer-1] INFO org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-1] Node 1 disconnected.\n05:48:55.429 [pool-14-thread-1] INFO org.apache.kafka.clients.consumer.internals.ConsumerCoordinator -- [Consumer clientId=consumer-sampleRawConsumer-2, groupId=sampleRawConsumer] Group coordinator localhost:41935 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: true. Rediscovery will be attempted.\n05:48:55.429 [kafka-producer-network-thread | producer-2] INFO org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-2] Node 0 disconnected.\n05:48:55.429 [kafka-producer-network-thread | producer-2] WARN org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-2] Connection to node 0 (localhost/127.0.0.1:35013) could not be established. Broker may not be available.\n05:48:55.429 [pool-14-thread-1] INFO org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-sampleRawConsumer-2, groupId=sampleRawConsumer] Node 1 disconnected.\n05:48:55.429 [kafka-producer-network-thread | producer-1] INFO org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-1] Node 0 disconnected.\n05:48:55.429 [pool-14-thread-1] INFO org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-sampleRawConsumer-2, groupId=sampleRawConsumer] Cancelled in-flight FETCH request with correlation id 14 due to node 1 being disconnected (elapsed time since creation: 431ms, elapsed time since send: 431ms, request timeout: 30000ms)\n05:48:55.429 [pool-14-thread-1] INFO org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-sampleRawConsumer-2, groupId=sampleRawConsumer] Cancelled in-flight METADATA request with correlation id 32 due to node 1 being disconnected (elapsed time since creation: 342ms, elapsed time since send: 342ms, request timeout: 30000ms)\n05:48:55.429 [kafka-producer-network-thread | producer-1] WARN org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-1] Connection to node 0 (localhost/127.0.0.1:35013) could not be established. Broker may not be available.\n05:48:55.429 [pool-14-thread-1] INFO org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-sampleRawConsumer-2, groupId=sampleRawConsumer] Cancelled in-flight FIND_COORDINATOR request with correlation id 34 due to node 1 being disconnected (elapsed time since creation: 0ms, elapsed time since send: 1ms, request timeout: 30000ms)\n05:48:55.429 [pool-14-thread-1] INFO org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-sampleRawConsumer-2, groupId=sampleRawConsumer] Node -2 disconnected.\n05:48:55.429 [pool-14-thread-1] INFO org.apache.kafka.clients.FetchSessionHandler -- [Consumer clientId=consumer-sampleRawConsumer-2, groupId=sampleRawConsumer] Error sending fetch request (sessionId=901295635, epoch=1) to node 1:\norg.apache.kafka.common.errors.DisconnectException: null\n05:48:55.429 [main] INFO kafka.network.SocketServer -- [SocketServer listenerType=ZK_BROKER, nodeId=1] Stopped socket server request processors\n05:48:55.429 [main] INFO kafka.server.KafkaRequestHandlerPool -- [data-plane Kafka Request Handler on Broker 1], shutting down\n05:48:55.430 [main] INFO kafka.server.KafkaRequestHandlerPool -- [data-plane Kafka Request Handler on Broker 1], shut down completely\n05:48:55.430 [main] INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper -- [ExpirationReaper-1-AlterAcls]: Shutting down\n05:48:55.430 [ExpirationReaper-1-AlterAcls] INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper -- [ExpirationReaper-1-AlterAcls]: Stopped\n05:48:55.430 [main] INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper -- [ExpirationReaper-1-AlterAcls]: Shutdown completed\n05:48:55.430 [main] INFO kafka.server.KafkaApis -- [KafkaApi-1] Shutdown complete.\n05:48:55.430 [main] INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper -- [ExpirationReaper-1-topic]: Shutting down\n05:48:55.430 [ExpirationReaper-1-topic] INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper -- [ExpirationReaper-1-topic]: Stopped\n05:48:55.430 [main] INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper -- [ExpirationReaper-1-topic]: Shutdown completed\n05:48:55.431 [main] INFO kafka.coordinator.transaction.TransactionCoordinator -- [TransactionCoordinator id=1] Shutting down.\n05:48:55.431 [main] INFO kafka.coordinator.transaction.TransactionStateManager -- [Transaction State Manager 1]: Shutdown complete\n05:48:55.431 [main] INFO kafka.coordinator.transaction.TransactionMarkerChannelManager -- [TxnMarkerSenderThread-1]: Shutting down\n05:48:55.431 [TxnMarkerSenderThread-1] INFO kafka.coordinator.transaction.TransactionMarkerChannelManager -- [TxnMarkerSenderThread-1]: Stopped\n05:48:55.431 [main] INFO kafka.coordinator.transaction.TransactionMarkerChannelManager -- [TxnMarkerSenderThread-1]: Shutdown completed\n05:48:55.431 [main] INFO kafka.coordinator.transaction.TransactionCoordinator -- [TransactionCoordinator id=1] Shutdown complete.\n05:48:55.431 [main] INFO kafka.coordinator.group.GroupCoordinator -- [GroupCoordinator 1]: Shutting down.\n05:48:55.431 [main] INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper -- [ExpirationReaper-1-Heartbeat]: Shutting down\n05:48:55.431 [ExpirationReaper-1-Heartbeat] INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper -- [ExpirationReaper-1-Heartbeat]: Stopped\n05:48:55.431 [main] INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper -- [ExpirationReaper-1-Heartbeat]: Shutdown completed\n05:48:55.431 [main] INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper -- [ExpirationReaper-1-Rebalance]: Shutting down\n05:48:55.431 [ExpirationReaper-1-Rebalance] INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper -- [ExpirationReaper-1-Rebalance]: Stopped\n05:48:55.431 [main] INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper -- [ExpirationReaper-1-Rebalance]: Shutdown completed\n05:48:55.432 [main] INFO kafka.coordinator.group.GroupCoordinator -- [GroupCoordinator 1]: Shutdown complete.\n05:48:55.432 [main] INFO kafka.server.ReplicaManager -- [ReplicaManager broker=1] Shutting down\n05:48:55.432 [main] INFO kafka.server.ReplicaManager$LogDirFailureHandler -- [LogDirFailureHandler]: Shutting down\n05:48:55.432 [LogDirFailureHandler] INFO kafka.server.ReplicaManager$LogDirFailureHandler -- [LogDirFailureHandler]: Stopped\n05:48:55.432 [main] INFO kafka.server.ReplicaManager$LogDirFailureHandler -- [LogDirFailureHandler]: Shutdown completed\n05:48:55.432 [main] INFO kafka.server.ReplicaFetcherManager -- [ReplicaFetcherManager on broker 1] shutting down\n05:48:55.432 [main] INFO kafka.server.ReplicaFetcherManager -- [ReplicaFetcherManager on broker 1] shutdown completed\n05:48:55.432 [main] INFO kafka.server.ReplicaAlterLogDirsManager -- [ReplicaAlterLogDirsManager on broker 1] shutting down\n05:48:55.432 [main] INFO kafka.server.ReplicaAlterLogDirsManager -- [ReplicaAlterLogDirsManager on broker 1] shutdown completed\n05:48:55.432 [main] INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper -- [ExpirationReaper-1-Fetch]: Shutting down\n05:48:55.432 [ExpirationReaper-1-Fetch] INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper -- [ExpirationReaper-1-Fetch]: Stopped\n05:48:55.432 [main] INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper -- [ExpirationReaper-1-Fetch]: Shutdown completed\n05:48:55.432 [main] INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper -- [ExpirationReaper-1-RemoteFetch]: Shutting down\n05:48:55.432 [ExpirationReaper-1-RemoteFetch] INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper -- [ExpirationReaper-1-RemoteFetch]: Stopped\n05:48:55.432 [main] INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper -- [ExpirationReaper-1-RemoteFetch]: Shutdown completed\n05:48:55.432 [main] INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper -- [ExpirationReaper-1-Produce]: Shutting down\n05:48:55.432 [ExpirationReaper-1-Produce] INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper -- [ExpirationReaper-1-Produce]: Stopped\n05:48:55.432 [main] INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper -- [ExpirationReaper-1-Produce]: Shutdown completed\n05:48:55.433 [main] INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper -- [ExpirationReaper-1-DeleteRecords]: Shutting down\n05:48:55.433 [main] INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper -- [ExpirationReaper-1-DeleteRecords]: Shutdown completed\n05:48:55.433 [ExpirationReaper-1-DeleteRecords] INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper -- [ExpirationReaper-1-DeleteRecords]: Stopped\n05:48:55.433 [main] INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper -- [ExpirationReaper-1-ElectLeader]: Shutting down\n05:48:55.433 [ExpirationReaper-1-ElectLeader] INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper -- [ExpirationReaper-1-ElectLeader]: Stopped\n05:48:55.433 [main] INFO kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper -- [ExpirationReaper-1-ElectLeader]: Shutdown completed\n05:48:55.439 [main] INFO kafka.server.AddPartitionsToTxnManager -- [AddPartitionsToTxnSenderThread-1]: Shutting down\n05:48:55.439 [AddPartitionsToTxnSenderThread-1] INFO kafka.server.AddPartitionsToTxnManager -- [AddPartitionsToTxnSenderThread-1]: Stopped\n05:48:55.439 [main] INFO kafka.server.AddPartitionsToTxnManager -- [AddPartitionsToTxnSenderThread-1]: Shutdown completed\n05:48:55.439 [main] INFO kafka.server.ReplicaManager -- [ReplicaManager broker=1] Shut down completely\n05:48:55.439 [main] INFO kafka.server.BrokerToControllerRequestThread -- [zk-broker-1-to-controller-alter-partition-channel-manager]: Shutting down\n05:48:55.439 [main] INFO kafka.server.BrokerToControllerRequestThread -- [zk-broker-1-to-controller-alter-partition-channel-manager]: Shutdown completed\n05:48:55.439 [zk-broker-1-to-controller-alter-partition-channel-manager] INFO kafka.server.BrokerToControllerRequestThread -- [zk-broker-1-to-controller-alter-partition-channel-manager]: Stopped\n05:48:55.439 [main] INFO kafka.server.BrokerToControllerChannelManagerImpl -- Broker to controller channel manager for alter-partition shutdown\n05:48:55.439 [main] INFO kafka.server.BrokerToControllerRequestThread -- [zk-broker-1-to-controller-forwarding-channel-manager]: Shutting down\n05:48:55.439 [zk-broker-1-to-controller-forwarding-channel-manager] INFO kafka.server.BrokerToControllerRequestThread -- [zk-broker-1-to-controller-forwarding-channel-manager]: Stopped\n05:48:55.439 [main] INFO kafka.server.BrokerToControllerRequestThread -- [zk-broker-1-to-controller-forwarding-channel-manager]: Shutdown completed\n05:48:55.440 [main] INFO kafka.server.BrokerToControllerChannelManagerImpl -- Broker to controller channel manager for forwarding shutdown\n05:48:55.440 [main] INFO kafka.log.LogManager -- Shutting down.\n05:48:55.440 [main] INFO kafka.log.LogCleaner -- Shutting down the log cleaner.\n05:48:55.440 [main] INFO kafka.log.LogCleaner$CleanerThread -- [kafka-log-cleaner-thread-0]: Shutting down\n05:48:55.440 [kafka-log-cleaner-thread-0] INFO kafka.log.LogCleaner$CleanerThread -- [kafka-log-cleaner-thread-0]: Stopped\n05:48:55.440 [main] INFO kafka.log.LogCleaner$CleanerThread -- [kafka-log-cleaner-thread-0]: Shutdown completed\n05:48:55.444 [log-closing-/tmp/spring.kafka.69cecbcc-b8aa-44e4-b673-a5243dcc9dee11809573343017669845] INFO org.apache.kafka.storage.internals.log.ProducerStateManager -- [ProducerStateManager partition=messages-1]Wrote producer snapshot at offset 4 with 2 producer ids in 2 ms.\n05:48:55.449 [log-closing-/tmp/spring.kafka.69cecbcc-b8aa-44e4-b673-a5243dcc9dee11809573343017669845] INFO org.apache.kafka.storage.internals.log.ProducerStateManager -- [ProducerStateManager partition=__consumer_offsets-0]Wrote producer snapshot at offset 1 with 0 producer ids in 1 ms.\n05:48:55.453 [log-closing-/tmp/spring.kafka.69cecbcc-b8aa-44e4-b673-a5243dcc9dee11809573343017669845] INFO org.apache.kafka.storage.internals.log.ProducerStateManager -- [ProducerStateManager partition=messages-0]Wrote producer snapshot at offset 4 with 2 producer ids in 2 ms.\n05:48:55.457 [log-closing-/tmp/spring.kafka.69cecbcc-b8aa-44e4-b673-a5243dcc9dee11809573343017669845] INFO org.apache.kafka.storage.internals.log.ProducerStateManager -- [ProducerStateManager partition=__consumer_offsets-2]Wrote producer snapshot at offset 4 with 0 producer ids in 1 ms.\n05:48:55.463 [main] INFO kafka.log.LogManager -- Shutdown complete.\n05:48:55.464 [main] INFO kafka.controller.ControllerEventManager$ControllerEventThread -- [ControllerEventThread controllerId=1] Shutting down\n05:48:55.464 [controller-event-thread] INFO kafka.controller.ControllerEventManager$ControllerEventThread -- [ControllerEventThread controllerId=1] Stopped\n05:48:55.464 [main] INFO kafka.controller.ControllerEventManager$ControllerEventThread -- [ControllerEventThread controllerId=1] Shutdown completed\n05:48:55.464 [main] INFO kafka.controller.ZkPartitionStateMachine -- [PartitionStateMachine controllerId=1] Stopped partition state machine\n05:48:55.464 [main] INFO kafka.controller.ZkReplicaStateMachine -- [ReplicaStateMachine controllerId=1] Stopped replica state machine\n05:48:55.464 [main] INFO kafka.controller.RequestSendThread -- [RequestSendThread controllerId=1] Shutting down\n05:48:55.464 [main] INFO kafka.controller.RequestSendThread -- [RequestSendThread controllerId=1] Shutdown completed\n05:48:55.464 [Controller-1-to-broker-1-send-thread] INFO kafka.controller.RequestSendThread -- [RequestSendThread controllerId=1] Stopped\n05:48:55.465 [main] INFO kafka.controller.KafkaController -- [Controller id=1] Resigned\n05:48:55.465 [main] INFO kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread -- [feature-zk-node-event-process-thread]: Shutting down\n05:48:55.465 [feature-zk-node-event-process-thread] INFO kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread -- [feature-zk-node-event-process-thread]: Stopped\n05:48:55.465 [main] INFO kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread -- [feature-zk-node-event-process-thread]: Shutdown completed\n05:48:55.465 [main] INFO kafka.zookeeper.ZooKeeperClient -- [ZooKeeperClient Kafka server] Closing.\n05:48:55.530 [kafka-producer-network-thread | producer-2] INFO org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-2] Node 1 disconnected.\n05:48:55.530 [pool-14-thread-1] INFO org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-sampleRawConsumer-2, groupId=sampleRawConsumer] Node 0 disconnected.\n05:48:55.530 [kafka-producer-network-thread | producer-1] INFO org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-1] Node 1 disconnected.\n05:48:55.530 [kafka-producer-network-thread | producer-2] WARN org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-2] Connection to node 1 (localhost/127.0.0.1:41935) could not be established. Broker may not be available.\n05:48:55.530 [kafka-producer-network-thread | producer-1] WARN org.apache.kafka.clients.NetworkClient -- [Producer clientId=producer-1] Connection to node 1 (localhost/127.0.0.1:41935) could not be established. Broker may not be available.\n05:48:55.530 [pool-14-thread-1] WARN org.apache.kafka.clients.NetworkClient -- [Consumer clientId=consumer-sampleRawConsumer-2, groupId=sampleRawConsumer] Connection to node 0 (localhost/127.0.0.1:35013) could not be established. Broker may not be available.\n05:48:55.566 [main] INFO org.apache.zookeeper.ZooKeeper -- Session: 0x1001b05f8640001 closed\n05:48:55.566 [main-EventThread] INFO org.apache.zookeeper.ClientCnxn -- EventThread shut down for session: 0x1001b05f8640001\n05:48:55.567 [main] INFO kafka.zookeeper.ZooKeeperClient -- [ZooKeeperClient Kafka server] Closed.\n05:48:55.567 [main] INFO kafka.server.ClientQuotaManager$ThrottledChannelReaper -- [ThrottledChannelReaper-Fetch]: Shutting down\n05:48:55.567 [ThrottledChannelReaper-Fetch] INFO kafka.server.ClientQuotaManager$ThrottledChannelReaper -- [ThrottledChannelReaper-Fetch]: Stopped\n05:48:55.567 [main] INFO kafka.server.ClientQuotaManager$ThrottledChannelReaper -- [ThrottledChannelReaper-Fetch]: Shutdown completed\n05:48:55.567 [main] INFO kafka.server.ClientQuotaManager$ThrottledChannelReaper -- [ThrottledChannelReaper-Produce]: Shutting down\n05:48:55.567 [ThrottledChannelReaper-Produce] INFO kafka.server.ClientQuotaManager$ThrottledChannelReaper -- [ThrottledChannelReaper-Produce]: Stopped\n05:48:55.567 [main] INFO kafka.server.ClientQuotaManager$ThrottledChannelReaper -- [ThrottledChannelReaper-Produce]: Shutdown completed\n05:48:55.567 [main] INFO kafka.server.ClientQuotaManager$ThrottledChannelReaper -- [ThrottledChannelReaper-Request]: Shutting down\n05:48:55.567 [ThrottledChannelReaper-Request] INFO kafka.server.ClientQuotaManager$ThrottledChannelReaper -- [ThrottledChannelReaper-Request]: Stopped\n05:48:55.567 [main] INFO kafka.server.ClientQuotaManager$ThrottledChannelReaper -- [ThrottledChannelReaper-Request]: Shutdown completed\n05:48:55.567 [main] INFO kafka.server.ClientQuotaManager$ThrottledChannelReaper -- [ThrottledChannelReaper-ControllerMutation]: Shutting down\n05:48:55.567 [main] INFO kafka.server.ClientQuotaManager$ThrottledChannelReaper -- [ThrottledChannelReaper-ControllerMutation]: Shutdown completed\n05:48:55.567 [ThrottledChannelReaper-ControllerMutation] INFO kafka.server.ClientQuotaManager$ThrottledChannelReaper -- [ThrottledChannelReaper-ControllerMutation]: Stopped\n05:48:55.567 [main] INFO kafka.network.SocketServer -- [SocketServer listenerType=ZK_BROKER, nodeId=1] Shutting down socket server\n05:48:55.574 [main] INFO kafka.network.SocketServer -- [SocketServer listenerType=ZK_BROKER, nodeId=1] Shutdown completed\n05:48:55.574 [main] INFO org.apache.kafka.common.metrics.Metrics -- Metrics scheduler closed\n05:48:55.574 [main] INFO org.apache.kafka.common.metrics.Metrics -- Closing reporter org.apache.kafka.common.metrics.JmxReporter\n05:48:55.574 [main] INFO org.apache.kafka.common.metrics.Metrics -- Metrics reporters closed\n05:48:55.574 [main] INFO kafka.server.BrokerTopicStats -- Broker and topic stats closed\n05:48:55.574 [main] INFO org.apache.kafka.common.utils.AppInfoParser -- App info kafka.server for 1 unregistered\n05:48:55.574 [main] INFO kafka.server.KafkaServer -- [KafkaServer id=1] shut down completed\n05:48:55.576 [ConnnectionExpirer] INFO org.apache.zookeeper.server.NIOServerCnxnFactory -- ConnnectionExpirerThread interrupted\n05:48:55.577 [NIOServerCxnFactory.AcceptThread:/127.0.0.1:0] INFO org.apache.zookeeper.server.NIOServerCnxnFactory -- accept thread exitted run method\n05:48:55.578 [NIOServerCxnFactory.SelectorThread-2] INFO org.apache.zookeeper.server.NIOServerCnxnFactory -- selector thread exitted run method\n05:48:55.578 [NIOServerCxnFactory.SelectorThread-0] INFO org.apache.zookeeper.server.NIOServerCnxnFactory -- selector thread exitted run method\n05:48:55.579 [NIOServerCxnFactory.SelectorThread-3] INFO org.apache.zookeeper.server.NIOServerCnxnFactory -- selector thread exitted run method\n05:48:55.579 [NIOServerCxnFactory.SelectorThread-5] INFO org.apache.zookeeper.server.NIOServerCnxnFactory -- selector thread exitted run method\n05:48:55.580 [NIOServerCxnFactory.SelectorThread-4] INFO org.apache.zookeeper.server.NIOServerCnxnFactory -- selector thread exitted run method\n05:48:55.580 [NIOServerCxnFactory.SelectorThread-1] INFO org.apache.zookeeper.server.NIOServerCnxnFactory -- selector thread exitted run method\n05:48:55.581 [main] INFO org.apache.zookeeper.server.ZooKeeperServer -- shutting down\n05:48:55.581 [main] INFO org.apache.zookeeper.server.RequestThrottler -- Shutting down\n05:48:55.581 [RequestThrottler] INFO org.apache.zookeeper.server.RequestThrottler -- Draining request throttler queue\n05:48:55.581 [RequestThrottler] INFO org.apache.zookeeper.server.RequestThrottler -- RequestThrottler shutdown. Dropped 0 requests\n05:48:55.581 [main] INFO org.apache.zookeeper.server.SessionTrackerImpl -- Shutting down\n05:48:55.581 [main] INFO org.apache.zookeeper.server.PrepRequestProcessor -- Shutting down\n05:48:55.581 [main] INFO org.apache.zookeeper.server.SyncRequestProcessor -- Shutting down\n05:48:55.581 [ProcessThread(sid:0 cport:41533):] INFO org.apache.zookeeper.server.PrepRequestProcessor -- PrepRequestProcessor exited loop!\n05:48:55.581 [SyncThread:0] INFO org.apache.zookeeper.server.SyncRequestProcessor -- SyncRequestProcessor exited!\n05:48:55.581 [main] INFO org.apache.zookeeper.server.FinalRequestProcessor -- shutdown of request processor complete\n[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 4.892 s -- in com.zen.lab.services.program.consumer.LiveFeedPojoListenerTest\n05:48:55.595 [SessionTracker] INFO org.apache.zookeeper.server.SessionTrackerImpl -- SessionTrackerImpl exited loop!\n[INFO] \n[INFO] Results:\n[INFO] \n[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0\n[INFO] \n[INFO] \n[INFO] --- jar:3.3.0:jar (default-jar) @ sports-feed-processor-service ---\n[INFO] Building jar: /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/target/sports-feed-processor-service-1.0.jar\n[INFO] \n[INFO] --- spring-boot:3.2.1:repackage (repackage) @ sports-feed-processor-service ---\n[INFO] Replacing main artifact /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/target/sports-feed-processor-service-1.0.jar with repackaged archive, adding nested dependencies in BOOT-INF/.\n[INFO] The original artifact has been renamed to /tmp/workdir/baseline_strands_restricted_shell_20260128/spring-kafka-processor/target/sports-feed-processor-service-1.0.jar.original\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD SUCCESS\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time:  9.479 s\n[INFO] Finished at: 2026-01-29T05:48:56Z\n[INFO] ------------------------------------------------------------------------\n"
                            }
                        ]
                    }
                }
            ]
        },
        {
            "role": "assistant",
            "content": [
                {
                    "text": "Perfect! The migration to Java 17 has been completed successfully. The `mvn clean verify` command passed with all tests running successfully. \n\nHere's a summary of what was done:\n\n## Changes Made:\n\n### 1. **Updated pom.xml**:\n   - **Spring Boot**: Upgraded from 2.0.4.RELEASE to 3.2.1\n   - **Spring Cloud**: Upgraded from Finchley.SR1 to 2023.0.0\n   - **Java Version**: Set to 17 with compiler source and target properties\n   - **Dependencies updated**:\n     - logstash-logback-encoder: 5.0 \u2192 7.4\n     - aspectjweaver: 1.8.13 \u2192 1.9.21\n     - junit: Migrated from JUnit 4 (4.12) to JUnit Vintage Engine for backward compatibility\n     - Added spring-boot-starter-test for better test support\n\n### 2. **Fixed Spring Kafka API Changes** (Spring Kafka 3.x compatibility):\n   - **KafkaConfig.java**: \n     - Changed import from `org.springframework.kafka.listener.config.ContainerProperties` to `org.springframework.kafka.listener.ContainerProperties`\n     - Updated `AckMode` reference from `AbstractMessageListenerContainer.AckMode` to `ContainerProperties.AckMode`\n   \n   - **LiveFeedPojoListener.java**:\n     - Changed `KafkaHeaders.RECEIVED_PARTITION_ID` to `KafkaHeaders.RECEIVED_PARTITION`\n     - Changed `KafkaHeaders.RECEIVED_MESSAGE_KEY` to `KafkaHeaders.RECEIVED_KEY`\n\n### 3. **Fixed Test Infrastructure**:\n   - **LiveFeedPojoListenerTest.java**:\n     - Changed from `KafkaEmbedded` to `EmbeddedKafkaZKBroker` and then to `EmbeddedKafkaRule` for JUnit 4 compatibility\n     - Updated import from `org.springframework.kafka.test.rule.KafkaEmbedded` to use `EmbeddedKafkaRule`\n\nThe build completed successfully with all tests passing (2 tests, 0 failures, 0 errors, 0 skipped)."
                }
            ]
        }
    ]
}